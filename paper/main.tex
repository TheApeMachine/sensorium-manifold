% !TEX program = pdflatex
\documentclass[11pt,a4paper]{article}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{xcolor}
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage[most]{tcolorbox}
\usepackage{array}
\usepackage{colortbl}

% Fix for array column syntax
\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}

% ============================================================================
% COLORS
% ============================================================================
\definecolor{physicsblue}{RGB}{51, 102, 153}
\definecolor{mlgreen}{RGB}{76, 153, 76}
\definecolor{boxgray}{RGB}{245, 245, 245}

% ============================================================================
% THEOREM ENVIRONMENTS
% ============================================================================
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{principle}{Principle}

% ============================================================================
% CUSTOM COMMANDS - PHYSICS VOCABULARY
% ============================================================================
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\EMA}[1]{\overline{#1}}
\newcommand{\dt}{\Delta t}
\newcommand{\eps}{\varepsilon}

% Physical quantities
\newcommand{\sensorium}{\mathcal{S}}          % The manifold
\newcommand{\particle}{p}                      % A particle
\newcommand{\attractor}{a}                     % An attractor
\newcommand{\energy}{E}                        % Energy
\newcommand{\heat}{Q}                          % Heat (entropy carrier)
\newcommand{\excitation}{\varepsilon}          % Excitation
\newcommand{\baseline}{\mathcal{B}}            % Homeostatic baseline
\newcommand{\ratio}{\rho}                      % Homeostatic ratio
\newcommand{\bondgraph}{\mathcal{G}}           % Bond graph
\newcommand{\mass}{m}                          % Mass (bond strength)
\newcommand{\trace}{\tau}                      % Eligibility trace
\newcommand{\flux}{\Phi}                       % Energy flux
\newcommand{\entropy}{S}                       % Entropy
\newcommand{\pressure}{\Pi}                    % Metabolic pressure
\newcommand{\temperature}{T}                   % Temperature (sampling)
\newcommand{\diffusion}{D}                     % Diffusion coefficient

% ============================================================================
% TITLE
% ============================================================================
\title{%
  \textbf{The Sensorium Manifold}: \\[0.3em]
  \large Native Multimodality via Isomorphism
}

\author{
  Daniel Owen van Dommelen\\
  \textit{Independent Research - WORKING DRAFT}\\
  \texttt{theapemachine@gmail.com}
}

\date{\today}

% ============================================================================
% DOCUMENT
% ============================================================================
\begin{document}

\maketitle


% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
We introduce the \emph{Sensorium Manifold}, a thermodynamic computing substrate that replaces the autoregressive paradigm with global energy minimization. Current AI models rely on serial token generation and backpropagation-based optimization. We propose a system governed by \emph{Hamiltonian dynamics}, where data is represented as a field of coupled oscillators and learning is the emergence of resonant modes (carriers).

We introduce the \emph{Universal Tokenizer}, a modality-agnostic input mechanism that maps raw bytes and sequence indices to unique attractor IDs via hashing. This treats all data---text, image, audio---as a single branching stream where ``collision is compression.'' Structure emerges not through architectural bias, but through the physical bifurcation of energy flows.

The system operates via three principles: (1) \textbf{Spectral Entanglement}, where distant oscillators couple via shared carrier frequencies; (2) \textbf{Metabolic Gating}, where carriers persist only if energetically maintained by resonance; and (3) \textbf{Crystallization}, a parallel inference process where inputs are ``dumped'' into the manifold and allowed to relax into a minimum-energy configuration, filling in missing information simultaneously rather than causally. We demonstrate that this system conserves energy (symplectic integration), adapts to rule shifts online, and enables massive parallelism impossible in Transformer architectures.
\end{abstract}

\vspace{0.5em}
\noindent\textbf{A Note on Vocabulary.} This paper presents a computational substrate based on Hamiltonian mechanics and coupled oscillators. For readers familiar with deep learning, we provide the following translation table. Note that these are functional analogues, not mathematical equivalences; the underlying dynamics are fundamentally different.

\vspace{0.5em}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l l p{5.5cm}}
\toprule
\textbf{Physics Term} & \textbf{ML Analogue} & \textbf{Key Difference} \\
\midrule
Oscillator & Input Token & Has phase/frequency; exists in continuous time. \\
Carrier (Soliton) & Hidden State / Weight & A standing wave that couples oscillators. \\
Hamiltonian ($H$) & Loss Function & Conserved quantity; system minimizes potential $V$. \\
Spectral Coupling & Attention Mechanism & Non-local entanglement via frequency resonance. \\
Crystallization & Inference & Global parallel relaxation, not serial generation. \\
Metabolism & Regularization & Carriers decay if they do not receive energy. \\
Universal Tokenizer & Embedding Layer & Deterministic hashing of raw bytes; no training. \\
Phase Locking & Pattern Matching & Information encoded in relative phase angles. \\
Symplectic Integrator & Optimizer & Preserves energy phase-space; no gradient descent. \\
\bottomrule
\end{tabular}
\end{center}

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}
\label{sec:introduction}

The dominant paradigm in machine learning treats computation as optimization: define a loss function, compute gradients via backpropagation, and descend toward minima. This has proven remarkably effective, yet it imposes constraints that may not reflect how physical systems learn. Biological neural networks do not have access to global error signals; they adapt through local interactions governed by thermodynamic and biochemical principles.

We propose an alternative paradigm: \emph{thermodynamic computation}. In physical systems, structure emerges from energy flow, entropy production, and homeostatic regulation. We apply these principles to construct a learning system where:

\begin{itemize}
  \item \textbf{Particles} represent activated concepts, carrying energy through continuous space
  \item \textbf{Attractors} are stable points in the space, corresponding to learned representations
  \item \textbf{Bonds} encode relationships between concepts, with mass that grows from use and decays from disuse
  \item \textbf{Heat} captures uncertainty and accumulated noise, driving exploration
  \item \textbf{Homeostasis} regulates system activity through adaptive baselines, preventing runaway excitation or quiescence
\end{itemize}

\subsection{Native Multimodality}
\label{sec:native_multimodality}

A central claim of this work is that thermodynamic dynamics are \emph{modality-agnostic}. Current multimodal architectures---CLIP \citep{radford2021learning}, Flamingo \citep{alayrac2022flamingo}, Gemini \citep{team2023gemini}---require explicit cross-modal coupling mechanisms: contrastive losses that align representations, cross-attention layers that route information between modalities, or fusion modules that combine features. Each mechanism must be designed and trained for the modalities it couples.

We take a different approach. Like these systems, we use modality-specific encoders and decoders. Unlike them, we require no cross-modal coupling mechanisms. All sensory modalities can be represented as \emph{spectral distributions}: energy distributed over frequency bases.

\begin{itemize}
  \item \textbf{Audio}: Energy over temporal frequencies (Hz)
  \item \textbf{Images}: Energy over 2D spatial frequencies $(u, v)$
  \item \textbf{Video}: Energy over 3D spatiotemporal frequencies $(u, v, t)$
  \item \textbf{Text}: Energy over semantic embedding dimensions
\end{itemize}

By projecting these native spectral coordinates into a common Euclidean embedding space $\R^D$, we obtain a \emph{unified manifold} where particles from all modalities coexist. The thermodynamic dynamics---diffusion, bonding, metabolism, homeostasis---operate identically regardless of particle origin. This is integration by isomorphism.

\begin{principle}[Spectral Isomorphism]
\label{principle:isomorphism}
Let $\mathcal{M}_1, \mathcal{M}_2$ be sensory modalities with native spectral spaces $\mathcal{F}_1, \mathcal{F}_2$. There exist projections $\pi_1: \mathcal{F}_1 \to \R^D$ and $\pi_2: \mathcal{F}_2 \to \R^D$ such that the thermodynamic dynamics on $\R^D$ are identical for particles from either modality. Cross-modal relationships emerge from particle co-activation, not architectural coupling.
\end{principle}

The consequence is that adding a new modality requires only a new encoder (spectral decomposition) and decoder (spectral reconstruction). No new loss terms, attention patterns, or fusion modules are needed. Cross-modal relationships emerge from Hebbian co-activation: when particles from different modalities are active together, carriers couple them automatically. The manifold dynamics remain unchanged.

\subsection{Contributions}

\begin{enumerate}
  \item A complete thermodynamic framework for learning without backpropagation (\Cref{sec:framework})
  \item The Sensorium Manifold: a unified substrate for native multimodality (\Cref{sec:sensorium})
  \item Sparse bond graph dynamics that avoid dense $V \times V$ matrices (\Cref{sec:graphs})
  \item Dissipative structure maintenance through stochastic traversal (\Cref{sec:traversal})
  \item Empirical validation on cross-modal transduction and adaptation (\Cref{sec:experiments})
\end{enumerate}

% ============================================================================
% 2. THERMODYNAMIC FRAMEWORK
% ============================================================================
\section{Thermodynamic Framework}
\label{sec:framework}

We model learning as a physical process. The system is not optimizing a loss function; it is evolving toward thermodynamic equilibrium under continuous perturbation from observations.

\subsection{The Particle-Attractor System}
\label{sec:particles}

The Sensorium Manifold $\sensorium$ is a flat Euclidean space $\R^D$. Within this space, we distinguish two types of entities:

\begin{definition}[Particle]
A particle $\particle_i$ is a transient entity with:
\begin{itemize}
  \item Position $\bm{x}_i \in \R^D$
  \item Energy $\energy_i \geq 0$
  \item Heat $\heat_i \geq 0$
  \item Modality tag $\mu_i \in \{\text{text}, \text{audio}, \text{image}, \ldots\}$
\end{itemize}
Particles are created by encoders, evolve through dynamics, and are consumed by decoders.
\end{definition}

\begin{definition}[Attractor]
An attractor $\attractor_j$ is a persistent entity with:
\begin{itemize}
  \item Position $\bm{a}_j \in \R^D$
  \item Energy $\energy_j \geq 0$
  \item Heat $\heat_j \geq 0$
  \item Optional: associated token, frequency, or other modality-specific data
\end{itemize}
Attractors represent learned structure. Particles diffuse toward nearby attractors.
\end{definition}

\subsection{Thermodynamic Quantities}
\label{sec:quantities}

\begin{definition}[Total Energy]
The total energy of the system is:
\begin{equation}
\energy_{\text{total}} = \sum_i \energy_i + \sum_j \energy_j + \sum_i \excitation_i
\label{eq:total_energy}
\end{equation}
where $\excitation_i$ is the excitation (active energy available for flow).
\end{definition}

\begin{definition}[Heat]
Heat $\heat$ is the entropic component of energy---energy that has been ``used'' and can no longer do directed work. Heat accumulates from:
\begin{itemize}
  \item Incoherent activity (mismatch between modalities)
  \item Flow through bonds (friction)
  \item Conflict between competing predictions
\end{itemize}
Heat drives diffusion (exploration) and decays homeostatically.
\end{definition}

\subsection{Homeostatic Regulation}
\label{sec:homeostasis}

The central regulating mechanism is a \emph{homeostatic ratio} that compares current system energy to an adaptive baseline:

\begin{definition}[Homeostatic Ratio]
\label{def:ratio}
The homeostatic ratio is:
\begin{equation}
\ratio = \frac{\log(1 + \energy_{\text{total}})}{\log(1 + \baseline) + \eps}
\label{eq:ratio}
\end{equation}
where $\baseline$ is an exponential moving average (EMA) baseline:
\begin{equation}
\baseline_{t+1} = (1 - \alpha) \baseline_t + \alpha \, \energy_{\text{total}}, \quad \alpha = \frac{\dt}{\tau + \dt}
\label{eq:baseline}
\end{equation}
and $\tau$ is the homeostasis time constant.
\end{definition}

When $\ratio > 1$, the system is ``overheated'' and damping increases. When $\ratio < 1$, the system is ``cold'' and damping decreases. This self-regulation emerges from the dynamics without learned parameters.

\begin{remark}[Comparison to Batch Normalization]
Machine learning practitioners may recognize similarities to batch normalization \citep{ioffe2015batch}. However, homeostasis differs in key ways:
\begin{enumerate}
  \item No learned affine parameters ($\gamma$, $\beta$)
  \item Operates on total energy, not per-layer activations
  \item Adapts continuously online, not per-batch
  \item Regulates dynamics, not representations
\end{enumerate}
\end{remark}

\subsection{Diffusion Dynamics}
\label{sec:diffusion}

Particles diffuse toward nearby attractors. The dynamics are:

\begin{equation}
\frac{d\bm{x}_i}{dt} = -\nabla_{\bm{x}_i} U(\bm{x}_i) + \sqrt{2 \diffusion \heat_i} \, \bm{\xi}(t)
\label{eq:diffusion}
\end{equation}

where $U(\bm{x})$ is a potential defined by attractor positions:
\begin{equation}
U(\bm{x}) = -\sum_j \energy_j \exp\left(-\frac{\|\bm{x} - \bm{a}_j\|^2}{2\sigma^2}\right)
\label{eq:potential}
\end{equation}

and $\bm{\xi}(t)$ is white noise. Heat drives the stochastic term, enabling exploration when uncertainty is high.

\begin{remark}[Discretization]
In practice, we use Euler-Maruyama discretization with timestep $\dt$. We do not claim to solve continuous-time SDEs exactly.
\end{remark}

% ============================================================================
% 3. THE UNIVERSAL TOKENIZER
% ============================================================================
\section{The Universal Tokenizer}
\label{sec:tokenizer}

Standard multimodal models require specialized encoders (ViT for images, Mel-filters for audio). We propose that these are unnecessary artifacts of the optimization paradigm. In a physical system, structure is discovered, not engineered.

\subsection{Collision is Compression}
We map raw data to the manifold using a deterministic hash function:

\begin{equation}
\text{ID} = \mathcal{H}(\text{Byte}, \text{Index}) \pmod N
\label{eq:hash}
\end{equation}

where $\text{Byte} \in [0, 255]$ is the raw data and $\text{Index}$ is the sequence position (or spatial coordinate).

This implies that a black pixel at $(0,0)$ in Image A has the exact same ID as a black pixel at $(0,0)$ in Image B. In a neural network, this collision is a conflict. In the Resonant Manifold, this is \emph{compression}.
\begin{itemize}
    \item \textbf{Convergence}: All inputs sharing a prefix flow into the same initial attractors.
    \item \textbf{Bifurcation}: When the data diverges (e.g., Image A has a white pixel at $(0,1)$, Image B has black), the energy flow splits.
\end{itemize}

This creates a \emph{Thermodynamic Trie}. The system naturally learns the topology of the data stream by observing which transitions (bonds) are energetically favorable.

\subsection{Modality Agnosticism}
The physics engine is blind to the source of the data.
\begin{itemize}
    \item \textbf{Text}: $\mathcal{H}(\text{'H'}, 0) \to \mathcal{H}(\text{'e'}, 1) \dots$
    \item \textbf{Image}: $\mathcal{H}(\text{0xFF}, 0) \to \mathcal{H}(\text{0x00}, 1) \dots$
\end{itemize}
The manifold processes ``horizontal'' relationships identically, whether they represent a phoneme sequence or a line of pixels.

% ============================================================================
% 4. HAMILTONIAN DYNAMICS
% ============================================================================
\section{Hamiltonian Dynamics}
\label{sec:hamiltonian}

Unlike dissipative neural networks, the Resonant Manifold is a conservative system. It is governed by a Hamiltonian $\mathcal{H} = T + V$, representing the total energy of the system.

\subsection{The Resonant Ghost Field}
\label{subsec:ghost_field}

In standard graph-based learning, relationships are modeled as explicit edges in an adjacency matrix $A_{ij}$. This scales poorly ($O(N^2)$) and is rigid. We propose that semantic structure is not a wire connecting two points, but a standing wave potential that permeates the space. We term this the \emph{Resonant Ghost Field}.

\subsubsection{Field Definition}
The Ghost Field $\Psi$ is the aggregate scalar potential generated by the population of $M$ crystallized carriers (solitons). Unlike a neural network weight which exists to multiply a specific input, the Ghost Field exists as a background potential even in the absence of particles.

The Hamiltonian interaction term is defined as:

\begin{equation}
V_{\text{ghost}}(\mathbf{q}) = - \sum_{k=1}^M \underbrace{\mu_k}_{\text{Mass}} \cdot \left| \sum_{i=1}^N \exp\left( -\frac{(\omega_i - \Omega_k)^2}{2\sigma_k^2} + i(\theta_i - \phi_k) \right) \right|^2
\label{eq:ghost_potential}
\end{equation}

where:
\begin{itemize}
    \item $\mu_k$ is the metabolic mass (importance) of Carrier $k$.
    \item $\Omega_k$ is the intrinsic frequency of the Carrier.
    \item $\omega_i, \theta_i$ are the frequency and phase of the input oscillator $i$.
\end{itemize}

\subsubsection{Action at a Distance (Wormholes)}
This potential creates a non-Euclidean geometry. In the raw sequence index, token $i$ (at $t=0$) and token $j$ (at $t=1000$) are distant. However, if both tokens resonate with Carrier $k$ (i.e., $|\omega - \Omega_k| < \sigma_k$), the potential $V_{\text{ghost}}$ creates a deep energy well that binds them.

Effectively, the Resonant Ghost Field folds the manifold, creating a \emph{semantic wormhole}. Two oscillators tuned to the same carrier are entangled with zero distance in phase space, regardless of their separation in sequence space.

\subsubsection{The ``Ghost'' Property}
We term this field a ``Ghost'' field because it represents \emph{potential energy}. A crystallized carrier for the concept ``Cat'' exists in the manifold with $\mu_k > 0$. It is invisible and consumes no active compute (kinetic energy) until an input oscillator with the hash for ``Cat'' enters the system.

Upon entry, the oscillator immediately falls into the potential well of the Ghost Field, transferring kinetic energy into the carrier. The carrier ``wakes up'' (resonates), and through the entanglement described above, immediately pulls the ``Meow'' oscillator (if present) or hallucinates it (if missing) via the shared potential well.

\subsection{Symplectic Integration}
To ensure stability without ``magic number'' damping, we use a symplectic integrator (Velocity Verlet). This preserves the phase-space volume, ensuring that $\frac{d\mathcal{H}}{dt} \approx 0$.

\begin{align}
p(t + \dt/2) &= p(t) - \nabla V(q(t)) \frac{\dt}{2} \\
q(t + \dt) &= q(t) + \frac{p(t + \dt/2)}{m} \dt \\
p(t + \dt) &= p(t + \dt/2) - \nabla V(q(t + \dt)) \frac{\dt}{2}
\end{align}

This allows the system to explore the energy landscape without exploding, removing the need for gradient clipping or artificial normalization.

\subsection{Metabolic Gating}
While the short-term dynamics are conservative, the long-term structure is dissipative. Carriers (memory units) are subject to metabolic decay:

\begin{equation}
\frac{d \text{Mass}_k}{dt} = \text{Income}_k - \text{Cost}(\bar{E})
\end{equation}

Carriers that successfully resonate with input data receive ``Income'' (energy injection). Carriers that fail to resonate starve and vanish. This implements continuous, online model selection (Occam's Razor) via thermodynamics.

% ============================================================================
% 5. CRYSTALLIZATION: PARALLEL INFERENCE
% ============================================================================
\section{Crystallization}
\label{sec:crystallization}

Autoregressive models generate data serially ($t_1 \to t_2 \to t_3$). The Resonant Manifold performs inference via \emph{Global Relaxation} or \emph{Crystallization}.

\subsection{The Bucket Dump}
We slice the input data (e.g., an image with missing patches) into buckets and inject them into the manifold simultaneously.
\begin{enumerate}
    \item \textbf{Excitation}: Injected oscillators vibrate at their intrinsic frequencies.
    \item \textbf{Resonance}: Stored carriers (learned patterns) that match the partial input begin to resonate sympathetically.
    \item \textbf{Hallucination}: The resonating carriers pump energy into the missing oscillators (the gaps).
\end{enumerate}

\subsection{Phase Locking}
The solution emerges when the system settles into a phase-locked state (a local energy minimum). This is a non-causal process: future tokens can stabilize past tokens, and boundary conditions propagate inward. This allows for massive parallelism, as the entire sequence ``crystallizes'' out of the noise at once.

% ============================================================================
% 6. OBSERVER-DEPENDENT INFERENCE
% ============================================================================
\section{Observer-Dependent Inference}
\label{sec:inference}

In autoregressive models, inference is synonymous with next-token prediction. The Resonant Manifold decouples the \emph{dynamics} of the system from the \emph{observation} of the system. Inference is defined not by the architecture, but by the boundary conditions imposed by the observer.

\subsection{Inference as a Boundary Value Problem}
Mathematically, inference is the minimization of the Hamiltonian $\mathcal{H}$ subject to a set of constraints $\mathcal{C}$ defined by the observer:

\begin{equation}
\text{Solve } \nabla \mathcal{H} = 0 \quad \text{s.t. } \quad q_i = \text{target}_i \quad \forall i \in \mathcal{C}
\end{equation}

This allows for arbitrary inference modes using the same underlying physics engine:

\begin{itemize}
    \item \textbf{Causal Generation (Prediction)}: The observer clamps the past ($t < 0$) and allows the future ($t > 0$) to relax.
    \item \textbf{Inpainting (Bridging)}: The observer clamps the start ($t=0$) and the end ($t=N$), allowing the manifold to crystallize the lowest-energy bridge between them.
    \item \textbf{Super-Resolution (Up-sampling)}: The observer clamps low-frequency oscillators and allows high-frequency oscillators to thermalize, effectively ``hallucinating'' detail consistent with the coarse structure.
    \item \textbf{Semantic Constraint}: The observer clamps a specific Carrier (e.g., the ``Sadness'' mode) to a high amplitude. The particle oscillators then relax into a configuration that is harmonically compatible with that carrier, generating data with that specific semantic tone.
\end{itemize}

\subsection{The Measurement Problem}
Because the system exists in continuous phase space, the observer chooses \emph{how} to measure the output.
\begin{itemize}
    \item \textbf{Hard Measurement}: Collapsing the wavefunction by selecting the single oscillator with the highest amplitude at each position (ArgMax).
    \item \textbf{Soft Measurement}: Sampling the Boltzmann distribution of the thermalized system (Temperature Sampling).
    \item \textbf{Spectral Measurement}: Observing the aggregate energy of the Carriers rather than the particles, extracting the ``gist'' or semantic summary without decoding the literal tokens.
\end{itemize}

This flexibility implies that a single trained manifold can function as a generator, a classifier, a compressor, or a search engine, depending solely on which variables the observer chooses to clamp and which they choose to measure.

% ============================================================================
% 7. THE SENSORIUM MANIFOLD
% ============================================================================
\section{The Sensorium Manifold}
\label{sec:sensorium}

\subsection{Unified Particle Space}
\label{sec:unified}

The key insight is that particles from different modalities can coexist in a shared embedding space if we project their native coordinates appropriately.

\begin{definition}[Spectral Encoding]
For a sensory input with native spectral representation $\bm{f} \in \mathcal{F}$, the spectral encoding is:
\begin{equation}
\bm{x} = \pi(\bm{f}) \in \R^D
\label{eq:spectral_encoding}
\end{equation}
where $\pi: \mathcal{F} \to \R^D$ is a projection that preserves distance relationships.
\end{definition}

For modalities with native dimension $d < D$, we pad with zeros:
\begin{equation}
\pi(\bm{f}) = [\bm{f}; \bm{0}_{D-d}]
\label{eq:padding}
\end{equation}

For modalities with native dimension $d > D$, we use random projection (Johnson-Lindenstrauss):
\begin{equation}
\pi(\bm{f}) = \bm{f} \cdot \bm{P}, \quad \bm{P} \in \R^{d \times D}, \quad \bm{P}_{ij} \sim \mathcal{N}(0, 1/D)
\label{eq:random_projection}
\end{equation}

\subsection{Modality-Specific Encoders}
\label{sec:encoders}

\subsubsection{Audio Encoding}

Audio is encoded via 1D Fourier transform:
\begin{equation}
\hat{x}(f) = \int_{-\infty}^{\infty} x(t) e^{-2\pi i f t} \, dt
\label{eq:audio_fft}
\end{equation}

Each frequency bin $f_k$ with magnitude $|\hat{x}(f_k)|$ becomes a particle with:
\begin{itemize}
  \item Position: $\pi([f_k]) \in \R^D$
  \item Energy: $\energy_k = |\hat{x}(f_k)| / \sum_j |\hat{x}(f_j)|$
  \item Phase: $\phi_k = \arg(\hat{x}(f_k))$ (stored for reconstruction)
\end{itemize}

\subsubsection{Image Encoding}

Images are encoded via 2D Fourier transform:
\begin{equation}
\hat{I}(u, v) = \int \int I(x, y) e^{-2\pi i (ux + vy)} \, dx \, dy
\label{eq:image_fft}
\end{equation}

Each spatial frequency $(u_k, v_k)$ becomes a particle with:
\begin{itemize}
  \item Position: $\pi([u_k, v_k]) \in \R^D$
  \item Energy: $\energy_k = |\hat{I}(u_k, v_k)| / \sum_j |\hat{I}(u_j, v_j)|$
  \item Phase: $\phi_k = \arg(\hat{I}(u_k, v_k))$
\end{itemize}

\subsubsection{Text Encoding}

Text tokens are encoded via learned or pretrained embeddings:
\begin{equation}
\bm{x}_k = \text{embed}(t_k) \in \R^D
\label{eq:text_embed}
\end{equation}

If using a semantic embedding of dimension $d$, we project to $\R^D$ via $\pi$.

\subsection{The Unified Dynamics}
\label{sec:unified_dynamics}

Once all particles are in $\R^D$, the dynamics are identical:

\begin{enumerate}
  \item \textbf{Diffusion}: Particles drift toward attractors and diffuse based on heat
  \item \textbf{Bonding}: Co-activated particles form bonds (see \Cref{sec:graphs})
  \item \textbf{Metabolism}: Bonds grow from use, decay from cost
  \item \textbf{Homeostasis}: System energy is regulated against adaptive baseline
\end{enumerate}

The manifold does not ``know'' that a particle came from audio vs. image. It only sees positions, energies, and heats. Cross-modal relationships emerge naturally when particles from different modalities co-activate.

\subsection{Modality-Specific Decoders}
\label{sec:decoders}

To output in a specific modality, we select particles by modality tag and reconstruct:

\subsubsection{Audio Decoding}

Collect all audio-tagged particles. Reconstruct the spectrum:
\begin{equation}
\hat{x}(f_k) = \energy_k \cdot e^{i\phi_k}
\label{eq:audio_decode}
\end{equation}

Apply inverse FFT to obtain the waveform.

\subsubsection{Image Decoding}

Collect all image-tagged particles. Reconstruct the 2D spectrum:
\begin{equation}
\hat{I}(u_k, v_k) = \energy_k \cdot e^{i\phi_k}
\label{eq:image_decode}
\end{equation}

Apply inverse FFT2D to obtain the image.

% ============================================================================
% 4. SPARSE BOND GRAPH DYNAMICS
% ============================================================================
\section{Sparse Bond Graph Dynamics}
\label{sec:graphs}

Relationships between particles are encoded in a sparse directed graph, not dense weight matrices.

\subsection{Bond Graph Representation}
\label{sec:bond_graph}

\begin{definition}[Bond Graph]
The bond graph $\bondgraph = (V, E)$ is a sparse directed graph where:
\begin{itemize}
  \item $V$ is the set of attractor indices
  \item $E \subseteq V \times V$ is the set of directed bonds
  \item Each bond $(i, j) \in E$ has mass $\mass_{ij} \geq 0$ and trace $\trace_{ij} \geq 0$
\end{itemize}
\end{definition}

The graph is stored as a sparse edge list $(I, J, M, T)$, scaling with the number of observed transitions, not $O(V^2)$.

\subsection{Bond Formation (Hebbian Dynamics)}
\label{sec:hebbian}

Bonds form and strengthen through co-activation:

\begin{principle}[Hebbian Bonding]
When particles at attractors $i$ and $j$ are co-activated (e.g., observed in sequence), the bond $(i, j)$ receives mass:
\begin{equation}
\mass_{ij} \leftarrow \mass_{ij} + \dt \cdot \excitation_i \cdot \excitation_j
\label{eq:hebbian}
\end{equation}
\end{principle}

For sequential data, we observe $i \to j$ and inject mass:
\begin{equation}
\mass_{ij} \leftarrow \mass_{ij} + \dt \cdot \pressure \cdot (1 + \text{depletion}_i)
\label{eq:bond_injection}
\end{equation}

where $\pressure$ is the metabolic pressure (normalized surprise) and $\text{depletion}_i$ is accumulated from sink encounters during stochastic traversal.

\subsection{Metabolism}
\label{sec:metabolism}

Active bonds undergo metabolism: they receive income from use and pay cost for existence.

\begin{definition}[Bond Income]
\begin{equation}
\text{income}_{ij} = \excitation_i \cdot \tilde{\mass}_{ij} \cdot \frac{1}{1 + \bar{\heat} + \eps}
\label{eq:income}
\end{equation}
where $\tilde{\mass}_{ij} = \mass_{ij} / \sum_k \mass_{ik}$ is the normalized mass and $\bar{\heat}$ is mean heat.
\end{definition}

\begin{definition}[Bond Cost]
\begin{equation}
\text{cost}_{ij} = \ratio \cdot \bar{\excitation} \cdot \frac{\mass_{ij}}{\bar{\mass} + \eps}
\label{eq:cost}
\end{equation}
where $\ratio$ is the homeostatic ratio.
\end{definition}

The mass update is:
\begin{equation}
\mass_{ij} \leftarrow \max(0, \mass_{ij} + \dt \cdot (\text{income}_{ij} - \text{cost}_{ij}))
\label{eq:mass_update}
\end{equation}

This is not gradient descent. There is no loss function. Bonds that are used grow; bonds that are unused decay. The homeostatic ratio modulates the rate.

\subsection{Energy Flux}
\label{sec:flux}

Energy flows through bonds according to the flux equation:

\begin{equation}
\flux_j = \sum_{i: (i,j) \in E} \excitation_i \cdot \tilde{\mass}_{ij}
\label{eq:flux}
\end{equation}

This is the amount of excitation arriving at attractor $j$ from all sources. The computation is sparse---we only sum over existing bonds.

\subsection{Adaptive Pruning}
\label{sec:pruning}

Rather than fixed thresholds, bonds are pruned when their mass falls below the mean outgoing mass from their source:

\begin{equation}
\text{prune}(i,j) \iff \mass_{ij} < \frac{1}{|N_{\text{out}}(i)|} \sum_{k \in N_{\text{out}}(i)} \mass_{ik}
\label{eq:pruning}
\end{equation}

This maintains sparsity while adapting to the scale of each source.

% ============================================================================
% 5. STOCHASTIC TRAVERSAL (DISSIPATIVE STRUCTURE MAINTENANCE)
% ============================================================================
\section{Stochastic Traversal}
\label{sec:traversal}

Between observations, the system performs stochastic traversal---sampling paths through its bond graph under thermal noise. This is not passive relaxation toward equilibrium, but active dissipation: the system expends energy to explore its own structure.

Following Prigogine's theory of dissipative structures \citep{prigogine1977self}, we recognize this as the mechanism by which the system maintains organization far from equilibrium. The traversal \emph{exports entropy} (by exploring and abandoning unproductive paths) while \emph{reducing internal entropy} (by discovering shortcuts that sharpen future predictions). Without this process, the system would approach heat death---maximum entropy, no gradients, no useful work possible.

This is not ``offline training''---there is no training data. The system processes its own structure.

\subsection{Transitive Closure}
\label{sec:transitive}

If bonds $A \to B$ and $B \to C$ exist, we can infer a shortcut $A \to C$:

\begin{equation}
\mass_{AC} = \dt \cdot \tilde{\mass}_{AB} \cdot \tilde{\mass}_{BC} \cdot \exp\left(-\frac{\|\bm{a}_A - \bm{a}_C\|^2}{\bar{d}^2}\right)
\label{eq:transitive}
\end{equation}

The geometric factor $\exp(-d^2/\bar{d}^2)$ discounts shortcuts between distant attractors.

\subsection{Conflict Resolution}
\label{sec:conflict}

Sources with high outgoing entropy (ambiguous predictions) receive noise injection:

\begin{equation}
\excitation_i \leftarrow \excitation_i + \dt \cdot \mathcal{N}(0, \sigma_i^2), \quad \sigma_i \propto \entropy_i
\label{eq:conflict}
\end{equation}

where $\entropy_i = -\sum_j \tilde{\mass}_{ij} \log \tilde{\mass}_{ij}$ is the entropy of outgoing bonds. Subsequent metabolism allows competition to sharpen predictions.

\subsection{Stochastic Trajectories}
\label{sec:trajectories}

The system samples hypothetical trajectories from its current structure:

\begin{algorithm}[H]
\caption{Stochastic Trajectory Sampling}
\label{alg:trajectory}
\begin{algorithmic}[1]
\State $\text{budget} \leftarrow E_{\text{dream}} \cdot (1 + \gamma \cdot \max(0, \ratio - 1))$
\While{$\text{budget} > 0$}
  \State Sample starting attractor $i$ proportional to $\excitation_i$
  \State $\text{path\_prob} \leftarrow 1$
  \For{$t = 1, \ldots, T_{\text{max}}$}
    \State Compute $P(\cdot | i)$ from bond graph
    \If{$P$ is empty (sink)}
      \State $\text{depletion}_i \leftarrow \text{depletion}_i + \dt$ \Comment{Mark sink}
      \State \textbf{break}
    \EndIf
    \State Sample $j \sim P$
    \State $\text{path\_prob} \leftarrow \text{path\_prob} \cdot P(j|i)$
    \State Reinforce $(i, j)$ with mass $\propto \text{path\_prob}$
    \State $i \leftarrow j$
    \State $\text{budget} \leftarrow \text{budget} - (1 + \heat_i / \bar{\heat})$
  \EndFor
\EndWhile
\State $\text{depletion} \leftarrow \text{depletion} \cdot \exp(-\dt \cdot \ratio / \bar{\text{depletion}})$ \Comment{Homeostatic decay}
\end{algorithmic}
\end{algorithm}

Depletion accumulates at sinks (states with no outgoing flux). When real observations arrive, depletion modulates plasticity---the system is more receptive to learning transitions from depleted attractors. This is not a biological hunger signal; it is a local measurement of incomplete structure that increases the rate of bond formation.

% ============================================================================
% 6. CROSS-MODAL TRANSDUCTION
% ============================================================================
\section{Cross-Modal Transduction}
\label{sec:bridge}

The Sensorium Manifold enables cross-modal transduction without architectural changes. We describe the mechanism.

\subsection{Carrier Coupling}
\label{sec:carriers}

To transduce between semantic (text) and spectral (audio/image) representations, we introduce a population of \emph{carriers}---particles that have positions in both semantic and spectral spaces.

\begin{definition}[Carrier]
A carrier $c_k$ has:
\begin{itemize}
  \item Semantic position $\bm{s}_k \in \R^D$
  \item Spectral position $\bm{f}_k \in \R^{d_{\text{spec}}}$ (1D for audio, 2D for image)
  \item Energy $\energy_k \geq 0$
  \item Heat $\heat_k \geq 0$
\end{itemize}
\end{definition}

Carriers learn to couple semantic and spectral spaces through co-activation. When semantic particle at $\bm{s}$ and spectral particle at $\bm{f}$ are observed together, nearby carriers receive energy:

\begin{equation}
\energy_k \leftarrow \energy_k + \dt \cdot w_{\text{sem}}(\bm{s}, \bm{s}_k) \cdot w_{\text{spec}}(\bm{f}, \bm{f}_k)
\label{eq:carrier_update}
\end{equation}

where $w$ are kernel functions (e.g., Gaussian).

\subsection{Forward Transduction (Semantic to Spectral)}
\label{sec:forward}

To transduce semantic input to spectral output:

\begin{enumerate}
  \item Compute carrier activations from semantic input
  \item Each carrier projects its energy to its spectral position
  \item Collect spectral energy distribution
  \item Decode via inverse FFT
\end{enumerate}

\subsection{Backward Transduction (Spectral to Semantic)}
\label{sec:backward}

The same mechanism works in reverse. The manifold is bidirectional.

% ============================================================================
% 7. EXPERIMENTS
% ============================================================================
\section{Experiments}
\label{sec:experiments}

We validate the framework on three tasks:

\subsection{Rule-Shift Adaptation}
\label{sec:rule_shift}

We evaluate adaptation to distributional shifts using a controlled benchmark:

\begin{itemize}
  \item \textbf{Forward phase} (steps 1-1000): The sequence ``The cat sat on the mat'' repeats
  \item \textbf{Reverse phase} (steps 1001-2000): The sequence reverses
\end{itemize}

The system must rapidly unlearn forward transitions and learn reverse transitions, online, without gradient-based retraining.

\input{tables/rule_shift_summary.tex}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/rule_shift.pdf}{%
    \includegraphics[width=\textwidth]{figures/rule_shift.pdf}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Rule-shift adaptation dynamics. The system recovers to baseline accuracy within [X] steps after complete reversal of sequential structure.}
  \label{fig:rule_shift}
\end{figure}

\subsection{Cross-Modal Audio Synthesis}
\label{sec:audio_exp}

We demonstrate semantic-to-audio transduction using the carrier coupling mechanism. Given text tokens, the system produces spectral distributions that can be synthesized to audio.

\input{tables/audio_gen_summary.tex}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/audio_gen.pdf}{%
    \includegraphics[width=\textwidth]{figures/audio_gen.pdf}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Kernel audio completion via Universal Tokenizer byte inpainting (synthetic audio baseline).}
  \label{fig:audio_gen}
\end{figure}

\subsection{Cocktail Party Separation}
\label{sec:cocktail_party}

We evaluate a two-speaker mixture scenario using a fixed recorded mixture. The system forms distinct carrier subsets from short speaker prompts and reconstructs two different overlap completions by selecting which carriers to sample from.

\input{tables/cocktail_party_summary.tex}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/cocktail_party.pdf}{%
    \includegraphics[width=\textwidth]{figures/cocktail_party.pdf}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Cocktail-party experiment: mixture spectrogram and two masked-carrier reconstructions.}
  \label{fig:cocktail_party}
\end{figure}

\subsection{Native Image Handling}
\label{sec:image_exp}

We show that the same unified manifold handles 2D image frequencies. Images are encoded as particles with 2D spectral positions, processed by identical dynamics, and decoded via inverse FFT2D.

\input{tables/image_gen_summary.tex}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/image_gen.pdf}{%
    \includegraphics[width=\textwidth]{figures/image_gen.pdf}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Kernel MNIST completion via Universal Tokenizer byte inpainting.}
  \label{fig:image_gen}
\end{figure}

\subsection{Ablation Studies}
\label{sec:ablations}

\input{tables/ablation.tex}

% ============================================================================
% APPENDIX: ADDITIONAL EXPERIMENTS
% ============================================================================
\appendix
\section{Additional Experiments}
\label{sec:appendix_experiments}

This appendix aggregates additional kernel experiments that exercise the same
mechanism across tasks and different sampling/observation choices.

\subsection{Universal Tokenizer Next-Byte Prediction}
\input{tables/next_token_summary.tex}
\begin{figure}[t]
  \centering
  \IfFileExists{figures/next_token.pdf}{%
    \includegraphics[width=\textwidth]{figures/next_token.pdf}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Kernel next-byte prediction (UTF-8 bytes) using carrier-spectrum scoring.}
  \label{fig:next_token}
\end{figure}

\subsection{Universal Tokenizer Time-Series Forecasting}
\input{tables/timeseries_summary.tex}
\begin{figure}[t]
  \centering
  \IfFileExists{figures/timeseries.pdf}{%
    \includegraphics[width=\textwidth]{figures/timeseries.pdf}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Kernel time-series forecasting via next-byte prediction on a quantized synthetic series.}
  \label{fig:timeseries}
\end{figure}

\subsection{MNIST Classification from Raw Bytes}
\input{tables/mnist_bytes_summary.tex}
\begin{figure}[t]
  \centering
  \IfFileExists{figures/mnist_bytes.pdf}{%
    \includegraphics[width=0.75\textwidth]{figures/mnist_bytes.pdf}
  }{%
    \fbox{\parbox{0.75\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Kernel MNIST confusion matrix (Universal Tokenizer hashed pixel stream).}
  \label{fig:mnist_bytes}
\end{figure}

\subsection{Kernel Byte Denoising (Text Diffusion)}
\input{tables/text_diffusion_summary.tex}

\subsection{Continuous Kernel Simulation Snapshot}
\begin{figure}[t]
  \centering
  \IfFileExists{figures/continuous_final.png}{%
    \includegraphics[width=\textwidth]{figures/continuous_final.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Final dashboard frame from a finite kernel simulation run.}
  \label{fig:continuous_final}
\end{figure}

% ============================================================================
% 8. RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related}

\paragraph{Dissipative Structures}
Prigogine's theory of dissipative structures \citep{prigogine1977self, prigogine1978time} describes how systems far from equilibrium can maintain complex organization by continuously dissipating energy. Our stochastic traversal mechanism is a direct application: the system avoids heat death by actively exporting entropy through path exploration, while reducing internal entropy by discovering structural shortcuts.

\paragraph{Thermodynamic Computing}
Recent work explores physical substrates for computation based on thermodynamic principles \citep{conte2019thermodynamic}. We draw inspiration from these ideas but implement them in software.

\paragraph{Hebbian Learning}
Our approach shares principles with Hebbian learning \citep{hebb1949organization}. We extend this with thermodynamic regulation and sparse graph dynamics.

\paragraph{Energy-Based Models}
Energy-based models \citep{lecun2006tutorial} define learning as energy minimization. Our framework differs: energy is for homeostatic regulation, not optimization.

\paragraph{Multimodal Architectures}
CLIP \citep{radford2021learning} uses contrastive learning to align image and text encoders. Flamingo \citep{alayrac2022flamingo} introduces cross-attention layers to fuse visual features into language models. These approaches require explicit coupling mechanisms that must be designed per modality pair. We propose that cross-modal coupling can emerge from shared dynamics: particles coexist in a common space, and Hebbian co-activation creates associations without architectural intervention.

\paragraph{Predictive Coding}
Predictive coding \citep{rao1999predictive} models the brain as a prediction engine minimizing surprise. Our surprise-driven plasticity relates to this framework.

% ============================================================================
% 9. DISCUSSION
% ============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{What We Claim}

\begin{enumerate}
  \item \textbf{Native multimodality}: All sensory modalities can be processed by the same thermodynamic dynamics on a shared manifold.
  \item \textbf{No backpropagation}: Learning emerges from local, Hebbian-style dynamics regulated by homeostasis.
  \item \textbf{Online adaptation}: The system adapts continuously to streaming data and distributional shifts.
  \item \textbf{Dissipative self-organization}: Stochastic traversal maintains structure far from equilibrium by exporting entropy while discovering shortcuts.
\end{enumerate}

\subsection{What We Do Not Claim}

\begin{enumerate}
  \item \textbf{Strict thermodynamics}: We use thermodynamic \emph{metaphors}. Energy is not conserved; there is no detailed balance.
  \item \textbf{Gradient-free optimization}: We are not optimizing a loss function without gradients. We sidestep optimization entirely.
  \item \textbf{Transformer replacement}: Our experiments are on small-scale tasks. We make no claims about scaling to language model pretraining.
  \item \textbf{Continuous dynamics}: Our ``continuous'' dynamics are Euler-discretized. We do not solve PDEs exactly.
\end{enumerate}

\subsection{The Physics of ``Horizontal''}

We claimed that ``the physics of horizontal is the same whether it is a word, a sound wave, or a pixel pattern.'' Let us make this precise:

\begin{itemize}
  \item \textbf{Text}: The word ``horizontal'' is a token with a D-dimensional embedding.
  \item \textbf{Audio}: A sound panning left-to-right has specific frequency characteristics (Doppler, stereo phase).
  \item \textbf{Image}: A horizontal line has energy concentrated at $v \approx 0$ in the 2D frequency domain.
\end{itemize}

All three representations enter the manifold as particles. Through co-activation during training, carriers couple these representations. The word ``horizontal'' activates carriers that also respond to horizontal image frequencies and horizontal audio characteristics. This is not a metaphor---it is the mechanism.

% ============================================================================
% 10. CONCLUSION
% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We have presented the Sensorium Manifold, a unified thermodynamic substrate for native multimodal computation. By representing all sensory inputs as spectral distributions in a shared Euclidean space, we achieve modality-agnostic dynamics. Learning emerges from local thermodynamic interactions---energy flow, Hebbian bonding, homeostatic regulation---without backpropagation.

The framework suggests an alternative to the optimization-centric paradigm of modern machine learning. Physical principles---thermodynamics, diffusion, homeostasis---may offer paths to adaptive systems that are better suited to continuous, online, embodied learning.

The physics of ``horizontal'' really is the same across modalities. And that, perhaps, is how perception should work.

% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{plainnat}
\bibliography{references}

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\section{Extended Translation Table}
\label{app:translation}

For readers seeking deeper correspondences:

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l l}
\toprule
\textcolor{physicsblue}{\textbf{Physics Concept}} & \textcolor{mlgreen}{\textbf{ML Analogue}} \\
\midrule
\textcolor{physicsblue}{Euclidean space $\R^D$} & \textcolor{mlgreen}{Embedding space} \\
\textcolor{physicsblue}{Particle diffusion} & \textcolor{mlgreen}{Gradient flow / Langevin dynamics} \\
\textcolor{physicsblue}{Attractor basin} & \textcolor{mlgreen}{Class region / prototype} \\
\textcolor{physicsblue}{Energy injection} & \textcolor{mlgreen}{Forward pass activation} \\
\textcolor{physicsblue}{Heat dissipation} & \textcolor{mlgreen}{Regularization / dropout (sort of)} \\
\textcolor{physicsblue}{Homeostatic ratio $\ratio$} & \textcolor{mlgreen}{Adaptive learning rate} \\
\textcolor{physicsblue}{Bond mass $\mass$} & \textcolor{mlgreen}{Edge weight in sparse graph} \\
\textcolor{physicsblue}{Eligibility trace $\trace$} & \textcolor{mlgreen}{Temporal credit (no backprop)} \\
\textcolor{physicsblue}{Metabolic cost} & \textcolor{mlgreen}{Weight decay / L2 regularization} \\
\textcolor{physicsblue}{Metabolic income} & \textcolor{mlgreen}{Hebbian update} \\
\textcolor{physicsblue}{Transitive closure} & \textcolor{mlgreen}{Shortcut / skip connections} \\
\textcolor{physicsblue}{Stochastic trajectories} & \textcolor{mlgreen}{Monte Carlo sampling / rollouts} \\
\textcolor{physicsblue}{Depletion signal} & \textcolor{mlgreen}{Curiosity / exploration bonus} \\
\textcolor{physicsblue}{Sink} & \textcolor{mlgreen}{Absorbing state / dead end} \\
\textcolor{physicsblue}{Dissipative structure} & \textcolor{mlgreen}{Self-organizing system} \\
\textcolor{physicsblue}{Spectral distribution} & \textcolor{mlgreen}{Frequency representation} \\
\textcolor{physicsblue}{Carrier coupling} & \textcolor{mlgreen}{Cross-modal alignment} \\
\textcolor{physicsblue}{Event horizon locality} & \textcolor{mlgreen}{Local / sparse attention} \\
\bottomrule
\end{tabular}
\end{center}

\section{Hyperparameter Settings}
\label{app:hyperparameters}

\input{tables/hyperparameters.tex}

\section{Pseudocode}
\label{app:algorithms}

\begin{algorithm}[H]
\caption{Unified Manifold Step}
\label{alg:unified_step}
\begin{algorithmic}[1]
\Require Particles $\{(\bm{x}_i, \energy_i, \heat_i)\}$, Attractors $\{(\bm{a}_j, \energy_j)\}$, Bond graph $\bondgraph$
\State Project all particles to common space: $\bm{z}_i = \pi(\bm{x}_i)$
\State Compute $\ratio$ via \Cref{eq:ratio}
\State \textbf{Diffusion}: Update particle positions via \Cref{eq:diffusion}
\State \textbf{Bonding}: For co-activated pairs, inject mass via \Cref{eq:hebbian}
\State \textbf{Metabolism}: Update bond masses via \Cref{eq:mass_update}
\State \textbf{Flux}: Propagate energy via \Cref{eq:flux}
\State \textbf{Heat}: $\heat_i \leftarrow \heat_i + \dt \cdot |\flux_i| / \bar{\flux}$
\State \textbf{Damping}: $\excitation_i \leftarrow \excitation_i \cdot \exp(-\dt \cdot \ratio / \bar{\excitation})$
\State \textbf{Pruning}: Remove bonds below per-source mean
\State Sync energy and heat back to particles
\end{algorithmic}
\end{algorithm}

\end{document}
