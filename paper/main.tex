% !TEX program = pdflatex
\documentclass[11pt,a4paper]{article}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{xcolor}
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage[most]{tcolorbox}
\usepackage{array}
\usepackage{colortbl}
\usepackage{float}      % Provides [H] for "exactly here" placement
\usepackage{placeins}   % Provides \FloatBarrier to stop float drift

% ============================================================================
% FLOAT SETTINGS - Keep figures/tables near their text
% ============================================================================
% Allow more floats per page and be more permissive about placement
\renewcommand{\topfraction}{0.9}        % max fraction of page for floats at top
\renewcommand{\bottomfraction}{0.9}     % max fraction of page for floats at bottom
\renewcommand{\textfraction}{0.1}       % min fraction of page for text
\renewcommand{\floatpagefraction}{0.7}  % min fraction for a float-only page
\setcounter{topnumber}{3}               % max floats at top of page
\setcounter{bottomnumber}{3}            % max floats at bottom of page
\setcounter{totalnumber}{5}             % max floats per page

% Fix for array column syntax
\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}

% ============================================================================
% COLORS
% ============================================================================
\definecolor{physicsblue}{RGB}{51, 102, 153}
\definecolor{mlgreen}{RGB}{76, 153, 76}
\definecolor{boxgray}{RGB}{245, 245, 245}

% ============================================================================
% THEOREM ENVIRONMENTS
% ============================================================================
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{principle}{Principle}

% ============================================================================
% CUSTOM COMMANDS - PHYSICS VOCABULARY
% ============================================================================
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\EMA}[1]{\overline{#1}}
\newcommand{\dt}{\Delta t}
\newcommand{\eps}{\varepsilon}

% Physical quantities
\newcommand{\sensorium}{\mathcal{S}}          % The manifold
\newcommand{\particle}{p}                      % A particle
\newcommand{\psifield}{\Psi}                   % Hydrodynamic ω-field (wavefunction)
\newcommand{\psimode}{\Psi_k}                  % ω-bin complex amplitude (contextual index k)
\newcommand{\omegagrid}{\{\omega_k\}_{k=1}^M}  % Fixed ω-lattice
\newcommand{\energy}{E}                        % Energy
\newcommand{\heat}{Q}                          % Heat (entropy store)
\newcommand{\excitation}{\varepsilon}          % Excitation
\newcommand{\baseline}{\mathcal{B}}            % Homeostatic baseline
\newcommand{\ratio}{\rho}                      % Homeostatic ratio
\newcommand{\mass}{m}                          % Mass (particle)
\newcommand{\flux}{\Phi}                       % Energy flux
\newcommand{\entropy}{S}                       % Entropy
% (removed: \pressure was legacy "metabolic pressure" concept, no longer used)
\newcommand{\temperature}{T}                   % Temperature (sampling)
\newcommand{\diffusion}{D}                     % Diffusion coefficient

% Energy bookkeeping (explicit stores)
% Name: Kinetic energy (per particle)
% Formula: E_kin = (1/2) m ||v||^2
% Reason: separates directed motion from internal energy; used for conservation audits.
\newcommand{\ekin}{E_{\text{kin}}}
% Name: Oscillator (spectral) internal energy
% Formula: E_osc >= 0 (tracked state)
% Reason: amplitude A = sqrt(E_osc) and Planck equilibrium require an explicit oscillator store.
\newcommand{\eosc}{E_{\text{osc}}}
% Name: Total internal energy (thermal + oscillator)
% Formula: U = Q + E_osc
% Reason: temperature is derived from total internal energy; avoids one-way “heat-only” excitation.
\newcommand{\eint}{U}

% NOTE (LaTeX): Avoid double subscripts like E_{kin}_i by writing E_{kin,i}.
\newcommand{\ekinI}[1]{E_{\text{kin},#1}}
\newcommand{\eoscI}[1]{E_{\text{osc},#1}}
\newcommand{\eintI}[1]{U_{#1}}

% ============================================================================
% TITLE
% ============================================================================
\title{%
  \textbf{The Sensorium Manifold}: \\[0.3em]
  \large Native Multimodality via Isomorphism
}

\author{
  Daniel Owen van Dommelen\\
  \textit{Independent Research - WORKING DRAFT}\\
  \texttt{theapemachine@gmail.com}
}

\date{\today}

% ============================================================================
% DOCUMENT
% ============================================================================
\begin{document}

\maketitle


% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
We present the \emph{Sensorium Manifold}, a computing substrate that substitutes the traditional optimization paradigm for physics-based learning.
The intended objective is one of exploration, at the core of which is the question: if deep learning is essentially a balancing effort, where data is introduced into a chaotic system, then an error signal is reduced to a stable state, are there any other systems that can model these dynamics?
We propose that physics is such a system, once a subset of the physical laws are accurately modelled. Through that lens, it is not nearly such a huge leap to make, where deep learning relies on the perceptron as a model of the biological neuron, and halts this methaphor at the boundary of utility, where it becomes a highly successful tool to produce systems that learn, so have we defined such boundaries.
We suggest that there is indeed value to commit to a workable definition of what intelligence does, no what it is, where out stance is that intellligence in essense a data management system. $\Psi(\omega)$ that supports interference, superposition, and tunneling. Under observation-driven forcing and controlled dissipation, stable structure emerges---persistent wells in $|\Psi(\omega)|$ that encode patterns and bind related particles through phase coherence.

There is no loss function and no gradient descent. Structure forms because the physics favors it.

This crystallization mechanism implements a \emph{Holographic Content Addressable Memory}: partial inputs address complete patterns because information is distributed across the interference field. Because time is treated as a spatial dimension, generation is a boundary value problem rather than serial autoregression, with latency scaling as $O(k)$ in pattern complexity rather than $O(N)$ in sequence length. The same dynamics process all sensory modalities---text, image, audio---through spectral isomorphism.

We demonstrate these properties on small-scale experiments. Whether the approach scales is an open question; that thermodynamic dynamics can implement associative memory at all is the contribution.
\end{abstract}

\vspace{0.5em}
\noindent\textbf{A Note on Vocabulary.} This paper presents a computational substrate based on Hamiltonian mechanics and wave physics. Particles couple through a hydrodynamic wave field $\Psi(\omega)$ that supports interference, superposition, and tunneling in frequency space. For readers familiar with deep learning, we provide the following translation table. Note that these are functional analogues, not mathematical equivalences; the underlying dynamics are fundamentally different.

\vspace{0.5em}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l l p{5.5cm}}
\toprule
\textbf{Physics Term} & \textbf{ML Analogue} & \textbf{Key Difference} \\
\midrule
Particle (with $\omega_i, \theta_i$) & Input Token & Has phase/frequency; couples into wave field. \\
Wave Field $\Psi(\omega)$ & Hidden State / Weight & Complex field with interference, superposition, tunneling. \\
Well (Peak in $|\Psi(\omega)|$) & Learned Feature & Persistent wave structure that binds particles. \\
Hamiltonian ($H$) & Loss Function & Conserved quantity; system minimizes potential $V$. \\
Wave Coupling & Attention Mechanism & Non-local binding via interference in $\Psi(\omega)$. \\
Crystallization & Inference & Global parallel relaxation, not serial generation. \\
Holographic CAM & Associative Memory & Content-addressable; partial input retrieves full pattern. \\
Dissipation (Open System) & Regularization & Controlled decay stabilizes attractors. \\
Universal Tokenizer & Embedding Layer & Deterministic hashing of raw bytes; no training. \\
Inelastic Collision & Token Merging & Mass-conserving merge; frequency $=$ mass. \\
Morton Code & LSH / Spatial Index & Deterministic, collision-free, locality-preserving. \\
Phase Alignment & Pattern Matching & Information encoded in wave interference. \\
Symplectic Integrator & Optimizer & Preserves energy phase-space; no gradient descent. \\
\bottomrule
\end{tabular}
\end{center}

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}
\label{sec:introduction}

The perceptron, modeled on biological neurons, demonstrated that systems balancing toward equilibrium can encode information. Decades of work have validated this at scale, even as the architectures diverged from biological plausibility.

We ask whether similar behavior arises in other physical domains. After extensive exploration, we identified thermodynamic and wave-physical dynamics as sufficient ingredients: particles with intrinsic frequencies, a hydrodynamic wave field supporting interference and tunneling, and Hamiltonian evolution under observation-driven forcing.

The key insight is this: if information lacks intrinsic modality, then physical systems that naturally equilibrate may serve as substrates for computation. The physical world itself balances around information.

We test this hypothesis on AI---arguably the most demanding information-processing problem. If thermodynamic dynamics can implement associative memory and multimodal binding, the principle may generalize. If it cannot scale, we will have learned something about the limits of physics-as-computation.

We take this claim seriously enough to build it. The implementation includes GPU kernels for compressible Navier-Stokes, Gross-Pitaevskii field dynamics, and symplectic integration---not a sketch, but a system that runs. The code is available; we invite readers to find where it fails.

\paragraph{Physics as a computing paradigm.}
We emphasize that this is \emph{physics as a computing paradigm}, not a physics simulation. The goal is computation, not physical prediction. The equations we employ---compressible Navier-Stokes, Gross-Pitaevskii, Planck statistics---are real physics with production-grade numerics. But we apply them to an abstract computational space rather than modeling a specific physical system. The mathematical structure of thermodynamics and wave physics provides the substrate for learning and inference; whether this substrate could be realized in physical hardware is a separate question we do not address here.

\paragraph{System overview.}
The Sensorium Manifold is a learning system where:
\begin{itemize}
  \item \textbf{Particles} represent activated concepts, carrying energy through continuous space
  \item \textbf{Hydrodynamic $\omega$-Field} is a complex wavefunction $\Psi(\omega)$ whose wells correspond to learned structure
  \item \textbf{Crystallization} is the emergence of persistent peaks in $|\Psi(\omega)|$ under observation-driven forcing and dissipation
  \item \textbf{Heat} captures uncertainty and accumulated noise, driving exploration
  \item \textbf{Homeostasis} regulates system activity through adaptive baselines, preventing runaway excitation or quiescence
\end{itemize}

\subsection{Native Multimodality}
\label{sec:native_multimodality}

A central claim of this work is that thermodynamic dynamics are \emph{modality-agnostic}. Current multimodal architectures---CLIP \citep{radford2021learning}, Flamingo \citep{alayrac2022flamingo}, Gemini \citep{team2023gemini}---require explicit cross-modal coupling mechanisms: contrastive losses that align representations, cross-attention layers that route information between modalities, or fusion modules that combine features. Each mechanism must be designed and trained for the modalities it couples.

We take a different approach. Like these systems, we use modality-specific encoders and decoders. Unlike them, we require no cross-modal coupling mechanisms. All sensory modalities can be represented as \emph{spectral distributions}: energy distributed over frequency bases.

\begin{itemize}
  \item \textbf{Audio}: Energy over temporal frequencies (Hz)
  \item \textbf{Images}: Energy over 2D spatial frequencies $(u, v)$
  \item \textbf{Video}: Energy over 3D spatiotemporal frequencies $(u, v, t)$
  \item \textbf{Text}: Energy over semantic embedding dimensions
\end{itemize}

By projecting these native spectral coordinates into a common Euclidean embedding space $\R^D$, we obtain a \emph{unified manifold} where particles from all modalities coexist. The thermodynamic dynamics---diffusion, field-mediated coupling through $\Psi(\omega)$, and homeostasis---operate identically regardless of particle origin. This is integration by isomorphism.

\begin{principle}[Spectral Isomorphism]
\label{principle:isomorphism}
Let $\mathcal{M}_1, \mathcal{M}_2$ be sensory modalities with native spectral spaces $\mathcal{F}_1, \mathcal{F}_2$. There exist projections $\pi_1: \mathcal{F}_1 \to \R^D$ and $\pi_2: \mathcal{F}_2 \to \R^D$ such that the thermodynamic dynamics on $\R^D$ are identical for particles from either modality. Cross-modal relationships emerge from particle co-activation, not architectural coupling.
\end{principle}

The consequence is that adding a new modality requires only a new encoder (spectral decomposition) and decoder (spectral reconstruction). No new loss terms, attention patterns, or fusion modules are needed. Cross-modal relationships emerge from Hebbian co-activation: when particles from different modalities are active together, they reinforce shared wells in $\Psi(\omega)$ and become coupled through the same $\omega$-space geometry. The manifold dynamics remain unchanged.

\subsection{Contributions}

\begin{enumerate}
  \item A complete thermodynamic framework for learning without backpropagation (\Cref{sec:sensorium})
  \item The Universal Tokenizer: modality-agnostic encoding via deterministic hashing (\Cref{sec:tokenizer})
  \item Collision-as-compression: inelastic merging of redundant tokens via conservation laws, implementing dynamic sparsity without pruning schedules (\Cref{sec:collision_is_compression,sec:inelastic_collision})
  \item Locality-preserving linearization via Morton codes for deterministic, cache-efficient spatial indexing (\Cref{sec:morton_codes})
  \item Hydrodynamic $\omega$-field coupling: non-local entanglement via resonance and interference in $\Psi(\omega)$ (\Cref{sec:sensorium})
  \item Crystallization: all-token prediction as a boundary value problem (\Cref{sec:crystallization})
  \item Holographic Content Addressable Memory: partial inputs retrieve complete patterns (\Cref{subsec:holographic})
  \item Empirical validation on cross-modal transduction and adaptation (\Cref{sec:experiments})
\end{enumerate}

% ============================================================================
% 2. THE SENSORIUM MANIFOLD
% ============================================================================
\section{The Sensorium Manifold}
\label{sec:sensorium}

We model learning as a physical process. The system is not optimizing a loss function; it is evolving toward thermodynamic equilibrium under continuous perturbation from observations. The Sensorium Manifold $\sensorium$ is a three-dimensional thermodynamic simulation volume where particles interact via physical forces and a global $\omega$-space hydrodynamic field provides non-local coupling. Structure emerges from the interplay of local thermodynamic flow and global resonance/interference in $\Psi(\omega)$.

\subsection{The Two-Layer Architecture}
\label{sec:two_layer}

The system operates two physics engines in parallel, with each particle having dual identity:

\begin{definition}[Particle (Spatial Layer)]
A particle $\particle_i$ is an entity in the thermodynamic simulation with:
\begin{itemize}
  \item Position $\bm{x}_i \in \R^3$ and velocity $\bm{v}_i \in \R^3$
  \item Mass $m_i > 0$
  \item Oscillator energy $\eoscI{i} \geq 0$ (spectral/internal-mode energy; drives amplitude)
  \item Heat $\heat_i \geq 0$ (thermal energy; entropic store)
\end{itemize}
Particles are coupled to a compressible ideal-gas continuum (Navier--Stokes) via a conservative particle-in-cell (PIC) transfer.
\end{definition}

\begin{definition}[Particle ($\omega$-Hydrodynamic Layer)]
The same entity viewed in the wave layer has:
\begin{itemize}
  \item Intrinsic frequency $\omega_i$ (assigned by the Universal Tokenizer; conserved)
  \item Phase $\theta_i \in [0, 2\pi)$ (evolves under wave-field coupling)
  \item Amplitude $A_i = \sqrt{\eoscI{i}}$ (coupling strength into $\Psi$)
\end{itemize}
Particles couple non-locally via the hydrodynamic wave field $\Psi(\omega)$, which supports interference, superposition, and tunneling between neighboring $\omega$ bins. This is not Kuramoto-style oscillator synchronization; it is wave physics with complex amplitudes.
\end{definition}

The spatial layer governs local thermodynamics: gas pressure/transport on a periodic grid coupled to particles via PIC. The $\omega$-space hydrodynamic layer governs non-local coupling: stable peaks in $|\Psi(\omega)|$ act as persistent wells that align particle phases and recruit related particles through wave interference.

\subsection{Thermodynamic Quantities}
\label{sec:quantities}

\begin{definition}[Energy Stores and Total Energy]
We explicitly track three particle-local energy stores:
\begin{align}
\ekinI{i} &= \frac{1}{2} m_i \|\bm{v}_i\|^2 \\
\eintI{i} &= \heat_i + \eoscI{i} \\
\temperature_i &= \frac{\heat_i}{m_i c_v}
\end{align}
where $c_v$ is the specific heat capacity (constant in the current implementation).

\noindent The total tracked energy (used for dashboards and invariants) is:
\begin{equation}
\energy_{\text{total}} = \sum_i \left(\ekinI{i} + \heat_i + \eoscI{i}\right).
\label{eq:total_energy}
\end{equation}
This quantity is conserved by collision response and by the internal exchange $\heat \leftrightarrow \eosc$ (up to numerical error); it is not globally conserved in an \emph{open} system with observation-driven energy injection, drag-to-heat conversion, and driven/dissipative $\omega$-field dynamics.
\end{definition}

\begin{definition}[Heat]
Heat $\heat$ is the entropic component of energy---energy that has been ``used'' and can no longer do directed work. Heat accumulates from:
\begin{itemize}
  \item Incoherent activity (mismatch between modalities)
  \item Viscous drag and particle collisions
  \item Conflict between competing predictions
\end{itemize}
Heat diffuses spatially and exchanges bidirectionally with oscillator energy $\eosc$ toward thermodynamic equilibrium (\Cref{sec:planck_exchange}).
\end{definition}

\subsection{Homeostatic Regulation}
\label{sec:homeostasis}

The central regulating mechanism is a \emph{homeostatic ratio} that compares current system energy to an adaptive baseline:

\begin{definition}[Homeostatic Ratio]
\label{def:ratio}
The homeostatic ratio is:
\begin{equation}
\ratio = \frac{\log(1 + \energy_{\text{total}})}{\log(1 + \baseline) + \eps}
\label{eq:ratio}
\end{equation}
where $\baseline$ is an exponential moving average (EMA) baseline:
\begin{equation}
\baseline_{t+1} = (1 - \alpha) \baseline_t + \alpha \, \energy_{\text{total}}, \quad \alpha = \frac{\dt}{\tau + \dt}
\label{eq:baseline}
\end{equation}
and $\tau$ is the homeostasis time constant.
\end{definition}

When $\ratio > 1$, the system is ``overheated'' and damping increases. When $\ratio < 1$, the system is ``cold'' and damping decreases. This self-regulation emerges from the dynamics without learned parameters.

\begin{remark}[Comparison to Batch Normalization]
Machine learning practitioners may recognize similarities to batch normalization \citep{ioffe2015batch}. However, homeostasis differs in key ways:
\begin{enumerate}
  \item No learned affine parameters ($\gamma$, $\beta$)
  \item Operates on total energy, not per-layer activations
  \item Adapts continuously online, not per-batch
  \item Regulates dynamics, not representations
\end{enumerate}
\end{remark}

\subsection{Spatial Dynamics}
\label{sec:diffusion}

We evolve a compressible ideal-gas continuum on a periodic grid, then advect particles as Lagrangian parcels coupled via PIC.

\paragraph{Eulerian gas dynamics (grid).}
Let $\rho(\bm{x},t)$ be mass density, $\bm{u}(\bm{x},t)$ velocity, and $e_{\text{int}}(\bm{x},t)\equiv \rho(\bm{x},t)\,e(\bm{x},t)$ internal energy density. We use the \emph{dual-energy} formalism: we evolve internal energy directly (rather than recovering it as a small difference of total and kinetic energy), which is the standard robust treatment for high-Mach flows.
\begin{align}
\partial_t \rho + \nabla\cdot(\rho\bm{u}) &= 0, \\
\partial_t(\rho\bm{u}) + \nabla\cdot(\rho\bm{u}\otimes\bm{u} + p\mathbf{I}) &= \nabla\cdot\tau + \rho \bm{g}, \\
\partial_t e_{\text{int}} + \nabla\cdot\!\left(e_{\text{int}}\bm{u}\right) &= -p\,(\nabla\cdot\bm{u}) + \tau:\nabla\bm{u} + \nabla\cdot(k\nabla T),
\label{eq:ns}
\end{align}
with ideal-gas closure
\begin{equation}
p = \rho R_{\mathrm{specific}} T, 
\qquad
p = (\gamma-1)\,e_{\text{int}},
\label{eq:eos}
\end{equation}
where $R_{\mathrm{specific}} = (N_A k_B)/M$ is the specific gas constant (molar mass $M$), and $\gamma=c_p/c_v$.

\begin{tcolorbox}[colback=white,colframe=black,title=\textbf{Dual-Energy Formalism (High-Mach Robustness)}]
\textbf{Why internal energy?} In high-Mach regimes, total energy density is dominated by kinetic energy, and internal energy becomes a small residual. Recovering internal energy via \(E - \tfrac{1}{2}\rho\|\bm{u}\|^2\) is therefore numerically ill-conditioned in finite precision and can spuriously produce negative pressure/temperature. We instead evolve \(e_{\text{int}}=\rho e\) directly, adding compressional work \(-p\nabla\cdot\bm{u}\) and transport terms, which is mathematically rigorous (it is the internal-energy equation derived from the conservative total-energy form) and standard practice in cosmological/astrophysical hydrodynamics solvers \citep{stone1992zeus2d, bryan1995ppm}. This choice avoids unphysical negative \(p\) and the associated pathologies in the ideal-gas entropy \(s\propto \ln\!\big(p/\rho^\gamma\big)\), aligning the numerics with the Second Law by keeping the thermodynamic state well-defined.
\end{tcolorbox}

\begin{remark}[Lagrangian steepening and finite-time breakdown (analogy)]
Particle methods can exhibit rapid Lagrangian steepening and extreme concentration (caustic-like behavior) in finite time under adverse flow geometry; classical boundary-layer analysis contains well-known examples of such breakdown in the Lagrangian description \citep{vandommelen1980singularity}. In our setting this motivates (i) fail-loud diagnostics for nonphysical thermodynamic states, and (ii) the dual-energy choice above so that, when gradients become extreme, pressure/temperature are not computed by subtracting nearly equal large numbers.
\end{remark}

\paragraph{Gravity (grid).}
We compute $\phi$ via the periodic Poisson solve
\begin{equation}
\nabla^2 \phi = 4\pi G \rho,
\end{equation}
and apply body acceleration $\bm{g}=-\nabla\phi$.

\paragraph{PIC coupling (particles $\leftrightarrow$ grid).}
Particles carry $(m_i,\bm{v}_i,\eoscI{i},\heat_i)$, and we transfer conserved quantities to the grid with trilinear (CIC) weights:
\[
\rho \leftarrow \sum_i w_i \, m_i,\quad
\rho\bm{u} \leftarrow \sum_i w_i \, (m_i\bm{v}_i),\quad
e_{\text{int}} \leftarrow \sum_i w_i \, \heat_i,
\]
then gather $(\bm{u},T)$ back to particles to advect $\bm{x}_i$ and update thermal energy via $T_i=\heat_i/(m_i c_v)$.

\noindent\textbf{Implementation note.} The grid internal-energy field is strictly thermal: oscillator energy \(\eosc\) is tracked separately and does not contribute directly to gas pressure. Kinetic energy is also tracked separately for diagnostics; it is \emph{not} deposited into \(e_{\text{int}}\).

\begin{remark}[Implementation]
The reference discretization is an explicit finite-volume update with a robust Rusanov/LLF flux and RK2 time stepping, with $\dt$ capped by CFL and diffusive stability constraints. In the current code snapshot accompanying this draft, the thermodynamic grid fields are used as PIC-reconstructed diagnostics for particle updates (scatter $\to$ FFT Poisson gravity $\to$ gather); the Eulerian RK2 grid evolution is treated as an optional module and is not required for the experiments reported here.
\end{remark}

\subsection{Thermal--Oscillator Equilibration (Planck Relaxation)}
\label{sec:planck_exchange}
To avoid a one-way ``temperature $\to$ excitation'' coupling, we explicitly couple the thermal store $\heat_i$ and the oscillator store $\eoscI{i}$ via a conservative local exchange that relaxes toward the Planck distribution.

\paragraph{Equilibrium energy.}
For an oscillator of intrinsic frequency $\omega_i$ in a heat bath at temperature $\temperature_i$, the mean equilibrium energy is:
\begin{equation}
E_{\text{osc,eq}}(\omega_i,\temperature_i) =
\frac{\hbar \omega_i}{\exp\!\left(\frac{\hbar\omega_i}{k_B \temperature_i}\right) - 1}.
\label{eq:planck_energy}
\end{equation}
This has the correct limits: $E_{\text{osc,eq}}\approx k_B\temperature$ for $\hbar\omega \ll k_B\temperature$ (classical equipartition) and $E_{\text{osc,eq}}\to 0$ for $\hbar\omega \gg k_B\temperature$ (freeze-out).

\paragraph{Conservative exchange.}
We apply a local relaxation step
\begin{align}
\Delta \eoscI{i} &= \alpha_i \left(E_{\text{osc,eq}}(\omega_i,\temperature_i) - \eoscI{i}\right), \\
\eoscI{i} &\leftarrow \eoscI{i} + \Delta \eoscI{i}, \\
\heat_i &\leftarrow \heat_i - \Delta \eoscI{i},
\end{align}
with the constraint $\heat_i \ge 0$ enforced by capping positive $\Delta\eoscI{i}$ to the available $\heat_i$.

\paragraph{Timescale (no tunable knobs).}
The exchange rate $\alpha_i$ is derived from an existing conduction timescale for a sphere of radius $r$ in a medium of thermal conductivity $\kappa$:
\begin{equation}
\tau_i = \frac{m_i c_v}{4\pi \kappa r},
\qquad
\alpha_i = 1 - \exp\!\left(-\frac{\dt}{\tau_i}\right).
\label{eq:planck_alpha}
\end{equation}
This ensures that equilibration speed is set by physical transport parameters already present in the spatial model.

\subsection{Non-Local Coupling via the \texorpdfstring{$\omega$}{ω}-Field}
\label{sec:omega_field}

The $\omega$-field is the key mechanism for non-local coupling and memory formation. It is a complex wave field that evolves via Gross-Pitaevskii / nonlinear Schrödinger dynamics, with particle coupling entering through the observation-driven potential $V_{\mathrm{ext}}(\omega)$ and through phase feedback. (The full mathematical treatment appears in \Cref{sec:hamiltonian}.)

\begin{definition}[$\omega$-Field Degree of Freedom]
Each $\omega$ bin $k$ stores a complex amplitude $\Psi_k \equiv \Psi(\omega_k)$. The field is a wavefunction over frequency space: it supports superposition and constructive/destructive interference, and tunneling/diffusion across neighboring $\omega$ bins via a discrete Laplacian.
\end{definition}

\paragraph{Phase dynamics.}
Particle phases are updated by a torque induced by the wave field:
\begin{align}
\dot{\theta}_i
&=
\omega_i
 + \kappa \sum_k T_{ik}\,A_i\,|\Psi_k|\,\sin(\arg\Psi_k - \theta_i),
\label{eq:phase_update}
\end{align}
where $\kappa$ is a coupling scale and $T_{ik}$ is the Hamiltonian-derived resonance/overlap weight (\Cref{eq:tik_factor}--\Cref{eq:spatial_overlap}). This term makes interference in $\Psi$ operational: phase-aligned support reinforces wells; phase-misaligned support cancels.

\paragraph{Field crystallization (memory).}
In the fixed $\omega$-lattice regime, ``crystallization'' refers to the emergence of persistent, localized structure in $|\Psi(\omega)|$: peaks that remain stable under the combined effect of the nonlinear term and kinetic tunneling in (\Cref{eq:gpe_omega}). Stable peaks act as memory---wave attractors that phase-lock particles and resist perturbation.

\paragraph{No spawning/splitting.}
The current implementation does not spawn, split, or delete $\omega$ bins. Mode separation occurs continuously through the field dynamics: tunneling spreads energy across nearby bins while the nonlinear interaction concentrates it, allowing the system to form multiple distinct wells across the fixed lattice without discrete bookkeeping.

\begin{remark}[Sparse evaluation without approximation]
The overlap term $O_{ik}$ contains a Gaussian factor in distance; in IEEE-754 fp32 it underflows to \emph{exactly zero} beyond a finite radius. Therefore, interactions outside this radius are provably irrelevant in fp32 and can be skipped with no approximation.
\end{remark}

\subsection{Idle Compute}
\label{sec:idle_compute}

Between observations, the system performs \emph{idle compute}---internal processing that refines $\omega$-field structure without external input. Following Prigogine's theory of dissipative structures \citep{prigogine1977self}, this is the mechanism by which the system maintains organization far from equilibrium.

The idle compute has three modes:
\begin{enumerate}
    \item \textbf{Consolidation}: Low noise, favoring crystallization of stable wells in $|\Psi(\omega)|$. The system ``locks in'' patterns that have been consistently reinforced.
    \item \textbf{Disambiguation}: Mode-separation forces active. Nearby wells in $\omega$-space repel each other, reducing mode collision.
    \item \textbf{Exploration}: High noise, low weight thresholds. Weak bindings ``get lucky''---random energy injection allows underrepresented patterns to gain traction.
\end{enumerate}

The system computes global energy statistics (mean, variance) via GPU reduction and uses these to adaptively scale decay rates, noise amplitude, and driving strength. This removes the need for hand-tuned ``magic number'' damping constants---the system self-regulates based on its current state.

% ============================================================================
% 3. THE UNIVERSAL TOKENIZER
% ============================================================================
\section{The Universal Tokenizer}
\label{sec:tokenizer}

Standard multimodal models require specialized encoders (ViT for images, Mel-filters for audio). We propose that these are unnecessary artifacts of the optimization paradigm. In a physical system, structure is discovered, not engineered.

\subsection{Collision is Compression}
\label{sec:collision_is_compression}

We map raw data to the manifold using a deterministic hash function:

\begin{equation}
\text{ID} = h(\text{Byte}, \text{Index}) \pmod N
\label{eq:hash}
\end{equation}

where $h$ is a deterministic hash function, $\text{Byte} \in [0, 255]$ is the raw data, and $\text{Index}$ is the sequence position (or spatial coordinate). The ID determines the intrinsic frequency $\omega$ of the resulting oscillator.

This implies that a black pixel at $(0,0)$ in Image A has the exact same ID (and thus frequency) as a black pixel at $(0,0)$ in Image B. In a neural network, this collision is a conflict. In the Sensorium Manifold, this is \emph{compression}.

\paragraph{A disambiguation.}
The word ``collision'' carries opposite connotations in physics and computer science. In CS, a hash collision is a \emph{failure mode}: two distinct data items map to the same memory address, requiring linked lists or probing to resolve the conflict---causing warp divergence on GPUs and destroying performance \citep{indyk1998approximate}. In physics, a collision is an \emph{interaction}: two particles occupy the same spatial cell and exchange momentum, energy, and information. The Sensorium Manifold exploits the physics meaning. When two particles collide in the manifold, they are not fighting for a memory slot---they are \emph{discovering that they represent the same thing}.

\begin{definition}[Physical Collision as Compression]
\label{def:physical_collision}
Let particle $p_a$ carry byte value $b_a$ at sequence position $s_a$, and particle $p_b$ carry $(b_b, s_b)$. A \emph{compressive collision} occurs when:
\begin{enumerate}
  \item \textbf{Spatial co-location}: $p_a$ and $p_b$ occupy the same grid cell $c$ in the simulation volume.
  \item \textbf{Identity match}: $h(b_a, s_a) = h(b_b, s_b)$---i.e., they have the same token ID.
\end{enumerate}
When both conditions hold, the two particles are informationally redundant: same content, same context, same spatial neighborhood. One can be absorbed into the other without information loss (\Cref{sec:inelastic_collision}).
\end{definition}

\begin{itemize}
    \item \textbf{Convergence}: All inputs sharing a prefix deposit support into the same region of $\Psi(\omega)$, reinforcing shared wells.
    \item \textbf{Bifurcation}: When the data diverges, support shifts to different $\omega$ regions, forming distinct wells that separate continuations.
\end{itemize}

\paragraph{The semantic collider.}
Consider the sequence ``AB AB.'' Tokens `A' at positions 0 and 3 receive the same hash $h(\texttt{0x41}, 0) = h(\texttt{0x41}, 3)$ if the segment size maps both to position 0 within their respective segments. In the spatial layer, the $\omega$-field coupling and gravity pull both `A' particles toward the same geometric coordinate, because their frequency-space support overlaps. When they arrive in the same cell, the merge operator (\Cref{sec:inelastic_collision}) absorbs one into the other. The result: a single particle `A' with mass 2. The system has \emph{physically folded} the sequence ``AB AB'' onto itself, discovering that it is ``AB'' repeated. Mass becomes frequency---the confidence that a pattern exists at that location.

This creates a \emph{Thermodynamic Trie}. The system naturally learns the topology of the data stream by observing which $\omega$-space wells emerge and persist.

\subsection{Modality Agnosticism}
The physics engine is blind to the source of the data.
\begin{itemize}
    \item \textbf{Text}: $h(\text{'H'}, 0) \to h(\text{'e'}, 1) \dots$
    \item \textbf{Image}: $h(\text{0xFF}, 0) \to h(\text{0x00}, 1) \dots$
\end{itemize}
All become oscillators with intrinsic frequencies. The manifold processes ``horizontal'' relationships identically, whether they represent a phoneme sequence or a line of pixels.

\subsection{Locality-Preserving Linearization}
\label{sec:morton_codes}

The manifold simulation volume is three-dimensional, but GPU memory is one-dimensional. The mapping between spatial coordinates and linear memory addresses determines whether particles that are \emph{physically close} are also \emph{close in memory}---a property critical for cache efficiency during collision detection and neighbor traversal.

\paragraph{The problem with na\"ive indexing.}
The standard row-major linearization $\text{idx} = x + y \cdot G_x + z \cdot G_x G_y$ places $(x, y, z)$ adjacent to $(x{+}1, y, z)$ in memory, but $(x, y{+}1, z)$ is a stride of $G_x$ away and $(x, y, z{+}1)$ is a stride of $G_x G_y$ away. When a collision kernel scans the 26 neighbors of a cell, it generates scattered memory accesses that thrash the GPU cache.

\paragraph{Morton codes (Z-order curve).}
A space-filling curve solves this by interleaving the bits of each coordinate \citep{morton1966computer, samet2006foundations}. For a cell at $(x, y, z)$ with $B$-bit coordinates, the Morton code is:
\begin{equation}
\mathcal{Z}(x, y, z) = \text{interleave}(x, y, z) = \sum_{b=0}^{B-1} \left[ x_b \cdot 2^{3b} + y_b \cdot 2^{3b+1} + z_b \cdot 2^{3b+2} \right],
\label{eq:morton}
\end{equation}
where $x_b$ denotes the $b$-th bit of $x$. This traces a ``Z''-shaped fractal path through 3D space, ensuring that cells close in Euclidean distance are close in the linear index. For $G = 64$ (6 bits per dimension), the Morton code produces an 18-bit collision-free index over $G^3 = 262{,}144$ cells.

\begin{remark}[Morton Codes as Locality-Sensitive Hashing]
The Morton code is a \emph{deterministic, collision-free} locality-sensitive hash \citep{indyk1998approximate}. Unlike randomized LSH schemes used in efficient transformers \citep{kitaev2020reformer}, the Morton code preserves \emph{exact} spatial relationships: cells sharing a Z-order prefix are guaranteed to lie within a bounded spatial region. This is exploited in GPU BVH construction \citep{lauterbach2009fast} and is directly applicable to our collision detection kernel.
\end{remark}

\paragraph{Composite sort key.}
To achieve both spatial grouping and deterministic ordering, we construct a 64-bit composite key for each particle:
\begin{equation}
\text{Key}_i = \left(\mathcal{Z}(\text{cell}_i) \ll 8\right) \,|\, \text{byte}_i,
\label{eq:composite_key}
\end{equation}
where $\text{cell}_i$ is the Morton-coded cell index (high bits) and $\text{byte}_i \in [0, 255]$ is the raw data content (low bits). Sorting the particle array by this key achieves three properties simultaneously:
\begin{enumerate}
  \item \textbf{Spatial grouping}: All particles in the same cell are contiguous in memory.
  \item \textbf{Content sub-ordering}: Within each cell, particles are ordered by byte value.
  \item \textbf{Determinism}: The particle ordering is invariant to thread scheduling---eliminating the non-deterministic \texttt{atomic\_add} race conditions present in conventional scatter-based particle binning.
\end{enumerate}

This composite key makes the merge operator (\Cref{sec:inelastic_collision}) trivial: after sorting, particles with identical keys are adjacent. A single linear scan identifies and compresses duplicates.

\subsection{Inelastic Collision as Dynamic Compression}
\label{sec:inelastic_collision}

When two particles occupy the same cell and carry the same byte value, they are informationally redundant. We resolve this via a \emph{perfectly inelastic collision}: the two particles merge into one, conserving mass, momentum, and energy.

\begin{definition}[Inelastic Merge Operator]
\label{def:merge}
Let particles $p_a$ and $p_b$ have identical composite keys (\Cref{eq:composite_key}). The merge produces a single particle $p_c$:
\begin{align}
m_c &= m_a + m_b, \label{eq:merge_mass} \\
\bm{v}_c &= \frac{m_a \bm{v}_a + m_b \bm{v}_b}{m_c}, \label{eq:merge_momentum} \\
\heat_c &= \heat_a + \heat_b, \label{eq:merge_heat} \\
\eoscI{c} &= \eoscI{a} + \eoscI{b}. \label{eq:merge_eosc}
\end{align}
Particle $p_b$ is marked as \emph{dead} ($m_b \leftarrow 0$) and excluded from subsequent dynamics.
\end{definition}

\paragraph{Physics interpretation.}
This is a perfectly inelastic collision in the center-of-mass frame. Kinetic energy in the relative velocity is converted to heat:
\begin{equation}
\Delta \heat = \frac{1}{2} \frac{m_a m_b}{m_a + m_b} \|\bm{v}_a - \bm{v}_b\|^2.
\label{eq:merge_heat_gain}
\end{equation}
The system automatically converts redundant kinetic energy into thermal energy, providing a natural regularization mechanism.

\paragraph{Computational interpretation.}
From the perspective of machine learning, the merge operator implements \emph{dynamic token merging} \citep{bolya2023token, bolya2023token_diffusion}. Standard Token Merging (ToMe) identifies similar tokens in a vision transformer by bipartite matching and averages their representations. Our approach is more principled: identical tokens in the same spatial context are identified by the composite sort key and merged via conservation laws. The particle count $N$ decreases monotonically as the simulation discovers redundancy, reducing computational cost without any explicit pruning schedule.

\paragraph{Information-theoretic view.}
The merge operator acts as a \emph{run-length encoder} on the vacuum. After sorting by the composite key, consecutive particles with the same key represent repeated observations of the same (byte, position) pair. Merging replaces $k$ copies with a single particle of mass $k \cdot m_0$. The mass field becomes a natural histogram:
\begin{equation}
m_i \propto \text{count}(\text{byte}_i, \text{cell}_i) = \text{frequency of pattern at location}.
\label{eq:mass_frequency}
\end{equation}
High-mass particles represent high-confidence patterns. Low-mass particles represent rare or noisy observations. The dynamics naturally amplify the former (deeper gravitational wells, stronger $\omega$-field coupling) and attenuate the latter (thermal diffusion, dissipation).

\begin{remark}[Agglomeration: Dust to Planets]
The merge operator implements astrophysical agglomeration in miniature. In N-body simulations of planet formation, colliding planetesimals merge and grow \citep{samet2006foundations}. Here, ``planetesimals'' are tokens: dust (single bytes) becomes pebbles (repeated bigrams) becomes planets (crystallized patterns). The mass hierarchy that emerges is the thermodynamic trie.
\end{remark}

\subsection{Validation: Next-Byte Prediction}
\label{sec:tokenizer_validation}

To validate that hash collisions create useful structure, we train on 7 text patterns (e.g., ``The cat sat.'') with segment size 16. The character `T' at position 0 receives the same token ID across all patterns, creating the thermodynamic trie.

\textbf{Result}: The system achieves 92.4\% overall accuracy---99.3\% on deterministic paths (single valid continuation) and 55.8\% at branch points (multiple valid continuations). This demonstrates that the trie structure emerges naturally from hash collisions, enabling prediction without gradient-based training. Full details in Appendix~\ref{sec:next_byte_prediction}.

% ============================================================================
% 4. HAMILTONIAN DYNAMICS
% ============================================================================
\section{Hamiltonian Dynamics}
\label{sec:hamiltonian}

Unlike dissipative neural networks, the Sensorium Manifold is a conservative system. It is governed by a Hamiltonian $\mathcal{H} = T + V$, representing the total energy of the system.

\subsection{The Hydrodynamic \texorpdfstring{$\omega$}{ω}-Field}
\label{subsec:omega_dynamics}

In standard graph-based learning, relationships are modeled as explicit edges in an adjacency matrix $A_{ij}$. This scales poorly ($O(N^2)$) and is rigid. We propose that semantic structure is not a wire connecting two points, but a \emph{wavefluid} in frequency space: a complex field whose superposition and tunneling properties implement association and generalization.

\subsubsection{Field definition (fixed \texorpdfstring{$\omega$}{ω}-lattice)}
We define a fixed $\omega$-lattice $\{\omega_k\}_{k=1}^M$ and a complex field
\begin{equation}
\Psi_k(t) \;\equiv\; \Psi(\omega_k, t) \in \mathbb{C}.
\end{equation}
This $\Psi(\omega)$ is not a ``population of spawned entities'' in the current implementation. It is a \emph{fixed} set of degrees of freedom whose energy can concentrate into persistent peaks (soliton-like attractors) or spread by tunneling across neighboring $\omega$ bins.

\subsubsection{Dynamics (dissipative Gross--Pitaevskii in \texorpdfstring{$\omega$}{ω})}
The $\omega$-field evolves by a dissipative Gross--Pitaevskii-style update:
\begin{equation}
i\hbar_{\mathrm{eff}}\,\frac{d\Psi_k}{dt}
\;=\;
\left(
V_{\mathrm{ext},k}(t)
 + g\,|\Psi_k|^2
 - \mu
\right)\Psi_k
\;-\;
\frac{\hbar_{\mathrm{eff}}^2}{2m_{\mathrm{eff}}}\,
\frac{\Delta_\omega \Psi_k}{(\Delta\omega)^2},
\label{eq:gpe_omega}
\end{equation}
with a non-unitary settling term
\begin{equation}
\Psi_k \leftarrow e^{-\Gamma\,\dt}\,\Psi_k,
\label{eq:gpe_damping}
\end{equation}
where:
\begin{itemize}
  \item $\Delta_\omega \Psi_k \equiv \Psi_{k-1} - 2\Psi_k + \Psi_{k+1}$ is the 1D discrete Laplacian on the uniform $\omega$ grid,
  \item $m_{\mathrm{eff}}$ controls tunneling speed in $\omega$-space (larger $m_{\mathrm{eff}}$ $\Rightarrow$ slower diffusion),
  \item $g$ is a nonlinear self-interaction (attractive $g<0$ enables soliton-like concentration),
  \item $\mu$ is a chemical potential term for population control / bias,
  \item $\Gamma \ge 0$ is an energy-decay rate (open-system relaxation),
  \item $\hbar_{\mathrm{eff}}$ is an effective Planck constant in simulation units.
\end{itemize}

\paragraph{External potential from observations.}
The external potential is derived from instantaneous oscillator support accumulated at each $\omega$ bin:
\begin{equation}
V_{\mathrm{ext},k}(t) \;\equiv\; -w_k(t),
\label{eq:vext_support}
\end{equation}
so bins with sustained support become energetically favorable wells. This is the mechanism by which repeated patterns sculpt a stable energy landscape in $\omega$.

\paragraph{Numerics (phase fidelity).}
To preserve interference structure, the implementation uses a symmetric split-step ordering (Strang-style): half potential/nonlinear rotation, full kinetic (Laplacian) update, then a second half potential/nonlinear rotation, followed by the dissipative decay (\Cref{eq:gpe_damping}).

\subsubsection{Particle--wave coupling (resonance + overlap)}
Particles do not couple by explicit edges. They couple through the wave field $\Psi(\omega)$ via a Hamiltonian-mediated weight
\begin{equation}
T_{ik} \;=\; R_{ik}\, O_{ik},
\label{eq:tik_factor}
\end{equation}
which factors into frequency resonance and real-space overlap.

\paragraph{Frequency resonance (linewidth / coherence time).}
We use a Lorentzian lineshape, appropriate for a finite coherence time / damped oscillator:
\begin{equation}
R_{ik}
=
\frac{\gamma_k^2}{(\omega_i-\Omega_k)^2 + \gamma_k^2},
\label{eq:lorentz_resonance}
\end{equation}
where $\gamma_k$ is the mode linewidth.

\paragraph{Real-space overlap (support anchors).}
$\omega$-field modes are global in frequency space, but their interaction strength is mediated by a real-space overlap integral proxy. Each mode/bin $k$ maintains a small set of \emph{anchors} (oscillator indices) representing its spatial support. Let $\mathcal{A}(k)$ be the anchor set of bin $k$, with anchor weights $w_{ka}\ge 0$ and anchor positions $\bm{x}_a$. Then:
\begin{equation}
O_{ik}
=
\frac{\sum_{a\in\mathcal{A}(k)} w_{ka}\,
\exp\!\left(-\frac{\|\Delta\bm{x}_{ia}\|^2}{4\sigma_x^2}\right)}
\sum_{a\in\mathcal{A}(k)} w_{ka} + \eps,
\label{eq:spatial_overlap}
\end{equation}
where $\Delta\bm{x}_{ia}$ is computed with the minimum-image convention on a periodic domain (wrap boundary conditions).

\paragraph{Coherence length (no guessed spatial scale).}
We set the spatial coherence length $\sigma_x$ from the thermal de Broglie wavelength. In the implementation we work in simulation (nondimensional) units with $\hbar_{\mathrm{eff}}=1$ and $k_B=1$, so this reduces to a temperature-coupled length scale derived from the current bath:
\begin{equation}
\lambda_T = \sqrt{2\pi}\,\frac{\hbar_{\mathrm{eff}}}{\sqrt{m\,k_B\,T}}
\;=\;
\frac{\sqrt{2\pi}}{\sqrt{m\,T}},
\qquad
\sigma_x \equiv \lambda_T,
\label{eq:thermal_de_broglie}
\end{equation}
using mean bath temperature and mass for the current tick, where the bath temperature is derived from particle heat via $T_i = Q_i/(m_i c_v)$, and the implementation clamps $\sigma_x$ only to derived resolution/domain bounds ($\Delta x \le \sigma_x \le 0.5\min(L_x,L_y,L_z)$).

With these definitions:
\begin{itemize}
    \item $\Psi_k$ is the complex $\omega$-field state (amplitude and phase) at bin $k$.
    \item $\omega_k$ is the intrinsic frequency coordinate of bin $k$ (fixed lattice).
    \item $\gamma_k$ is the linewidth (frequency selectivity / coherence time scale).
    \item $\omega_i, \theta_i$ are the frequency and phase of oscillator $i$.
\end{itemize}

\subsubsection{Action at a Distance (Semantic Wormholes)}
The wave field $\Psi(\omega)$ induces a non-Euclidean geometry in \emph{frequency space}. Two particles that are distant in sequence index (or geometric position) can become tightly coupled if they share support for the same $\omega$-region: they ``fall'' into the same $\Psi(\omega)$ well and are phase-torqued toward alignment through wave-mediated coupling.

\subsubsection{The latent property}
Persistent peaks in $|\Psi(\omega)|$ represent \emph{potential energy in frequency space}. They are latent in the sense that they need no active compute until new particles enter whose $(\omega_i,\theta_i)$ resonate with the well; then the wave coupling rapidly aligns phases and recruits related particles via the shared $\omega$-geometry.

\subsection{Symplectic Integration}
To ensure stability without ``magic number'' damping, we use a symplectic integrator (Velocity Verlet). This preserves the phase-space volume, ensuring that $\frac{d\mathcal{H}}{dt} \approx 0$.

\begin{align}
p(t + \dt/2) &= p(t) - \nabla V(q(t)) \frac{\dt}{2} \\
q(t + \dt) &= q(t) + \frac{p(t + \dt/2)}{m} \dt \\
p(t + \dt) &= p(t + \dt/2) - \nabla V(q(t + \dt)) \frac{\dt}{2}
\end{align}

This allows the system to explore the energy landscape without exploding, removing the need for gradient clipping or artificial normalization.

\subsection{Open-System Driving (No Metabolic State Machine)}
The current implementation is an open system: observations sculpt $V_{\mathrm{ext}}$ (\Cref{eq:vext_support}) and an explicit decay rate $\Gamma$ (\Cref{eq:gpe_damping}) provides settling. We do \emph{not} implement a separate ``metabolic mass'' bookkeeping or spawning/splitting lifecycle in the fixed $\omega$-lattice regime. Instead, the emergence and persistence of memory is identified with stable concentration of $|\Psi(\omega)|$ (and can be thresholded for reporting).

% ============================================================================
% 5. CRYSTALLIZATION AND INFERENCE
% ============================================================================
\section{Crystallization and Inference}
\label{sec:crystallization}

The dominant paradigm in generative AI is \emph{autoregression}: predicting the next token $x_{t+1}$ given $x_{0:t}$. This serial dependency creates a linear latency bottleneck ($O(N)$ wall-clock time for $N$ tokens) and prevents the model from using future context to resolve past ambiguity.

The Sensorium Manifold replaces autoregression with \emph{Crystallization}---treating generation as a \emph{Boundary Value Problem} (BVP) rather than an Initial Value Problem. Because the system is governed by a global Hamiltonian, we relax the entire field to satisfy observer-imposed constraints rather than integrating forward in time.

\subsection{Time as a Spatial Dimension}
In the manifold, the sequence index is treated as a spatial coordinate. The $\omega$-space field $\Psi(\omega)$ couples oscillators across the entire sequence length simultaneously.
\begin{itemize}
    \item \textbf{Input}: We inject a set of constraints (e.g., a prompt at $t=0$, a desired sentiment at $t=N$, or sparse keyframes in a video).
    \item \textbf{Relaxation}: We initialize the unconstrained particles (the ``empty'' space) with thermal noise.
    \item \textbf{Dynamics}: The system evolves under Hamiltonian dynamics. Resonant $\omega$-wells pump energy into compatible (empty) particles via wave-mediated phase-torque coupling.
\end{itemize}

\subsection{Massive Parallelism}
Because the interactions are mediated by the $\omega$-space field (computed globally per step), the oscillators at $t=10$ and $t=1000$ evolve in parallel.
\begin{equation}
\text{Latency}(N) \propto k_{\text{relaxation}}
\end{equation}
The wall-clock time to generate a sequence depends on the \emph{complexity} of the energy landscape (how long it takes to relax), not the \emph{length} of the sequence. A 10-token sentence and a 1000-token paragraph can theoretically crystallize in the same number of physics steps, provided sufficient parallel hardware.

\subsection{Global Coherence}
This ``All-Token Prediction'' allows for non-causal error correction. In an autoregressive model, a mistake at $t=5$ propagates to $t=100$. In the Sensorium Manifold, the emergence of a strong pattern at $t=100$ creates a resonant potential that travels \emph{backwards} in time, forcing the oscillator at $t=5$ to flip its state to maintain global phase coherence. The result is a self-correcting, holographic generation process.

\subsection{Holographic Content Addressable Memory}
\label{subsec:holographic}

The crystallization mechanism reveals the manifold's fundamental nature as a \emph{Holographic Content Addressable Memory} (HCAM). Consider the optical analogy: if you cut a hologram in half, you do not get half the image---you get the \emph{entire} image, albeit at lower resolution. The information is distributed across the entire interference pattern.

The Sensorium Manifold exhibits the same property:
\begin{itemize}
    \item \textbf{Distributed Encoding}: $\omega$-field modes encode patterns as standing waves across the entire phase space. The ``meaning'' of a sequence is not localized to specific oscillators but is distributed across resonant structure in $\Psi(\omega)$.
    \item \textbf{Content Addressing}: Injecting \emph{any} subset of the pattern (50\% of tokens, randomly scattered) is sufficient to address the complete memory. The $\omega$-field reconstructs the whole signal because stable wells resonate with partial input.
    \item \textbf{Graceful Degradation}: As the number of constraint tokens decreases, the reconstruction becomes ``blurrier'' (higher entropy, more ambiguity) but remains structurally coherent.
\end{itemize}

The mathematical basis is the $\omega$-field dynamics (\Cref{eq:gpe_omega,eq:vext_support}) together with the resonance/overlap coupling (\Cref{eq:tik_factor}--\Cref{eq:spatial_overlap}). When partial input sustains a well in $V_{\mathrm{ext}}$ near some $\omega_k$, the resulting peak in $|\Psi(\omega_k)|$ recruits compatible oscillators (frequency-resonant and within anchored spatial support)---including oscillators not directly observed but present in the relaxation field. The manifold ``hallucinates'' missing data by relaxing toward a low-energy configuration under these constraints.

\begin{equation}
\text{Query}(\mathcal{C}_{\text{partial}}) \to \arg\min_{\mathbf{q}} \mathcal{H}(\mathbf{q}) \quad \text{s.t.} \quad q_i = c_i \; \forall \, i \in \mathcal{C}_{\text{partial}}
\label{eq:hcam_query}
\end{equation}

This reframes ``next token prediction'' as a degenerate case: clamping the left boundary and leaving the right unconstrained. ``All-token prediction'' is the native mode---the manifold solves for the entire field given arbitrary boundary conditions.

\subsection{Observer-Dependent Inference}
\label{sec:inference}

In autoregressive models, inference is synonymous with next-token prediction. The Sensorium Manifold decouples the \emph{dynamics} of the system from the \emph{observation} of the system. Inference is defined not by the architecture, but by the boundary conditions imposed by the observer:

\begin{equation}
\text{Solve } \nabla \mathcal{H} = 0 \quad \text{s.t. } \quad q_i = \text{target}_i \quad \forall i \in \mathcal{C}
\end{equation}

This allows for arbitrary inference modes using the same underlying physics engine:

\begin{itemize}
    \item \textbf{Causal Generation (Prediction)}: The observer clamps the past ($t < 0$) and allows the future ($t > 0$) to relax.
    \item \textbf{Inpainting (Bridging)}: The observer clamps the start ($t=0$) and the end ($t=N$), allowing the manifold to crystallize the lowest-energy bridge between them.
    \item \textbf{Super-Resolution (Up-sampling)}: The observer clamps low-frequency oscillators and allows high-frequency oscillators to thermalize, effectively ``hallucinating'' detail consistent with the coarse structure.
    \item \textbf{Semantic Constraint}: The observer clamps a specific $\omega$-region (or a target profile of $|\Psi(\omega)|$) to high amplitude. Oscillators then relax into a configuration that is harmonically compatible with that constraint, generating data with the desired semantic tone.
\end{itemize}

\paragraph{The measurement problem.}
Because the system exists in continuous phase space, the observer chooses \emph{how} to measure the output:
\begin{itemize}
    \item \textbf{Hard Measurement}: Collapsing the wavefunction by selecting the single oscillator with the highest amplitude at each position (ArgMax).
    \item \textbf{Soft Measurement}: Sampling the Boltzmann distribution of the thermalized system (Temperature Sampling).
    \item \textbf{$\omega$-Field Measurement}: Observing global summaries of $\Psi(\omega)$ (e.g., peak structure in $|\Psi|$) rather than decoding individual particles, extracting a semantic ``gist''.
\end{itemize}

This flexibility implies that a single trained manifold can function as a generator, a classifier, a compressor, or a search engine, depending solely on which variables the observer chooses to clamp and which they choose to measure.

\paragraph{Dark particles.}
\label{sec:dark_particles}

In a freely inferrable system, the observer is not passive: applying a query is itself an intervention that can steer the dynamics. We therefore distinguish between \emph{endogenous} field-mediated feedback (stable wells in $\Psi(\omega)$ biasing phase alignment) and \emph{exogenous} observer-driven forcing.

\begin{definition}[Dark Particle (Probe Degree of Freedom)]
A \emph{dark particle} is an observer-introduced probe that couples into the manifold dynamics (injecting energy, heat, or phase pull into existing token identities), but is not itself eligible to become persistent knowledge.
\end{definition}

Operationally, this is a \emph{policy layer} on top of the physics engine. In the fixed $\omega$-lattice implementation described here, we do not yet enforce hard exclusions (e.g., ``dark particles cannot be chosen as anchors''); we treat this as future work for robust probing without query fossilization.

\paragraph{Experiment sketch (rule change).}
We evaluate observer policies that vary how strongly probes are allowed to perturb the substrate (energy injection, phase forcing, or neither). We measure switch speed under abrupt context change and stability after probe removal.

% ============================================================================
% 6. CROSS-MODAL TRANSDUCTION
% ============================================================================
\section{Cross-Modal Transduction}
\label{sec:bridge}

The Sensorium Manifold enables cross-modal transduction through frequency-based coupling in the hydrodynamic wave field $\Psi(\omega)$. The coupling structure is \emph{not} located in geometric space; it lives in frequency space and can bind particles from any modality.

\subsection{Frequency-Domain Coupling}
\label{sec:freq_coupling}

Cross-modal transduction works because the Universal Tokenizer assigns intrinsic frequencies ($\omega$) to all particles regardless of modality. A text token, an audio sample, and an image pixel all become particles with frequencies determined by their (byte, index) hash.

When particles from different modalities have nearby frequencies, they contribute support to the same region of $\Psi(\omega)$, reinforcing shared wells. This creates automatic cross-modal association:
\begin{itemize}
    \item An audio sample of a cat meowing may hash to frequencies near the text ``meow''.
    \item When both are present during training, the same $\omega$ wells become reinforced by both modalities.
    \item At inference, injecting one modality reshapes $V_{\mathrm{ext}}$ and excites shared wells, which in turn phase-align and recruit particles of the other modality.
\end{itemize}

\subsection{Bidirectional Transduction}

The coupling is inherently bidirectional. The same wave-field dynamics (\Cref{eq:gpe_omega,eq:gpe_damping}) and phase update (\Cref{eq:phase_update}) apply regardless of which particles are clamped (observed) and which are free (to be inferred).

\begin{itemize}
    \item \textbf{Text $\to$ Audio}: Clamp text particles, let audio particles relax.
    \item \textbf{Audio $\to$ Text}: Clamp audio particles, let text particles relax.
    \item \textbf{Image $\to$ Text + Audio}: Clamp image, let both text and audio relax.
\end{itemize}

The manifold does not distinguish these cases. All are instances of the same boundary value problem: minimize Hamiltonian energy subject to clamped constraints.

% ============================================================================
% 7. EXPERIMENTS
% ============================================================================
\section{Experiments}
\label{sec:experiments}

We validate the framework across five experiments that probe different aspects of the manifold's behavior: (1) the ``collision is compression'' mechanism where hash collisions create hierarchical trie structure, (2) online adaptation to distributional shifts, (3) audio waveform reconstruction, (4) multi-source audio separation, and (5) native image handling.

\subsection{Hash Collision Compression}
\label{sec:image_collision}

This experiment validates the core insight of the Universal Tokenizer: hash collisions are not errors but \emph{compression}. When multiple data streams share prefixes, they collide into the same particles, creating a thermodynamic trie that naturally clusters related patterns.

\paragraph{Method.}
We process MNIST images using the Universal Tokenizer, varying the collision rate by adjusting the hash modulus. At each collision rate, we measure: (1) spatial clustering of particles with identical token IDs, (2) energy accumulation in colliding particles, and (3) compression metrics (entropy, compression ratio).

\paragraph{Results.}
\Cref{tab:image_collision} shows that as collision rate increases, compression improves: particles with shared token IDs cluster together in 3D space, energy accumulates in shared nodes, and entropy decreases. \Cref{fig:image_collision_hero} visualizes the progressive clustering, while \Cref{fig:image_collision_bifurcation} shows the emergent trie structure---hierarchical branching where shared prefixes cluster and unique patterns diverge.

\input{tables/image_collision_summary.tex}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/image_collision_hero.png}{%
    \includegraphics[width=\textwidth]{figures/image_collision_hero.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Hash collisions create compression in a thermodynamic trie. As collision rate increases (left to right), particles with identical token IDs cluster in 3D space, energy accumulates in shared particles, and compression metrics improve. The visualization demonstrates that hash collisions naturally create hierarchical structure.}
  \label{fig:image_collision_hero}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/image_collision_bifurcation.png}{%
    \includegraphics[width=\textwidth]{figures/image_collision_bifurcation.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Bifurcation charts showing emergent trie structure. Top: hierarchical clustering dendrograms. Bottom: spatial bifurcation plots with convex hulls highlighting clusters. Higher collision rates create more pronounced branching---shared prefixes cluster together, unique patterns branch apart.}
  \label{fig:image_collision_bifurcation}
\end{figure}

\FloatBarrier

\subsection{Rule-Shift Adaptation}
\label{sec:rule_shift}

We evaluate adaptation to distributional shifts using a controlled benchmark where the sequential structure completely reverses mid-stream.

\paragraph{Method.}
The experiment uses two phrases with identical characters but reversed word order:
\begin{itemize}
  \item \textbf{Forward phase}: ``The cat sat on the mat.'' repeats 50 times
  \item \textbf{Reverse phase}: ``mat the on sat cat The.'' repeats 50 times
\end{itemize}

Both phrases are padded to a fixed segment size (24 bytes), ensuring the thermodynamic trie captures position-specific transitions. At evaluation, we test prediction accuracy using only data seen \emph{before} the current point---simulating online learning where the system must adapt incrementally.

\paragraph{Results.}
\Cref{tab:rule_shift} shows the key finding: when the rule shifts at repetition 50, accuracy immediately drops to 0\% (the forward trie cannot predict reversed patterns). However, after just 5 additional repetitions of the new pattern, accuracy recovers to 100\%. This demonstrates rapid online adaptation through open-system driving and relaxation of $\Psi(\omega)$---no gradient-based retraining required.

\input{tables/rule_shift_summary.tex}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/rule_shift.png}{%
    \includegraphics[width=\textwidth]{figures/rule_shift.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Rule-shift adaptation dynamics. \textbf{(A)} Accuracy over training repetitions; the vertical line marks the rule shift. Forward accuracy is stable at 100\%, drops to 0\% at the shift, then recovers to 100\% within 5 repetitions. \textbf{(B)} Summary statistics comparing forward and reverse phases. \textbf{(C)} Accuracy gain from the initial 0\% in the reverse phase, showing rapid adaptation.}
  \label{fig:rule_shift}
\end{figure}

\FloatBarrier  % Keep rule-shift floats in this section

\subsection{Audio Waveform Inpainting}
\label{sec:audio_exp}

This experiment tests audio sample reconstruction using the same periodic position mechanism as time-series forecasting.

\paragraph{Method.}
Audio waveforms are byte-quantized ([-1, 1] $\to$ [0, 255]) and the segment size is set to match the signal period (18 samples at 440 Hz). For periodic signals, values at the same phase position across periods should be similar. We mask random samples and reconstruct using the weighted average of values at matching segment positions in the training data.

\paragraph{Results.}
\Cref{tab:audio_gen} shows reconstruction quality across waveform types. SNR ranges from 5--10 dB depending on mask level, with lower masking yielding better results. MAE (amplitude error) increases approximately linearly with mask fraction. Exact byte accuracy is near zero because continuous values rarely match exactly---but the SNR metric confirms that reconstructed waveforms closely track the original.

\input{tables/audio_gen_summary.tex}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/audio_gen.png}{%
    \includegraphics[width=\textwidth]{figures/audio_gen.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Audio waveform inpainting. \textbf{(A)} Original vs.\ reconstructed sine wave (first 200 samples at 20\% masking). \textbf{(B)} SNR by waveform type at 20\% masking. \textbf{(C)} MAE vs.\ mask percentage for sine wave.}
  \label{fig:audio_gen}
\end{figure}

\FloatBarrier  % Keep audio floats in this section

\subsection{Cocktail Party Separation}
\label{sec:cocktail_party}

We evaluate audio source separation using a two-speaker mixture. This experiment demonstrates that the manifold can process raw audio bytes and separate them into distinct streams without any audio-specific preprocessing.

\paragraph{Method.}
The mixed audio file is tokenized byte-by-byte using the Universal Tokenizer, creating particles with frequencies determined by the (byte, position) hash. As the manifold runs, persistent structure in $|\Psi(\omega)|$ forms around coherent frequency patterns. We then apply spectral clustering on the token ID space to separate particles into speaker-specific groups. Each group is dehashed to recover the original byte values, producing separated audio streams.

\paragraph{Results.}
\Cref{tab:cocktail_party} shows that the system separates the mixture into two approximately equal streams. The separation score measures the ratio of inter-cluster to intra-cluster distance in the normalized frequency space---higher values indicate cleaner separation. Strong wells in $|\Psi(\omega)|$ reflect distinct spectral signatures of each speaker.

\IfFileExists{tables/cocktail_party_summary.tex}{%
  \input{tables/cocktail_party_summary.tex}
}{%
  \fbox{\parbox{0.9\textwidth}{\centering\vspace{0.8cm}\textit{Table will be generated by \texttt{make paper}}\vspace{0.8cm}}}
}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/cocktail_party.png}{%
    \includegraphics[width=\textwidth]{figures/cocktail_party.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Cocktail party separation via spectral clustering. \textbf{(A)} Token ID distributions for separated speakers, showing distinct frequency bands. \textbf{(B)} Temporal distribution of samples, with each color representing a different speaker stream. \textbf{(C)} Cluster statistics showing mean frequencies with standard deviation error bars; separation score quantifies inter-cluster vs intra-cluster distance.}
  \label{fig:cocktail_party}
\end{figure}

\FloatBarrier  % Keep cocktail party floats in this section

\subsection{Native Image Handling}
\label{sec:image_exp}

We show that the same unified manifold handles 2D image frequencies. Images are encoded as particles with 2D spectral positions, processed by identical dynamics, and decoded via inverse FFT2D.

\IfFileExists{tables/mnist_trie_recall_summary.tex}{%
  \input{tables/mnist_trie_recall_summary.tex}
}{%
  \fbox{\parbox{0.9\textwidth}{\centering\vspace{0.8cm}\textit{Table will be generated by \texttt{make paper}}\vspace{0.8cm}}}
}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/mnist_bytes.png}{%
    \includegraphics[width=0.9\textwidth]{figures/mnist_bytes.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{MNIST byte-level processing via Universal Tokenizer. The system encodes raw pixel bytes as position-aware tokens, enabling pattern learning without image-specific preprocessing.}
  \label{fig:mnist_bytes}
\end{figure}

\FloatBarrier  % Keep image handling floats in this section

\subsection{Cross-Modal Processing}
\label{sec:cross_modal_exp}

We demonstrate native multimodality by processing text and images simultaneously in the same manifold without modality-specific encoders.

\paragraph{Method.}
Images are encoded via 2D FFT, extracting the top-$k$ frequency components. Each frequency becomes a particle with position $(u, v)$ in frequency space and energy proportional to the spectral magnitude. Text tokens are encoded using the same hash-based tokenizer. Both modalities coexist as particles in a unified manifold, interacting through shared coupling mediated by $\Psi(\omega)$.
\par
\noindent In the present implementation, ``shared dynamics'' refers to shared thermodynamic substrate plus shared coupling through $\Psi(\omega)$ (the $\omega$-field lives on a 1D frequency lattice; 2D image frequencies are projected into the same oscillator frequency coordinate).

\paragraph{Results.}
\Cref{tab:cross_modal} shows reconstruction quality across different image patterns. Horizontal and vertical stripes achieve MSE $<$ 0.005, while checkerboard patterns achieve near-perfect reconstruction (MSE $\approx$ 0.0001). The key insight: \emph{no modality-specific processing is required}---the same thermodynamic dynamics that handle text naturally handle frequency-domain images.

\IfFileExists{tables/cross_modal_summary.tex}{%
  \input{tables/cross_modal_summary.tex}
}{%
  \fbox{\parbox{0.9\textwidth}{\centering\vspace{0.8cm}\textit{Table will be generated by \texttt{make paper}}\vspace{0.8cm}}}
}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/cross_modal.png}{%
    \includegraphics[width=\textwidth]{figures/cross_modal.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Cross-modal processing. \textbf{Top two rows}: Original and reconstructed images showing four pattern types. \textbf{(A)} Frequency-space particle distribution for horizontal stripes showing energy concentration along vertical axis. \textbf{(B)} MSE comparison across patterns. \textbf{(C)} Text-image associations demonstrating semantic binding.}
  \label{fig:cross_modal}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/cross_modal_embedding.png}{%
    \includegraphics[width=0.85\textwidth]{figures/cross_modal_embedding.png}
  }{%
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Cross-modal particles in common embedding space. Image frequencies (blue) and text tokens (red stars) coexist in a shared 3D representation. The manifold processes both modalities through identical thermodynamic dynamics, with semantic associations emerging from shared coupling in $\Psi(\omega)$.}
  % NOTE: kept figure caption text short; see main text for ω-field coupling details.
  \label{fig:cross_modal_embedding}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/frequency_particles.png}{%
    \includegraphics[width=0.7\textwidth]{figures/frequency_particles.png}
  }{%
    \fbox{\parbox{0.7\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Particle distribution in 2D frequency space. Each point represents a frequency component from the cross-modal experiment. Point size and color encode oscillator energy---higher energy indicates stronger resonance at that frequency. The cross pattern emerges from horizontal and vertical stripe images, with the DC component (center) having highest energy.}
  \label{fig:frequency_particles}
\end{figure}

\FloatBarrier  % Keep cross-modal floats in this section

\subsection{Scaling Analysis}
\label{sec:scaling_exp}

We empirically verify the scaling properties of the manifold, particularly the O($k$) latency claim and $\omega$-field structure dynamics.

\paragraph{$\omega$-Field Structure Dynamics.}
We track the emergence of strong wells in $|\Psi(\omega)|$ over 300 simulation steps. Results show rapid early-time organization followed by steady-state persistence under observation-driven forcing and controlled dissipation. Because the $\omega$ lattice is fixed, there are no discrete birth/death events; ``capacity'' corresponds to the finite resolution (number of $\omega$ bins) available to represent distinct wells.

\paragraph{O($k$) Latency Independence.}
We measure per-step latency across sequence lengths from 500 to 8,000 tokens. \Cref{tab:scaling} shows that latency remains stable at $\approx$3 ms/step with only 11\% coefficient of variation---empirically confirming that latency scales with $\omega$-mode count $k$ (field resolution) rather than sequence length $N$.

\paragraph{Generalization.}
We test structure emergence on four data types: repetitive text, semi-random patterns, natural-like text, and pure random data. All except semi-random achieve full crystallization, suggesting the manifold finds structure in both highly regular and high-entropy data.

\IfFileExists{tables/scaling_summary.tex}{%
  \input{tables/scaling_summary.tex}
}{%
  \fbox{\parbox{0.9\textwidth}{\centering\vspace{0.8cm}\textit{Table will be generated by \texttt{make paper}}\vspace{0.8cm}}}
}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/scaling_dynamics.png}{%
    \includegraphics[width=\textwidth]{figures/scaling_dynamics.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Scaling dynamics. \textbf{(A)} Peak count / active-well mass in $|\Psi(\omega)|$ over time. \textbf{(B)} Early-time transient reconfiguration of the $\omega$-field under new support. \textbf{(C)} Strong-well count vs.\ pattern count showing saturation of representable structure at fixed $\omega$ resolution. \textbf{(D)} Structure ratio across data types.}
  \label{fig:scaling_dynamics}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/scaling_compute.png}{%
    \includegraphics[width=\textwidth]{figures/scaling_compute.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Compute scaling. \textbf{(A)} Wall-clock time vs.\ particle count showing near-linear scaling. \textbf{(B)} Time vs.\ grid size (larger grids are slightly faster due to GPU optimization). \textbf{(C)} O($k$) latency test showing stable ms/step across 16$\times$ range of sequence lengths.}
  \label{fig:scaling_compute}
\end{figure}

\FloatBarrier  % Keep scaling floats in this section

\subsection{Ablation Studies}
\label{sec:ablations}

\input{tables/ablation.tex}

\FloatBarrier  % Ensure all experiment floats are placed before Related Work

% ============================================================================
% 8. RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related}

\paragraph{Dissipative Structures and Thermodynamic Computing}
Prigogine's theory of dissipative structures \citep{prigogine1977self, prigogine1978time} describes how systems far from equilibrium can maintain complex organization by continuously dissipating energy. Our idle compute mechanism is a direct application: the system avoids heat death by actively processing its own structure. Recent work on thermodynamic computing \citep{conte2019thermodynamic, boyd2022thermodynamic, whitelam2025generative} explores physical substrates for computation based on these principles; we implement them in software via GPU-accelerated field solvers and $\omega$-space hydrodynamic dynamics.

\paragraph{Oscillator Networks and the Kuramoto Model}
The Kuramoto model \citep{kuramoto1975international, strogatz2000kuramoto} describes synchronization in coupled oscillator populations and has recently been applied to deep learning. Artificial Kuramoto Oscillatory Neurons (AKOrN) \citep{miyato2025akorn} replace threshold units with oscillatory neurons that synchronize through generalized Kuramoto dynamics. Our approach is fundamentally different: rather than pairwise oscillator coupling, we use \emph{wave physics}. Particles couple through a complex hydrodynamic field $\Psi(\omega)$ that evolves via Gross-Pitaevskii dynamics, supporting interference, superposition, and tunneling---phenomena absent in Kuramoto models. The distinction is analogous to quantum mechanics vs. classical phase-locked loops.

\paragraph{Binding by Synchrony}
The hypothesis that neural oscillation synchrony solves the binding problem \citep{singer1999neuronal, engel2001dynamic, wang2010neurophysiological} provides biological grounding for our approach. Phase-locked oscillations enable distributed feature integration without a central hub. Our wave field implements this mechanism: particles with similar frequencies bind through shared wells in $\Psi(\omega)$, regardless of their spatial separation. The key difference is that our binding is mediated by wave interference rather than direct oscillator coupling.

\paragraph{Modern Hopfield Networks and Associative Memory}
Classical Hopfield networks \citep{hopfield1982neural} implement associative memory via energy minimization. Modern Hopfield networks \citep{ramsauer2021hopfield} achieve exponential storage capacity and connect to transformer attention. Recent work on sparse Hopfield networks \citep{santos2024sparse} and Hopfield Encoding Networks \citep{widrich2024hopfield} extends these capabilities. Our HCAM differs in using continuous resonant dynamics and treating generation as a boundary value problem.

\paragraph{Content-Addressable Memory in Transformers}
Recent work integrates content-addressable memory into transformers: CAMELoT \citep{liu2024camelot} adds training-free associative memory, ARMT \citep{bulatov2024armt} combines attention with segment-level recurrence for 50M+ token sequences, and Memory Mosaics \citep{liu2024memorymosaics} provide interpretable compositional memory. Our $\omega$-field modes (and their emergent wells) serve an analogous role but operate in frequency space.

\paragraph{Non-Autoregressive and Parallel Generation}
Non-autoregressive transformers \citep{gu2018nonautoregressive} generate all tokens in parallel but struggle with output dependencies. Masked diffusion language models \citep{sahoo2024mdlm, nie2024scaling} achieve competitive performance with autoregressive models while enabling parallel sampling. Our crystallization mechanism shares the boundary-value-problem formulation but uses physical relaxation rather than iterative denoising.

\paragraph{Diffusion Models and Langevin Dynamics}
Diffusion models \citep{sohl2015deep, ho2020denoising, song2021score} learn to reverse a noising process, generating samples through Langevin dynamics. Our system is also a relaxation process, but it is governed by a wavefluid in $\omega$-space (interference + tunneling) coupled to classical thermodynamics, rather than a learned score model.

\paragraph{Hamiltonian and Symplectic Neural Networks}
Hamiltonian Neural Networks \citep{greydanus2019hamiltonian} and Symplectic networks \citep{chen2020symplectic, jin2020sympnets} learn energy-conserving dynamics from data. Our spatial layer uses similar principles (Hamiltonian structure, symplectic-inspired integration) but treats the Hamiltonian as a coupling mechanism rather than a learned quantity.

\paragraph{Multimodal Architectures}
CLIP \citep{radford2021learning} and Flamingo \citep{alayrac2022flamingo} require explicit cross-modal coupling mechanisms. Unified-IO 2 \citep{lu2024unifiedio2} tokenizes all modalities into a shared space. Our Universal Tokenizer achieves modality-agnostic encoding through deterministic hashing, with cross-modal coupling emerging from shared $\omega$-space hydrodynamic dynamics.

\paragraph{Energy-Based Models}
Energy-based models \citep{lecun2006tutorial, du2021improved} define learning as energy minimization. Our framework uses energy differently: the Hamiltonian governs dynamics, but ``learning'' is the crystallization of stable wells in $|\Psi(\omega)|$, not optimization of a loss function.

\paragraph{Token Merging and Dynamic Sparsity}
Token Merging (ToMe) \citep{bolya2023token, bolya2023token_diffusion} reduces transformer cost by bipartite-matching similar tokens and averaging their representations. Our inelastic collision operator (\Cref{sec:inelastic_collision}) achieves the same computational reduction but through physics: identical tokens in the same spatial cell merge via conservation of mass, momentum, and energy. The key differences are: (i) merging is deterministic and spatially grounded via Morton-coded composite sort keys rather than learned similarity metrics, (ii) the merge conserves physical quantities rather than averaging representations, and (iii) mass accumulation creates a natural frequency/confidence signal without additional bookkeeping.

\paragraph{Locality-Sensitive Hashing in Attention}
Reformer \citep{kitaev2020reformer} uses locality-sensitive hashing (LSH) to approximate attention by routing queries and keys that hash to the same bucket. Our Morton code (\Cref{sec:morton_codes}) serves an analogous purpose---grouping spatially proximate particles for efficient interaction---but is \emph{deterministic} and \emph{collision-free}: every cell maps to a unique index, and the Z-order curve guarantees spatial locality in the linear layout. This eliminates the false-negative problem of randomized LSH schemes.

\paragraph{Hebbian Learning and Predictive Coding}
Our approach shares principles with Hebbian learning \citep{hebb1949organization}: resonance-driven support reinforces wells in $\Psi(\omega)$. Predictive coding \citep{rao1999predictive, friston2010free} models the brain as minimizing surprise; stable $\omega$-wells provide an endogenous prior that biases relaxation toward learned structure.

% ============================================================================
% 9. DISCUSSION
% ============================================================================
\section{Discussion}
\label{sec:discussion}

We reiterate the framing from \Cref{sec:introduction}: this is physics as a computing paradigm, not a physics simulation. The thermodynamic and wave-physical equations we employ are drawn from established physics, but the space they operate on is abstract and computational. We are not modeling molecules in a gas or photons in a waveguide; we are using the \emph{mathematical structure} of these systems to implement learning and inference. The question ``Is this physically realistic?'' is therefore somewhat misplaced---the goal is computation, and the physics provides the algorithmic substrate.

\subsection{What We Claim}

\begin{enumerate}
  \item \textbf{Native multimodality}: All sensory modalities can be processed by the same thermodynamic dynamics on a shared manifold.
  \item \textbf{No backpropagation}: Learning emerges from local thermodynamics coupled to global $\omega$-field dynamics regulated by homeostasis.
  \item \textbf{Online adaptation}: The system adapts continuously to streaming data and distributional shifts via observation-driven reshaping of $V_{\mathrm{ext}}$ and relaxation of $\Psi(\omega)$.
  \item \textbf{Dissipative self-organization}: Idle compute maintains structure far from equilibrium by continuing the same field dynamics under reduced external forcing.
  \item \textbf{Zero-latency scaling}: Sequence generation latency scales with relaxation complexity $O(k)$, not sequence length $O(N)$. Time is treated as a spatial dimension.
\end{enumerate}

\subsection{What We Do Not Claim}

\begin{enumerate}
  \item \textbf{Closed-system thermodynamics}: The manifold is an \emph{open} system with observation-driven energy injection and dissipation. We aim for physically grounded parameterization (e.g., $\sigma_x$ from $\lambda_T$, $\kappa \sim k_BT/\hbar$, linewidth from coherence-time considerations), but we do not claim detailed balance or exact equilibrium sampling.
  \item \textbf{Gradient-free optimization}: We are not optimizing a loss function without gradients. We sidestep optimization entirely---structure emerges from crystallization of stable wells in $|\Psi(\omega)|$.
  \item \textbf{Transformer replacement}: Our experiments are on small-scale tasks. We make no claims about scaling to language model pretraining.
  \item \textbf{Exact continuous dynamics}: The spatial layer uses discretized solvers (FFT Poisson; explicit finite-volume compressible Navier--Stokes). These are numerical approximations to continuous PDEs.
\end{enumerate}

\subsection{The Zero-Latency Paradigm}

The Sensorium Manifold exposes a fundamental inefficiency in autoregressive architectures. Transformers require $O(N)$ sequential forward passes to generate $N$ tokens---each token must wait for all preceding tokens. This serial bottleneck is intrinsic to the causal mask.

Crystallization inverts this relationship. By treating the sequence index $t$ as a spatial coordinate rather than a temporal one, the manifold solves the coupled system simultaneously: thermodynamics provides a stable substrate while the $\omega$-field provides global coupling. Constraints influence the entire relaxation field in parallel rather than only forward in an autoregressive chain.

The practical implication: generating a 10-token sentence and a 1000-token paragraph require approximately the same number of relaxation steps, assuming the energy landscape complexity is similar. Latency scales with the number of $\omega$-modes/wells that must be excited (pattern complexity), not the number of oscillators that must be resolved (sequence length).

This is \emph{Time as Space}---a geometric rather than temporal view of sequence generation.

\subsection{The Physics of ``Horizontal''}

We claimed that ``the physics of horizontal is the same whether it is a word, a sound wave, or a pixel pattern.'' Let us make this precise:

\begin{itemize}
  \item \textbf{Text}: The word ``horizontal'' is a token with a D-dimensional embedding.
  \item \textbf{Audio}: A sound panning left-to-right has specific frequency characteristics (Doppler, stereo phase).
  \item \textbf{Image}: A horizontal line has energy concentrated at $v \approx 0$ in the 2D frequency domain.
\end{itemize}

All three representations enter the manifold as particles. Through co-activation during training, shared wells in $\Psi(\omega)$ couple these representations. The word ``horizontal'' sustains wells that also respond to horizontal image frequencies and horizontal audio characteristics. This is not a metaphor---it is the mechanism.

% ============================================================================
% 10. CONCLUSION
% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We have presented the Sensorium Manifold, a unified thermodynamic substrate for native multimodal computation. By representing all sensory inputs as particles with intrinsic frequencies (via the Universal Tokenizer), we achieve modality-agnostic dynamics. Learning emerges from hydrodynamic crystallization in $\omega$-space---the spontaneous formation of persistent wave structure in $\Psi(\omega)$ that couples particles through interference and tunneling---without backpropagation.

The crystallization mechanism represents a fundamental departure from the autoregressive paradigm. By treating time as a spatial dimension and generation as a boundary value problem, we achieve latency that scales with pattern complexity rather than sequence length. The manifold functions as a Holographic Content Addressable Memory: partial inputs address complete patterns because information is distributed across the entire resonant field. ``Next token prediction'' is merely a degenerate case of this more general capability.

The framework suggests an alternative to the optimization-centric paradigm of modern machine learning. Physical principles---thermodynamics, diffusion, homeostasis, and holographic encoding---may offer paths to adaptive systems that are better suited to continuous, online, embodied learning.

The physics of ``horizontal'' really is the same across modalities. And that, perhaps, is how perception should work.

% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{plainnat}
\bibliography{references}

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\section{Additional Experiments}
\label{sec:appendix_experiments}

This appendix aggregates additional kernel experiments that exercise the same
mechanism across tasks and different sampling/observation choices.

\subsection{Universal Tokenizer Collision Regimes (TOY)}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/collision_compression_summary.png}{%
    \includegraphics[width=0.95\textwidth]{figures/collision_compression_summary.png}
  }{%
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Synthetic trie collision sweep. As collision rate increases, spatial clustering increases and compression ratio decreases, demonstrating that hash collisions act as compression.}
  \label{fig:collision_compression}
\end{figure}

\FloatBarrier

\subsection{Next-Byte Prediction via Thermodynamic Trie}
\label{sec:next_byte_prediction}

This experiment demonstrates how the Universal Tokenizer creates a \emph{thermodynamic trie}---a branching structure that emerges from controlled hash collisions. The key insight is that hash collisions are not errors but \emph{compression}: identical (byte, position) pairs across different occurrences collide into the same particle, allowing the manifold to learn statistical patterns.

\paragraph{Method.}
We train on 7 distinct text patterns (e.g., ``The cat sat.'', ``The dog ran.'') with a segment size of 16 bytes. The tokenizer resets the position counter every 16 bytes, so the character `T' at position 0 receives the same token ID regardless of which pattern instance it appears in. This creates the trie structure:

\begin{enumerate}
\item \textbf{Hash}: Each byte at position $p$ is hashed to token ID $t = (b \cdot 31 + p) \mod 4096$.
\item \textbf{Collide}: Repeated patterns produce repeated token IDs, accumulating energy in shared particles.
\item \textbf{Crystallize}: The $\omega$-field develops persistent wells that couple oscillators that co-occur, encoding transition structure.
\end{enumerate}

\paragraph{Inference.}
Given context bytes, we compute their token IDs and search for matching sequences in the trained manifold. At branch points (e.g., after ``The cat '', where `s', `r', or `a' may follow), the manifold contains particles for all valid continuations. We score candidates by their accumulated energy and select the most probable.

\paragraph{Results.}
\Cref{sec:next_byte_prediction} reports that the system achieves 99.3\% accuracy on deterministic paths (where only one continuation is valid) and 55.8\% on branch points (where it picks the most frequent continuation). The 92.4\% overall accuracy reflects the mixture of these cases. Importantly, the 99.4\% top-3 accuracy demonstrates that even at branch points, the correct answer is almost always among the top candidates---the system correctly learns the frequency distribution of continuations without any gradient-based training.

\input{tables/next_token_summary.tex}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/next_token.png}{%
    \includegraphics[width=\textwidth]{figures/next_token.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Thermodynamic trie structure for next-byte prediction. \textbf{(A)} Trie branching visualization showing candidate continuations at each position in the segment; node size encodes probability, edges show transitions between positions. \textbf{(B)} Probability distributions at branch points where multiple continuations are valid; the manifold learns frequency-proportional distributions (e.g., ``The cat '' $\to$ `s':50\%, `r':33\%, `a':17\%). \textbf{(C)} Accuracy by position in segment; green indicates deterministic paths ($>$95\%), orange indicates moderate ambiguity, red indicates branch points with multiple valid continuations.}
  \label{fig:next_token}
\end{figure}

\FloatBarrier

\subsection{Time-Series Forecasting via Position Periodicity}
\label{sec:timeseries}

This experiment tests forecasting on byte-quantized synthetic time series using position-based periodicity---the same mechanism that enables the thermodynamic trie for text.

\paragraph{Method.}
Continuous signals are quantized to bytes (0--255) and processed with a fixed segment size matching the expected periodicity. For prediction, we exploit the key insight: \emph{values at the same segment position should be similar across periods}. This is the time-series analog of the thermodynamic trie---periodic structure creates hash collisions that enable prediction.

\paragraph{Results.}
\Cref{tab:timeseries} shows that the approach works well for stationary periodic signals (sawtooth: MAE 7.9, 57\% direction accuracy; periodic: MAE 21.4, 55\% direction) but struggles with non-stationary patterns. Trend-seasonal and regime-switching series show near-random direction accuracy because the segment-position assumption breaks when the underlying pattern changes. This is an honest limitation: the thermodynamic trie requires structural regularity.

\input{tables/timeseries_summary.tex}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/timeseries.png}{%
    \includegraphics[width=\textwidth]{figures/timeseries.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Time-series forecasting results. \textbf{(A)} Actual vs.\ predicted values for the periodic signal (first 100 test points). \textbf{(B)} Direction accuracy by signal type; 50\% is random baseline. \textbf{(C)} Error distribution for periodic signal showing the concentration of predictions near true values.}
  \label{fig:timeseries}
\end{figure}

\FloatBarrier

\subsection{MNIST Classification from Raw Bytes}
\input{tables/mnist_bytes_summary.tex}
\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/mnist_bytes.png}{%
    \includegraphics[width=0.75\textwidth]{figures/mnist_bytes.png}
  }{%
    \fbox{\parbox{0.75\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Kernel MNIST confusion matrix (Universal Tokenizer hashed pixel stream).}
  \label{fig:mnist_bytes_appendix}
\end{figure}

\FloatBarrier

\subsection{MNIST Inpainting via Thermodynamic Trie}
\label{sec:mnist_inpainting}

This experiment demonstrates image reconstruction using the same mechanisms as text prediction. Pixels are treated as bytes, hashed with their 2D position to create a thermodynamic trie. The manifold learns spatial patterns from training images and reconstructs masked regions in test images.

\paragraph{Method.}
Each pixel at position $(x, y)$ is hashed to a token ID: $t = (p \cdot 31 + (y \cdot 28 + x)) \mod 4096$, where $p$ is the pixel intensity. For reconstruction, we use dual-domain inference: geometric locality (neighboring pixels) provides spatial smoothness, while $\omega$-space wells in $\Psi(\omega)$ provide pattern completion from similar images in the training set.

\paragraph{Results.}
\Cref{tab:image_gen} shows reconstruction quality degrades gracefully as mask fraction increases. At 50\% masking, the system still achieves 13.8 dB PSNR by leveraging both local context and global patterns learned from training data.

\IfFileExists{tables/image_gen_summary.tex}{%
  \input{tables/image_gen_summary.tex}
}{%
  \fbox{\parbox{0.9\textwidth}{\centering\vspace{0.8cm}\textit{Table will be generated by \texttt{make paper}}\vspace{0.8cm}}}
}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/image_gen.png}{%
    \includegraphics[width=\textwidth]{figures/image_gen.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{MNIST inpainting via thermodynamic trie. \textbf{(A)} Example reconstructions at different mask levels (top: original, bottom: reconstructed). \textbf{(B)} PSNR and MAE as functions of mask fraction, showing graceful degradation. \textbf{(C)} Error distribution histograms for each mask level.}
  \label{fig:image_gen}
\end{figure}

\FloatBarrier

\subsection{Byte Denoising via Thermodynamic Trie}
\label{sec:text_diffusion}

This experiment tests masked byte reconstruction using the thermodynamic trie---analogous to masked language modeling but operating on raw bytes.

\paragraph{Method.}
We train on clean text (repetitive phrases to enable pattern learning), then mask random positions in a held-out test segment. For each masked position, we use unmasked context bytes to build token IDs, search for matching patterns in training data, and predict the byte that most frequently follows similar contexts. This is the text equivalent of image inpainting.

\paragraph{Results.}
\Cref{tab:text_diffusion} shows that reconstruction accuracy ranges from 3--11\% across mask levels, well above the random baseline (1/256 $\approx$ 0.4\%). Accuracy is highest at low masking (10\%) where more context is available. The relatively modest accuracy reflects the challenge: text has high entropy per character, and the thermodynamic trie works best when patterns repeat exactly.

\input{tables/text_diffusion_summary.tex}

\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/text_diffusion.png}{%
    \includegraphics[width=0.9\textwidth]{figures/text_diffusion.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Byte denoising results. \textbf{(A)} Character accuracy vs.\ mask percentage with random baseline shown as dashed line. \textbf{(B)} Accuracy at each mask level, showing consistent above-random performance.}
  \label{fig:text_diffusion}
\end{figure}

\IfFileExists{tables/text_diffusion_examples.tex}{%
  \input{tables/text_diffusion_examples.tex}
}{}

\FloatBarrier

\subsection{Continuous Kernel Simulation Snapshot}
\begin{figure}[!htbp]
  \centering
  \IfFileExists{figures/continuous_final.png}{%
    \includegraphics[width=\textwidth]{figures/continuous_final.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Final dashboard frame from a finite kernel simulation run.}
  \label{fig:continuous_final}
\end{figure}

\FloatBarrier

\section{Extended Translation Table}
\label{app:translation}

For readers seeking deeper correspondences:

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l l}
\toprule
\textbf{Physics Concept} & \textbf{ML Analogue} \\
\midrule
3D Manifold space & Simulation volume / embedding space \\
Particle (spatial layer) & Token / feature vector \\
Particle (wave layer: $\omega_i, \theta_i$) & Hidden state with phase/frequency \\
Wave field $\Psi(\omega)$ & Distributed representation / weights \\
Wave field bin $\Psi_k$ & Learned mode / pattern detector \\
Tuning kernel $T_{ik}$ & Attention weight (frequency-based) \\
Mode linewidth $\gamma_k$ & Frequency selectivity / specialization \\
Wave crystallization (persistent wells) & Memory consolidation \\
Wave interference & Mode superposition / competition \\
Anchored particles & Associated tokens in memory \\
Phase-torque feedback & Prior / completion signal \\
Gravity field $\phi$ & Attraction potential \\
Heat diffusion & Uncertainty propagation \\
Particle collision & Local interaction / gating \\
Inelastic merge & Token merging / dynamic sparsity \\
Morton code (Z-order) & Locality-sensitive hashing \\
Composite sort key & Deterministic token binning \\
Mass accumulation & Pattern frequency / confidence \\
Langevin noise & Stochastic exploration / dropout \\
Idle compute & Offline consolidation / dreaming \\
Crystallization & All-token generation (cf. autoregression) \\
Holographic CAM & Content-addressable associative memory \\
Time as Space & Sequence index as spatial coordinate \\
Boundary Value Problem & Constrained generation (inpainting) \\
\bottomrule
\end{tabular}
\end{center}

\section{Pseudocode}
\label{app:algorithms}

\begin{algorithm}[H]
\caption{Unified Manifold Step (Thermodynamics + Wave Dynamics)}
\label{alg:unified_step}
\begin{algorithmic}[1]
\Require Particles $\{(\bm{x}_i, \bm{v}_i, \eoscI{i}, \heat_i, \omega_i, \theta_i)\}$, $\omega$-field $\{\Psi_k(\omega_k)\}_{k=1}^M$
\State \textbf{// Spatial Layer (Thermodynamic Physics)}
\State Scatter conserved quantities to grid (PIC): $\rho,\ \rho\bm{u},\ e_{\text{int}}$
\State Solve $\nabla^2 \phi = 4\pi G \rho$ (FFT Poisson); compute $\bm{g}=-\nabla\phi$
\State (Optional) Advance internal-energy Navier--Stokes on grid (RK2 + LLF flux; viscosity + conduction; EOS)
\State Gather $(\bm{u},T)$ to particles (PIC); advect $\bm{x}_i \leftarrow \bm{x}_i + \bm{u}(\bm{x}_i)\dt$ (periodic wrap)
\State Update particle thermal store via $T_i=\heat_i/(m_i c_v)$ (keeping $\eosc$ separate)
\State Compute Morton-coded composite sort keys $\text{Key}_i = (\mathcal{Z}(\text{cell}_i) \ll 8) \,|\, \text{byte}_i$
\State Sort particles by composite key (radix sort); merge identical keys via inelastic collision (\Cref{def:merge})
\State Local conservative exchange: $\heat_i \leftrightarrow \eoscI{i}$ via Planck relaxation (\Cref{eq:planck_energy}, \Cref{eq:planck_alpha})
\State \textbf{// Wave Layer ($\omega$-Hydrodynamics)}
\State Compute particle amplitudes: $A_i = \sqrt{\eoscI{i}}$
\State Accumulate support $w_k$ for each $\omega$ bin from particles (resonance + overlap)
\State Set external potential $V_{\mathrm{ext},k} \leftarrow -w_k$
\State Update wave field $\Psi(\omega)$ by split-step GPE (\Cref{eq:gpe_omega,eq:gpe_damping})
\State Update particle phases by wave-field torque (\Cref{eq:phase_update})
\end{algorithmic}
\end{algorithm}

\end{document}
