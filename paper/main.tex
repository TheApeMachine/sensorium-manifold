% !TEX program = pdflatex
\documentclass[11pt,a4paper]{article}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{xcolor}
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage[most]{tcolorbox}
\usepackage{array}
\usepackage{colortbl}

% Fix for array column syntax
\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}

% ============================================================================
% COLORS
% ============================================================================
\definecolor{physicsblue}{RGB}{51, 102, 153}
\definecolor{mlgreen}{RGB}{76, 153, 76}
\definecolor{boxgray}{RGB}{245, 245, 245}

% ============================================================================
% THEOREM ENVIRONMENTS
% ============================================================================
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{principle}{Principle}

% ============================================================================
% CUSTOM COMMANDS - PHYSICS VOCABULARY
% ============================================================================
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\EMA}[1]{\overline{#1}}
\newcommand{\dt}{\Delta t}
\newcommand{\eps}{\varepsilon}

% Physical quantities
\newcommand{\sensorium}{\mathcal{S}}          % The manifold
\newcommand{\particle}{p}                      % A particle
\newcommand{\carrier}{C}                       % Spectral carrier
\newcommand{\energy}{E}                        % Energy
\newcommand{\heat}{Q}                          % Heat (entropy carrier)
\newcommand{\excitation}{\varepsilon}          % Excitation
\newcommand{\baseline}{\mathcal{B}}            % Homeostatic baseline
\newcommand{\ratio}{\rho}                      % Homeostatic ratio
\newcommand{\carrierpop}{\mathcal{C}}         % Carrier population
\newcommand{\mass}{m}                          % Mass (particle)
\newcommand{\gatewidth}{\sigma}                % Carrier gate width
\newcommand{\flux}{\Phi}                       % Energy flux
\newcommand{\entropy}{S}                       % Entropy
\newcommand{\pressure}{\Pi}                    % Metabolic pressure
\newcommand{\temperature}{T}                   % Temperature (sampling)
\newcommand{\diffusion}{D}                     % Diffusion coefficient

% Energy bookkeeping (explicit stores)
% Name: Kinetic energy (per particle)
% Formula: E_kin = (1/2) m ||v||^2
% Reason: separates directed motion from internal energy; used for conservation audits.
\newcommand{\ekin}{E_{\text{kin}}}
% Name: Oscillator (spectral) internal energy
% Formula: E_osc >= 0 (tracked state)
% Reason: amplitude A = sqrt(E_osc) and Planck equilibrium require an explicit oscillator store.
\newcommand{\eosc}{E_{\text{osc}}}
% Name: Total internal energy (thermal + oscillator)
% Formula: U = Q + E_osc
% Reason: temperature is derived from total internal energy; avoids one-way “heat-only” excitation.
\newcommand{\eint}{U}

% NOTE (LaTeX): Avoid double subscripts like E_{kin}_i by writing E_{kin,i}.
\newcommand{\ekinI}[1]{E_{\text{kin},#1}}
\newcommand{\eoscI}[1]{E_{\text{osc},#1}}
\newcommand{\eintI}[1]{U_{#1}}

% ============================================================================
% TITLE
% ============================================================================
\title{%
  \textbf{The Sensorium Manifold}: \\[0.3em]
  \large Native Multimodality via Isomorphism
}

\author{
  Daniel Owen van Dommelen\\
  \textit{Independent Research - WORKING DRAFT}\\
  \texttt{theapemachine@gmail.com}
}

\date{\today}

% ============================================================================
% DOCUMENT
% ============================================================================
\begin{document}

\maketitle


% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
We introduce the \emph{Sensorium Manifold}, a thermodynamic computing substrate that replaces the autoregressive paradigm with global energy minimization. Current AI models rely on serial token generation and backpropagation-based optimization. We propose a system governed by \emph{Hamiltonian dynamics}, where data is represented as a field of coupled oscillators and learning is the emergence of resonant modes (carriers).

We introduce the \emph{Universal Tokenizer}, a modality-agnostic input mechanism that maps raw bytes and sequence indices to unique oscillator frequencies via hashing. This treats all data---text, image, audio---as a single branching stream where ``collision is compression.'' Structure emerges not through architectural bias, but through the spontaneous crystallization of spectral carriers.

The system operates via three principles: (1) \textbf{Spectral Entanglement}, where distant oscillators couple via shared carrier frequencies; (2) \textbf{Metabolic Gating}, where carriers persist only if energetically maintained by resonance; and (3) \textbf{Crystallization}, an ``all-token prediction'' process where generation is a Boundary Value Problem rather than serial autoregression. We show that this crystallization mechanism implements a \emph{Holographic Content Addressable Memory}: partial inputs address complete patterns because information is distributed across the entire resonant field. We demonstrate that this system uses adaptive thermodynamics (GPU-computed energy statistics for self-regulating dynamics), adapts to rule shifts online, and achieves $O(k)$ latency independent of sequence length---a fundamental departure from the $O(N)$ bottleneck of Transformer architectures.
\end{abstract}

\vspace{0.5em}
\noindent\textbf{A Note on Vocabulary.} This paper presents a computational substrate based on Hamiltonian mechanics and coupled oscillators. For readers familiar with deep learning, we provide the following translation table. Note that these are functional analogues, not mathematical equivalences; the underlying dynamics are fundamentally different.

\vspace{0.5em}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l l p{5.5cm}}
\toprule
\textbf{Physics Term} & \textbf{ML Analogue} & \textbf{Key Difference} \\
\midrule
Oscillator & Input Token & Has phase/frequency; exists in continuous time. \\
Carrier (Soliton) & Hidden State / Weight & A standing wave that couples oscillators. \\
Hamiltonian ($H$) & Loss Function & Conserved quantity; system minimizes potential $V$. \\
Spectral Coupling & Attention Mechanism & Non-local entanglement via frequency resonance. \\
Crystallization & Inference & Global parallel relaxation, not serial generation. \\
Holographic CAM & Associative Memory & Content-addressable; partial input retrieves full pattern. \\
Metabolism & Regularization & Carriers decay if they do not receive energy. \\
Universal Tokenizer & Embedding Layer & Deterministic hashing of raw bytes; no training. \\
Phase Locking & Pattern Matching & Information encoded in relative phase angles. \\
Symplectic Integrator & Optimizer & Preserves energy phase-space; no gradient descent. \\
\bottomrule
\end{tabular}
\end{center}

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}
\label{sec:introduction}

The dominant paradigm in machine learning treats computation as optimization: define a loss function, compute gradients via backpropagation, and descend toward minima. This has proven remarkably effective, yet it imposes constraints that may not reflect how physical systems learn. Biological neural networks do not have access to global error signals; they adapt through local interactions governed by thermodynamic and biochemical principles.

We propose an alternative paradigm: \emph{thermodynamic computation}. In physical systems, structure emerges from energy flow, entropy production, and homeostatic regulation. We apply these principles to construct a learning system where:

\begin{itemize}
  \item \textbf{Particles} represent activated concepts, carrying energy through continuous space
  \item \textbf{Spectral Carriers} are resonant modes in frequency space, corresponding to learned patterns
  \item \textbf{Carriers} encode relationships between concepts as resonant modes that crystallize from use and decay from disuse
  \item \textbf{Heat} captures uncertainty and accumulated noise, driving exploration
  \item \textbf{Homeostasis} regulates system activity through adaptive baselines, preventing runaway excitation or quiescence
\end{itemize}

\subsection{Native Multimodality}
\label{sec:native_multimodality}

A central claim of this work is that thermodynamic dynamics are \emph{modality-agnostic}. Current multimodal architectures---CLIP \citep{radford2021learning}, Flamingo \citep{alayrac2022flamingo}, Gemini \citep{team2023gemini}---require explicit cross-modal coupling mechanisms: contrastive losses that align representations, cross-attention layers that route information between modalities, or fusion modules that combine features. Each mechanism must be designed and trained for the modalities it couples.

We take a different approach. Like these systems, we use modality-specific encoders and decoders. Unlike them, we require no cross-modal coupling mechanisms. All sensory modalities can be represented as \emph{spectral distributions}: energy distributed over frequency bases.

\begin{itemize}
  \item \textbf{Audio}: Energy over temporal frequencies (Hz)
  \item \textbf{Images}: Energy over 2D spatial frequencies $(u, v)$
  \item \textbf{Video}: Energy over 3D spatiotemporal frequencies $(u, v, t)$
  \item \textbf{Text}: Energy over semantic embedding dimensions
\end{itemize}

By projecting these native spectral coordinates into a common Euclidean embedding space $\R^D$, we obtain a \emph{unified manifold} where particles from all modalities coexist. The thermodynamic dynamics---diffusion, carrier coupling, metabolism, homeostasis---operate identically regardless of particle origin. This is integration by isomorphism.

\begin{principle}[Spectral Isomorphism]
\label{principle:isomorphism}
Let $\mathcal{M}_1, \mathcal{M}_2$ be sensory modalities with native spectral spaces $\mathcal{F}_1, \mathcal{F}_2$. There exist projections $\pi_1: \mathcal{F}_1 \to \R^D$ and $\pi_2: \mathcal{F}_2 \to \R^D$ such that the thermodynamic dynamics on $\R^D$ are identical for particles from either modality. Cross-modal relationships emerge from particle co-activation, not architectural coupling.
\end{principle}

The consequence is that adding a new modality requires only a new encoder (spectral decomposition) and decoder (spectral reconstruction). No new loss terms, attention patterns, or fusion modules are needed. Cross-modal relationships emerge from Hebbian co-activation: when particles from different modalities are active together, carriers couple them automatically. The manifold dynamics remain unchanged.

\subsection{Contributions}

\begin{enumerate}
  \item A complete thermodynamic framework for learning without backpropagation (\Cref{sec:framework})
  \item The Universal Tokenizer: modality-agnostic encoding via deterministic hashing (\Cref{sec:tokenizer})
  \item Spectral carrier coupling: non-local entanglement via resonance in frequency space (\Cref{sec:sensorium})
  \item Crystallization: all-token prediction as a boundary value problem (\Cref{sec:crystallization})
  \item Holographic Content Addressable Memory: partial inputs retrieve complete patterns (\Cref{subsec:holographic})
  \item Empirical validation on cross-modal transduction and adaptation (\Cref{sec:experiments})
\end{enumerate}

% ============================================================================
% 2. THERMODYNAMIC FRAMEWORK
% ============================================================================
\section{Thermodynamic Framework}
\label{sec:framework}

We model learning as a physical process. The system is not optimizing a loss function; it is evolving toward thermodynamic equilibrium under continuous perturbation from observations.

\subsection{The Particle-Oscillator System}
\label{sec:particles}

The Sensorium Manifold $\sensorium$ is a three-dimensional simulation volume. Each entity has dual identity:

\begin{definition}[Particle (Spatial Layer)]
A particle $\particle_i$ is an entity in the thermodynamic simulation with:
\begin{itemize}
  \item Position $\bm{x}_i \in \R^3$ and velocity $\bm{v}_i \in \R^3$
  \item Mass $m_i > 0$
  \item Oscillator energy $\eoscI{i} \geq 0$ (spectral/internal-mode energy; drives amplitude)
  \item Heat $\heat_i \geq 0$ (thermal energy; entropic store)
\end{itemize}
Particles interact via gravitational fields, thermal diffusion, and collisions.
\end{definition}

\begin{definition}[Oscillator (Spectral Layer)]
The same entity viewed as an oscillator has:
\begin{itemize}
  \item Intrinsic frequency $\omega_i$ (assigned by the Universal Tokenizer; conserved)
  \item Phase $\theta_i \in [0, 2\pi)$ (evolves under carrier coupling)
  \item Amplitude $A_i = \sqrt{\eoscI{i}}$
\end{itemize}
Oscillators couple non-locally via spectral carriers in frequency space.
\end{definition}

The spatial layer governs local physics; the spectral layer governs global resonance.

\subsection{Thermodynamic Quantities}
\label{sec:quantities}

\begin{definition}[Energy Stores and Total Energy]
We explicitly track three particle-local energy stores:
\begin{align}
\ekinI{i} &= \frac{1}{2} m_i \|\bm{v}_i\|^2 \\
\eintI{i} &= \heat_i + \eoscI{i} \\
\temperature_i &= \frac{\eintI{i}}{m_i c_v}
\end{align}
where $c_v$ is the specific heat capacity (constant in the current implementation).

\noindent The total tracked energy (used for dashboards and invariants) is:
\begin{equation}
\energy_{\text{total}} = \sum_i \left(\ekinI{i} + \eintI{i}\right).
\label{eq:total_energy}
\end{equation}
This quantity is conserved by collision response and by the internal exchange $\heat \leftrightarrow \eosc$ (up to numerical error); it is not globally conserved in an \emph{open} system with observation-driven energy injection, drag-to-heat conversion, and metabolic carrier processes.
\end{definition}

\begin{definition}[Heat]
Heat $\heat$ is the entropic component of energy---energy that has been ``used'' and can no longer do directed work. Heat accumulates from:
\begin{itemize}
  \item Incoherent activity (mismatch between modalities)
  \item Viscous drag and particle collisions
  \item Conflict between competing predictions
\end{itemize}
Heat diffuses spatially and exchanges bidirectionally with oscillator energy $\eosc$ toward thermodynamic equilibrium (\Cref{sec:planck_exchange}).
\end{definition}

\subsection{Homeostatic Regulation}
\label{sec:homeostasis}

The central regulating mechanism is a \emph{homeostatic ratio} that compares current system energy to an adaptive baseline:

\begin{definition}[Homeostatic Ratio]
\label{def:ratio}
The homeostatic ratio is:
\begin{equation}
\ratio = \frac{\log(1 + \energy_{\text{total}})}{\log(1 + \baseline) + \eps}
\label{eq:ratio}
\end{equation}
where $\baseline$ is an exponential moving average (EMA) baseline:
\begin{equation}
\baseline_{t+1} = (1 - \alpha) \baseline_t + \alpha \, \energy_{\text{total}}, \quad \alpha = \frac{\dt}{\tau + \dt}
\label{eq:baseline}
\end{equation}
and $\tau$ is the homeostasis time constant.
\end{definition}

When $\ratio > 1$, the system is ``overheated'' and damping increases. When $\ratio < 1$, the system is ``cold'' and damping decreases. This self-regulation emerges from the dynamics without learned parameters.

\begin{remark}[Comparison to Batch Normalization]
Machine learning practitioners may recognize similarities to batch normalization \citep{ioffe2015batch}. However, homeostasis differs in key ways:
\begin{enumerate}
  \item No learned affine parameters ($\gamma$, $\beta$)
  \item Operates on total energy, not per-layer activations
  \item Adapts continuously online, not per-batch
  \item Regulates dynamics, not representations
\end{enumerate}
\end{remark}

\subsection{Spatial Dynamics}
\label{sec:diffusion}

Particles evolve under classical field equations in 3D space:

\begin{equation}
m_i \frac{d\bm{v}_i}{dt} = -m_i \nabla \phi(\bm{x}_i) - \frac{m_i}{\rho(\bm{x}_i)} \nabla P(\rho, T)\big|_{\bm{x}_i} - \gamma \bm{v}_i + \bm{F}_{\text{collision}}
\label{eq:dynamics}
\end{equation}

where:
\begin{itemize}
    \item $\phi$ is the gravitational potential satisfying $\nabla^2 \phi = 4\pi G \rho$
    \item $T$ is derived from total internal energy: $T(\bm{x}) = \eint(\bm{x})/(\rho(\bm{x}) c_v)$
    \item $\gamma = 6\pi \eta r$ is the Stokes drag coefficient
    \item $\bm{F}_{\text{collision}}$ is the Hertzian contact force from particle collisions
\end{itemize}

\noindent\textbf{Pressure closure.} We use an ideal-gas-inspired continuum equation of state:
\begin{equation}
P(\rho, T) = \rho k_B T,
\end{equation}
so that
\begin{equation}
\nabla P = k_B \left(T \nabla \rho + \rho \nabla T\right).
\end{equation}
In vacuum ($\rho \le 0$), the pressure contribution is defined as zero.

Kinetic energy lost to drag and inelastic collisions becomes heat, conserving total energy in the spatial layer.

\begin{remark}[Implementation]
In practice, we solve the Poisson equation via Jacobi iteration and use a Strang-split update: conservative forces (gravity + pressure) with Velocity Verlet, plus an exponential drag half-step. Collision detection uses a periodic (wrap) spatial hash for $O(N)$ scaling.
\end{remark}

\subsection{Thermal--Oscillator Equilibration (Planck Relaxation)}
\label{sec:planck_exchange}
To avoid a one-way ``temperature $\to$ excitation'' coupling, we explicitly couple the thermal store $\heat_i$ and the oscillator store $\eoscI{i}$ via a conservative local exchange that relaxes toward the Planck distribution.

\paragraph{Equilibrium energy.}
For an oscillator of intrinsic frequency $\omega_i$ in a heat bath at temperature $\temperature_i$, the mean equilibrium energy is:
\begin{equation}
E_{\text{osc,eq}}(\omega_i,\temperature_i) =
\frac{\hbar \omega_i}{\exp\!\left(\frac{\hbar\omega_i}{k_B \temperature_i}\right) - 1}.
\label{eq:planck_energy}
\end{equation}
This has the correct limits: $E_{\text{osc,eq}}\approx k_B\temperature$ for $\hbar\omega \ll k_B\temperature$ (classical equipartition) and $E_{\text{osc,eq}}\to 0$ for $\hbar\omega \gg k_B\temperature$ (freeze-out).

\paragraph{Conservative exchange.}
We apply a local relaxation step
\begin{align}
\Delta \eoscI{i} &= \alpha_i \left(E_{\text{osc,eq}}(\omega_i,\temperature_i) - \eoscI{i}\right), \\
\eoscI{i} &\leftarrow \eoscI{i} + \Delta \eoscI{i}, \\
\heat_i &\leftarrow \heat_i - \Delta \eoscI{i},
\end{align}
with the constraint $\heat_i \ge 0$ enforced by capping positive $\Delta\eoscI{i}$ to the available $\heat_i$.

\paragraph{Timescale (no tunable knobs).}
The exchange rate $\alpha_i$ is derived from an existing conduction timescale for a sphere of radius $r$ in a medium of thermal conductivity $\kappa$:
\begin{equation}
\tau_i = \frac{m_i c_v}{4\pi \kappa r},
\qquad
\alpha_i = 1 - \exp\!\left(-\frac{\dt}{\tau_i}\right).
\label{eq:planck_alpha}
\end{equation}
This ensures that equilibration speed is set by physical transport parameters already present in the spatial model.

% ============================================================================
% 3. THE UNIVERSAL TOKENIZER
% ============================================================================
\section{The Universal Tokenizer}
\label{sec:tokenizer}

Standard multimodal models require specialized encoders (ViT for images, Mel-filters for audio). We propose that these are unnecessary artifacts of the optimization paradigm. In a physical system, structure is discovered, not engineered.

\subsection{Collision is Compression}
We map raw data to the manifold using a deterministic hash function:

\begin{equation}
\text{ID} = h(\text{Byte}, \text{Index}) \pmod N
\label{eq:hash}
\end{equation}

where $h$ is a deterministic hash function, $\text{Byte} \in [0, 255]$ is the raw data, and $\text{Index}$ is the sequence position (or spatial coordinate). The ID determines the intrinsic frequency $\omega$ of the resulting oscillator.

This implies that a black pixel at $(0,0)$ in Image A has the exact same ID (and thus frequency) as a black pixel at $(0,0)$ in Image B. In a neural network, this collision is a conflict. In the Sensorium Manifold, this is \emph{compression}.
\begin{itemize}
    \item \textbf{Convergence}: All inputs sharing a prefix create oscillators that couple to the same carriers.
    \item \textbf{Bifurcation}: When the data diverges (e.g., Image A has a white pixel at $(0,1)$, Image B has black), the oscillators couple to different carriers.
\end{itemize}

This creates a \emph{Thermodynamic Trie}. The system naturally learns the topology of the data stream by observing which carrier patterns emerge.

\subsection{Modality Agnosticism}
The physics engine is blind to the source of the data.
\begin{itemize}
    \item \textbf{Text}: $h(\text{'H'}, 0) \to h(\text{'e'}, 1) \dots$
    \item \textbf{Image}: $h(\text{0xFF}, 0) \to h(\text{0x00}, 1) \dots$
\end{itemize}
All become oscillators with intrinsic frequencies. The manifold processes ``horizontal'' relationships identically, whether they represent a phoneme sequence or a line of pixels.

\subsection{Validation: Next-Byte Prediction}
\label{sec:tokenizer_validation}

To validate that hash collisions create useful structure, we train on 7 text patterns (e.g., ``The cat sat.'') with segment size 16. The character `T' at position 0 receives the same token ID across all patterns, creating the thermodynamic trie.

\textbf{Result}: The system achieves 92.4\% overall accuracy---99.3\% on deterministic paths (single valid continuation) and 55.8\% at branch points (multiple valid continuations). This demonstrates that the trie structure emerges naturally from hash collisions, enabling prediction without gradient-based training. Full details in Appendix~\ref{sec:next_byte_prediction}.

% ============================================================================
% 4. HAMILTONIAN DYNAMICS
% ============================================================================
\section{Hamiltonian Dynamics}
\label{sec:hamiltonian}

Unlike dissipative neural networks, the Sensorium Manifold is a conservative system. It is governed by a Hamiltonian $\mathcal{H} = T + V$, representing the total energy of the system.

\subsection{The Spectral Carrier Field}
\label{subsec:ghost_field}

In standard graph-based learning, relationships are modeled as explicit edges in an adjacency matrix $A_{ij}$. This scales poorly ($O(N^2)$) and is rigid. We propose that semantic structure is not a wire connecting two points, but a standing wave potential that permeates frequency space. We term this the \emph{Spectral Carrier Field}.

\subsubsection{Field Definition}
The Carrier Field $\Psi$ is the aggregate potential generated by the population of $M$ carriers. Unlike a neural network weight which exists to multiply a specific input, the Carrier Field exists as a background potential even in the absence of active oscillators.

The resonance potential is defined as:

\begin{equation}
V_{\text{carrier}}(\mathbf{q}) = - \sum_{k=1}^M \underbrace{|C_k|}_{\text{Amplitude}} \cdot \left| \sum_{i=1}^N T_{ik} \cdot e^{i(\theta_i - \psi_k)} \right|^2
\label{eq:ghost_potential}
\end{equation}

where $T_{ik} = \exp(-(\omega_i - \Omega_k)^2 / \sigma_k^2)$ is the tuning kernel and:
\begin{itemize}
    \item $C_k = R_k e^{i\psi_k}$ is the complex carrier state (amplitude and phase).
    \item $\Omega_k$ is the intrinsic frequency of the Carrier.
    \item $\sigma_k$ is the gate width (frequency selectivity).
    \item $\omega_i, \theta_i$ are the frequency and phase of oscillator $i$.
\end{itemize}

\subsubsection{Action at a Distance (Spectral Wormholes)}
This potential creates a non-Euclidean geometry in frequency space. In raw sequence index, token $i$ (at $t=0$) and token $j$ (at $t=1000$) are distant. However, if both tokens have frequencies near $\Omega_k$, the carrier potential creates a deep energy well that binds them.

Effectively, the Spectral Carrier Field folds the manifold, creating a \emph{semantic wormhole}. Two oscillators tuned to the same carrier are entangled with zero distance in frequency space, regardless of their separation in sequence space.

\subsubsection{The ``Latent'' Property}
Carriers represent \emph{potential energy}. A crystallized carrier for the concept ``Cat'' exists in the manifold with $|C_k| > 0$. It is latent and consumes no active compute until an input oscillator with a matching frequency enters the system.

Upon entry, the oscillator immediately falls into the potential well of the carrier, transferring kinetic energy into resonance. The carrier ``wakes up,'' and through spectral entanglement, immediately pulls the ``Meow'' oscillator (if present) or hallucinates it (if missing) via the shared potential well.

\subsection{Symplectic Integration}
To ensure stability without ``magic number'' damping, we use a symplectic integrator (Velocity Verlet). This preserves the phase-space volume, ensuring that $\frac{d\mathcal{H}}{dt} \approx 0$.

\begin{align}
p(t + \dt/2) &= p(t) - \nabla V(q(t)) \frac{\dt}{2} \\
q(t + \dt) &= q(t) + \frac{p(t + \dt/2)}{m} \dt \\
p(t + \dt) &= p(t + \dt/2) - \nabla V(q(t + \dt)) \frac{\dt}{2}
\end{align}

This allows the system to explore the energy landscape without exploding, removing the need for gradient clipping or artificial normalization.

\subsection{Metabolic Gating}
While the short-term dynamics are conservative, the long-term structure is dissipative. Carriers (memory units) are subject to metabolic decay:

\begin{equation}
\frac{d \text{Mass}_k}{dt} = \text{Income}_k - \text{Cost}(\bar{E})
\end{equation}

Carriers that successfully resonate with input data receive ``Income'' (energy injection). Carriers that fail to resonate starve and vanish. This implements continuous, online model selection (Occam's Razor) via thermodynamics.

% ============================================================================
% 5. CRYSTALLIZATION: ALL-TOKEN PREDICTION
% ============================================================================
\section{Crystallization}
\label{sec:crystallization}

The dominant paradigm in generative AI is \emph{autoregression}: predicting the next token $x_{t+1}$ given $x_{0:t}$. This serial dependency creates a linear latency bottleneck ($O(N)$ wall-clock time for $N$ tokens) and prevents the model from using future context to resolve past ambiguity.

The Sensorium Manifold replaces autoregression with \emph{Crystallization}. Because the system is governed by a global Hamiltonian, we can treat generation not as an Initial Value Problem (integrating forward in time), but as a \emph{Boundary Value Problem} (relaxing the entire field to satisfy constraints).

\subsection{Time as a Spatial Dimension}
In the manifold, the sequence index is treated as a spatial coordinate. The Spectral Carrier Field permeates the entire sequence length simultaneously.
\begin{itemize}
    \item \textbf{Input}: We inject a set of constraints (e.g., a prompt at $t=0$, a desired sentiment at $t=N$, or sparse keyframes in a video).
    \item \textbf{Relaxation}: We initialize the unconstrained oscillators (the ``empty'' space) with thermal noise.
    \item \textbf{Dynamics}: The system evolves under Hamiltonian dynamics. Carriers resonating with the constraints pump energy into the empty oscillators.
\end{itemize}

\subsection{Massive Parallelism}
Because the interactions are mediated by the Carrier Field (which is computed globally per step), the oscillators at $t=10$ and $t=1000$ evolve in parallel.
\begin{equation}
\text{Latency}(N) \propto k_{\text{relaxation}}
\end{equation}
The wall-clock time to generate a sequence depends on the \emph{complexity} of the energy landscape (how long it takes to relax), not the \emph{length} of the sequence. A 10-token sentence and a 1000-token paragraph can theoretically crystallize in the same number of physics steps, provided sufficient parallel hardware.

\subsection{Global Coherence}
This ``All-Token Prediction'' allows for non-causal error correction. In an autoregressive model, a mistake at $t=5$ propagates to $t=100$. In the Sensorium Manifold, the emergence of a strong pattern at $t=100$ creates a resonant potential that travels \emph{backwards} in time, forcing the oscillator at $t=5$ to flip its state to maintain global phase coherence. The result is a self-correcting, holographic generation process.

\subsection{Holographic Content Addressable Memory}
\label{subsec:holographic}

The crystallization mechanism reveals the manifold's fundamental nature as a \emph{Holographic Content Addressable Memory} (HCAM). Consider the optical analogy: if you cut a hologram in half, you do not get half the image---you get the \emph{entire} image, albeit at lower resolution. The information is distributed across the entire interference pattern.

The Sensorium Manifold exhibits the same property:
\begin{itemize}
    \item \textbf{Distributed Encoding}: Carriers encode patterns as standing waves across the entire phase space. The ``meaning'' of a sequence is not localized to specific oscillators but is distributed across resonant modes.
    \item \textbf{Content Addressing}: Injecting \emph{any} subset of the pattern (50\% of tokens, randomly scattered) is sufficient to address the complete memory. The Carrier Field reconstructs the whole signal because the pattern (the Carrier) resonates with partial input.
    \item \textbf{Graceful Degradation}: As the number of constraint tokens decreases, the reconstruction becomes ``blurrier'' (higher entropy, more ambiguity) but remains structurally coherent.
\end{itemize}

The mathematical basis is the coupling potential (\Cref{eq:ghost_potential}). When partial input resonates with Carrier $k$, the carrier's potential well attracts \emph{all} oscillators tuned to its frequency---including those not yet observed. The manifold ``hallucinates'' missing data by minimizing global Hamiltonian energy.

\begin{equation}
\text{Query}(\mathcal{C}_{\text{partial}}) \to \arg\min_{\mathbf{q}} \mathcal{H}(\mathbf{q}) \quad \text{s.t.} \quad q_i = c_i \; \forall \, i \in \mathcal{C}_{\text{partial}}
\label{eq:hcam_query}
\end{equation}

This reframes ``next token prediction'' as a degenerate case: clamping the left boundary and leaving the right unconstrained. ``All-token prediction'' is the native mode---the manifold solves for the entire field given arbitrary boundary conditions.

% ============================================================================
% 6. OBSERVER-DEPENDENT INFERENCE
% ============================================================================
\section{Observer-Dependent Inference}
\label{sec:inference}

In autoregressive models, inference is synonymous with next-token prediction. The Sensorium Manifold decouples the \emph{dynamics} of the system from the \emph{observation} of the system. Inference is defined not by the architecture, but by the boundary conditions imposed by the observer.

\subsection{Inference as a Boundary Value Problem}
Mathematically, inference is the minimization of the Hamiltonian $\mathcal{H}$ subject to a set of constraints $\mathcal{C}$ defined by the observer:

\begin{equation}
\text{Solve } \nabla \mathcal{H} = 0 \quad \text{s.t. } \quad q_i = \text{target}_i \quad \forall i \in \mathcal{C}
\end{equation}

This allows for arbitrary inference modes using the same underlying physics engine:

\begin{itemize}
    \item \textbf{Causal Generation (Prediction)}: The observer clamps the past ($t < 0$) and allows the future ($t > 0$) to relax.
    \item \textbf{Inpainting (Bridging)}: The observer clamps the start ($t=0$) and the end ($t=N$), allowing the manifold to crystallize the lowest-energy bridge between them.
    \item \textbf{Super-Resolution (Up-sampling)}: The observer clamps low-frequency oscillators and allows high-frequency oscillators to thermalize, effectively ``hallucinating'' detail consistent with the coarse structure.
    \item \textbf{Semantic Constraint}: The observer clamps a specific Carrier (e.g., the ``Sadness'' mode) to a high amplitude. The particle oscillators then relax into a configuration that is harmonically compatible with that carrier, generating data with that specific semantic tone.
\end{itemize}

\subsection{The Measurement Problem}
Because the system exists in continuous phase space, the observer chooses \emph{how} to measure the output.
\begin{itemize}
    \item \textbf{Hard Measurement}: Collapsing the wavefunction by selecting the single oscillator with the highest amplitude at each position (ArgMax).
    \item \textbf{Soft Measurement}: Sampling the Boltzmann distribution of the thermalized system (Temperature Sampling).
    \item \textbf{Spectral Measurement}: Observing the aggregate energy of the Carriers rather than the particles, extracting the ``gist'' or semantic summary without decoding the literal tokens.
\end{itemize}

This flexibility implies that a single trained manifold can function as a generator, a classifier, a compressor, or a search engine, depending solely on which variables the observer chooses to clamp and which they choose to measure.

\subsection{Dark Particles: Observer Interventions Without Query Fossilization}
\label{sec:dark_particles}

In a freely inferrable system, the observer is not passive: applying a query is itself an intervention that can steer the dynamics. We therefore distinguish between \emph{endogenous} top-down feedback (crystallized carriers biasing anchored oscillators) and \emph{exogenous} observer-driven forcing.

\begin{definition}[Dark Particle (Probe Degree of Freedom)]
A \emph{dark particle} is an observer-introduced probe that couples into the manifold dynamics (injecting energy, heat, or phase pull into existing token identities), but is not itself eligible to become persistent knowledge.
\end{definition}

Operationally, this means:
\begin{itemize}
    \item \textbf{Coupling allowed}: dark particles may drive chain reactions by injecting energy into substrate token identities, thereby activating carriers and shifting basins of attraction.
    \item \textbf{No direct recruitment}: dark particles are \emph{ineligible} to be selected as carrier \emph{anchors} or as conflict \emph{offenders} that seed splitting. New carriers induced during probing must be recruited/anchored by \emph{existing} substrate oscillators that align with the emergent concept.
\end{itemize}

This provides a physically grounded resolution to a common tension in online systems: \emph{inference is information} and should rapidly steer behavior (e.g., under rule change), but transient queries should not permanently overwrite the substrate by being ``written'' as anchors. Instead, the probe can create a candidate carrier, and the substrate either supports it (metabolic survival) or it decays.

\paragraph{Experiment sketch (rule change).}
We evaluate three observer policies: (i) \emph{no-write probes} (probes cannot create/split/anchor carriers), (ii) \emph{dark probes} (probes can drive dynamics and trigger candidate carriers, but remain ineligible for anchoring/offender selection), and (iii) \emph{full-write probes} (probes are treated as ordinary oscillators). We measure switch speed under abrupt context change, stability after probe removal, and ``query fossilization'' (probe indices appearing as anchors in crystallized carriers).

% ============================================================================
% 7. THE SENSORIUM MANIFOLD
% ============================================================================
\section{The Sensorium Manifold}
\label{sec:sensorium}

The Sensorium Manifold is a three-dimensional thermodynamic simulation volume where particles interact via physical forces, spectral carriers provide non-local coupling, and structure emerges from the interplay of energy flow and metabolic regulation.

\subsection{Particle-as-Oscillator Duality}
\label{sec:particle_oscillator}

Each particle in the manifold has a dual identity:
\begin{itemize}
    \item \textbf{Spatial identity}: Position $\bm{x} \in \R^3$, velocity $\bm{v}$, mass $m$, heat $Q$.
    \item \textbf{Spectral identity}: Intrinsic frequency $\omega$, amplitude $A = \sqrt{\eosc}$, phase $\theta$.
\end{itemize}

The spatial layer governs local thermodynamics: gravitational attraction, heat diffusion, viscous drag, and particle collisions. The spectral layer governs non-local coupling: carriers create resonant potential wells that entangle oscillators across arbitrary distances.

\begin{definition}[Oscillator State]
An oscillator is represented as a complex phasor:
\begin{equation}
z_i = A_i e^{i\theta_i}, \quad \text{where } A_i = \sqrt{\energy_i}
\label{eq:oscillator}
\end{equation}
The intrinsic frequency $\omega_i$ is a conserved property assigned at particle creation (e.g., from the Universal Tokenizer hash). It does not change during simulation.
\end{definition}

\subsection{The Two-Layer Architecture}
\label{sec:two_layer}

The system operates two physics engines in parallel:

\subsubsection{Layer 1: Thermodynamic Particle Physics}
Particles evolve according to classical field equations:
\begin{itemize}
    \item \textbf{Gravity}: $\nabla^2 \phi = 4\pi G \rho$ (Poisson equation for gravitational potential)
    \item \textbf{Heat diffusion}: $\partial T / \partial t = \alpha \nabla^2 T$ (thermal diffusion on the field)
    \item \textbf{Particle update}: Forces from $-\nabla \phi$ (gravity) and $-\nabla T$ (thermal pressure), viscous drag (Stokes' law), and collision response (Hertzian contact + momentum conservation).
\end{itemize}

This layer is implemented as a GPU-accelerated PDE solver with trilinear field interpolation and spatial-hash collision detection.

\subsubsection{Layer 2: Spectral Carrier Coupling}
Oscillators couple non-locally via a population of \emph{spectral carriers}---global modes that exist in frequency space, not physical space.

\begin{definition}[Spectral Carrier]
A carrier $C_k = R_k e^{i\psi_k}$ is a complex mode with:
\begin{itemize}
    \item Intrinsic frequency $\Omega_k$ (center of its tuning curve)
    \item Gate width $\sigma_k$ (frequency selectivity)
    \item Metabolic state: volatile $\to$ stable $\to$ crystallized
\end{itemize}
\end{definition}

Carriers are \emph{not} located in space. They are standing waves in the oscillator ensemble. Two oscillators with frequencies near $\Omega_k$ are entangled via the carrier regardless of their spatial separation.

\subsection{Carrier Dynamics}
\label{sec:carrier_dynamics}

The carrier-oscillator coupling follows a resonance potential (energy-based, not geometric):

\begin{equation}
V = -\sum_{i,k} T_{ik} \cdot \text{Re}(z_i C_k^*) + \frac{\lambda}{2} \sum_k |C_k|^2
\label{eq:carrier_potential}
\end{equation}

where the tuning kernel $T_{ik}$ determines frequency selectivity:
\begin{equation}
T_{ik} = \exp\left( -\frac{(\omega_i - \Omega_k)^2}{\sigma_k^2} \right)
\label{eq:tuning_kernel}
\end{equation}

\begin{remark}[Sparse evaluation without approximation]
Although the Gaussian tuning kernel has infinite support in real arithmetic, in IEEE-754 fp32 it underflows to \emph{exactly zero} when
\(
(\omega_i-\Omega_k)^2/\sigma_k^2 \ge x_0
\),
with
\(
x_0 = -\ln(\mathrm{FLT\_TRUE\_MIN}) = 149\ln 2
\).
Therefore, for fixed $\sigma_{\max}$, all interactions with
\(
|\omega_i-\Omega_k| \ge \sqrt{x_0}\,\sigma_{\max}
\)
are provably irrelevant in fp32 and can be skipped with no approximation. Our implementation exploits this by bucketing carriers into frequency bins and evaluating only a small neighborhood of bins per oscillator.
\end{remark}

The dynamics are Langevin flow on this potential:
\begin{align}
\dot{C}_k &= \sum_i T_{ik} z_i - \lambda C_k + \sqrt{2T} \, \xi_k(t) \label{eq:carrier_update} \\
\dot{\theta}_i &= \omega_i + \kappa \sum_k T_{ik} A_i R_k \sin(\psi_k - \theta_i) + \sqrt{2T} \, \eta_i(t) \label{eq:phase_update}
\end{align}

where $T$ is the Langevin temperature (noise strength) and $\kappa$ is the coupling scale.

\subsection{Carrier Crystallization (Memory)}
\label{sec:carrier_memory}

Carriers undergo a lifecycle that implements memory:
\begin{enumerate}
    \item \textbf{Volatile}: Newly spawned, decays quickly unless reinforced by resonance.
    \item \textbf{Stable}: Sufficient amplitude and coherence; decays slowly.
    \item \textbf{Crystallized}: Frozen as permanent memory; does not decay; provides top-down bias to anchored oscillators.
\end{enumerate}

Crystallized carriers store \emph{anchored oscillators}---a small set of oscillator indices and their relative phase offsets. When the carrier activates, it injects energy into its anchored oscillators, enabling pattern completion (the ``bucket dump'' reconstruction).

\subsection{Conflict-Driven Splitting}
\label{sec:splitting}

When a carrier experiences persistent phase incoherence (high conflict among its coupled oscillators), it identifies the ``offending'' oscillator---the one most misaligned with the carrier's target phase---and spawns a new carrier centered on that oscillator's frequency.

This implements automatic mode separation: a carrier that initially captures a broad frequency band will progressively specialize as conflict accumulates, eventually covering the oscillator population with a diverse set of narrow-band carriers.

% ============================================================================
% 8. IDLE COMPUTE (DISSIPATIVE STRUCTURE MAINTENANCE)
% ============================================================================
\section{Idle Compute}
\label{sec:idle_compute}

Between observations, the system performs \emph{idle compute}---internal processing that refines the carrier population without external input. Following Prigogine's theory of dissipative structures \citep{prigogine1977self}, this is the mechanism by which the system maintains organization far from equilibrium.

\subsection{Idle Modes}

The idle compute has three modes:
\begin{enumerate}
    \item \textbf{Consolidation}: Low noise, favoring crystallization of stable carriers. The system ``locks in'' patterns that have been consistently reinforced.
    \item \textbf{Disambiguation}: Carrier-frequency repulsion active. Nearby carriers in $\omega$-space repel each other, reducing mode collision.
    \item \textbf{Exploration}: High noise, low weight thresholds. Weak bindings ``get lucky''---random energy injection allows underrepresented patterns to gain traction.
\end{enumerate}

\subsection{Adaptive Thermodynamics}

The system computes global energy statistics (mean, variance) via GPU reduction and uses these to adaptively scale:
\begin{itemize}
    \item \textbf{Decay rates}: Carriers decay relative to system energy scale, preventing runaway growth.
    \item \textbf{Noise amplitude}: Langevin temperature scales with system energy, maintaining a consistent exploration/exploitation balance.
    \item \textbf{Metabolic cost}: Carriers that do not receive sufficient ``income'' (coherent drive) shrink, implementing continuous model selection.
\end{itemize}

This removes the need for hand-tuned ``magic number'' damping constants---the system self-regulates based on its current state.

% ============================================================================
% 9. CROSS-MODAL TRANSDUCTION
% ============================================================================
\section{Cross-Modal Transduction}
\label{sec:bridge}

The Sensorium Manifold enables cross-modal transduction through frequency-based carrier coupling. Carriers are \emph{not} located in geometric space; they exist in frequency space and can couple oscillators from any modality.

\subsection{Frequency-Domain Coupling}
\label{sec:freq_coupling}

Cross-modal transduction works because the Universal Tokenizer assigns intrinsic frequencies ($\omega$) to all particles regardless of modality. A text token, an audio sample, and an image pixel all become oscillators with frequencies determined by their (byte, index) hash.

When oscillators from different modalities have nearby frequencies, they couple to the same carriers. This creates automatic cross-modal association:
\begin{itemize}
    \item An audio sample of a cat meowing may hash to frequencies near the text ``meow''.
    \item When both are present during training, carriers that bind one will also bind the other.
    \item At inference, injecting one modality excites shared carriers, which in turn inject energy into oscillators of the other modality.
\end{itemize}

\subsection{Bidirectional Transduction}

The coupling is inherently bidirectional. The same carrier dynamics (\Cref{eq:carrier_update}, \Cref{eq:phase_update}) apply regardless of which oscillators are clamped (observed) and which are free (to be inferred).

\begin{itemize}
    \item \textbf{Text $\to$ Audio}: Clamp text oscillators, let audio oscillators relax.
    \item \textbf{Audio $\to$ Text}: Clamp audio oscillators, let text oscillators relax.
    \item \textbf{Image $\to$ Text + Audio}: Clamp image, let both text and audio relax.
\end{itemize}

The manifold does not distinguish these cases. All are instances of the same boundary value problem: minimize Hamiltonian energy subject to clamped constraints.

% ============================================================================
% 7. EXPERIMENTS
% ============================================================================
\section{Experiments}
\label{sec:experiments}

We validate the framework on three tasks:

\subsection{Rule-Shift Adaptation}
\label{sec:rule_shift}

We evaluate adaptation to distributional shifts using a controlled benchmark where the sequential structure completely reverses mid-stream.

\paragraph{Method.}
The experiment uses two phrases with identical characters but reversed word order:
\begin{itemize}
  \item \textbf{Forward phase}: ``The cat sat on the mat.'' repeats 50 times
  \item \textbf{Reverse phase}: ``mat the on sat cat The.'' repeats 50 times
\end{itemize}

Both phrases are padded to a fixed segment size (24 bytes), ensuring the thermodynamic trie captures position-specific transitions. At evaluation, we test prediction accuracy using only data seen \emph{before} the current point---simulating online learning where the system must adapt incrementally.

\paragraph{Results.}
\Cref{tab:rule_shift} shows the key finding: when the rule shifts at repetition 50, accuracy immediately drops to 0\% (the forward trie cannot predict reversed patterns). However, after just 5 additional repetitions of the new pattern, accuracy recovers to 100\%. This demonstrates rapid online adaptation through carrier dynamics---no gradient-based retraining required.

\input{tables/rule_shift_summary.tex}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/rule_shift.png}{%
    \includegraphics[width=\textwidth]{figures/rule_shift.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Rule-shift adaptation dynamics. \textbf{(A)} Accuracy over training repetitions; the vertical line marks the rule shift. Forward accuracy is stable at 100\%, drops to 0\% at the shift, then recovers to 100\% within 5 repetitions. \textbf{(B)} Summary statistics comparing forward and reverse phases. \textbf{(C)} Accuracy gain from the initial 0\% in the reverse phase, showing rapid adaptation.}
  \label{fig:rule_shift}
\end{figure}

\subsection{Audio Waveform Inpainting}
\label{sec:audio_exp}

This experiment tests audio sample reconstruction using the same periodic position mechanism as time-series forecasting.

\paragraph{Method.}
Audio waveforms are byte-quantized ([-1, 1] $\to$ [0, 255]) and the segment size is set to match the signal period (18 samples at 440 Hz). For periodic signals, values at the same phase position across periods should be similar. We mask random samples and reconstruct using the weighted average of values at matching segment positions in the training data.

\paragraph{Results.}
\Cref{tab:audio_gen} shows reconstruction quality across waveform types. SNR ranges from 5--10 dB depending on mask level, with lower masking yielding better results. MAE (amplitude error) increases approximately linearly with mask fraction. Exact byte accuracy is near zero because continuous values rarely match exactly---but the SNR metric confirms that reconstructed waveforms closely track the original.

\input{tables/audio_gen_summary.tex}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/audio_gen.png}{%
    \includegraphics[width=\textwidth]{figures/audio_gen.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Audio waveform inpainting. \textbf{(A)} Original vs.\ reconstructed sine wave (first 200 samples at 20\% masking). \textbf{(B)} SNR by waveform type at 20\% masking. \textbf{(C)} MAE vs.\ mask percentage for sine wave.}
  \label{fig:audio_gen}
\end{figure}

\subsection{Cocktail Party Separation}
\label{sec:cocktail_party}

We evaluate audio source separation using a two-speaker mixture. This experiment demonstrates that the manifold can process raw audio bytes and separate them into distinct streams without any audio-specific preprocessing.

\paragraph{Method.}
The mixed audio file is tokenized byte-by-byte using the Universal Tokenizer, creating particles with frequencies determined by the (byte, position) hash. As the manifold runs, carriers crystallize around coherent frequency patterns. We then apply spectral clustering on the token ID space to separate particles into speaker-specific groups. Each group is dehashed to recover the original byte values, producing separated audio streams.

\paragraph{Results.}
\Cref{tab:cocktail_party} shows that the system separates the mixture into two approximately equal streams. The separation score measures the ratio of inter-cluster to intra-cluster distance in the normalized frequency space---higher values indicate cleaner separation. The crystallized carriers capture the distinct spectral signatures of each speaker.

\IfFileExists{tables/cocktail_party_summary.tex}{%
  \input{tables/cocktail_party_summary.tex}
}{%
  \fbox{\parbox{0.9\textwidth}{\centering\vspace{0.8cm}\textit{Table will be generated by \texttt{make paper}}\vspace{0.8cm}}}
}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/cocktail_party.png}{%
    \includegraphics[width=\textwidth]{figures/cocktail_party.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Cocktail party separation via spectral clustering. \textbf{(A)} Token ID distributions for separated speakers, showing distinct frequency bands. \textbf{(B)} Temporal distribution of samples, with each color representing a different speaker stream. \textbf{(C)} Cluster statistics showing mean frequencies with standard deviation error bars; separation score quantifies inter-cluster vs intra-cluster distance.}
  \label{fig:cocktail_party}
\end{figure}

\subsection{Native Image Handling}
\label{sec:image_exp}

We show that the same unified manifold handles 2D image frequencies. Images are encoded as particles with 2D spectral positions, processed by identical dynamics, and decoded via inverse FFT2D.

\IfFileExists{tables/mnist_trie_recall_summary.tex}{%
  \input{tables/mnist_trie_recall_summary.tex}
}{%
  \fbox{\parbox{0.9\textwidth}{\centering\vspace{0.8cm}\textit{Table will be generated by \texttt{make paper}}\vspace{0.8cm}}}
}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/mnist_bytes.png}{%
    \includegraphics[width=0.9\textwidth]{figures/mnist_bytes.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{MNIST byte-level processing via Universal Tokenizer. The system encodes raw pixel bytes as position-aware tokens, enabling pattern learning without image-specific preprocessing.}
  \label{fig:mnist_bytes}
\end{figure}

\subsection{Cross-Modal Processing}
\label{sec:cross_modal_exp}

We demonstrate native multimodality by processing text and images simultaneously in the same manifold without modality-specific encoders.

\paragraph{Method.}
Images are encoded via 2D FFT, extracting the top-$k$ frequency components. Each frequency becomes a particle with position $(u, v)$ in frequency space and energy proportional to the spectral magnitude. Text tokens are encoded using the same hash-based tokenizer. Both modalities coexist as particles in a unified manifold, interacting through shared carrier dynamics.

\paragraph{Results.}
\Cref{tab:cross_modal} shows reconstruction quality across different image patterns. Horizontal and vertical stripes achieve MSE $<$ 0.005, while checkerboard patterns achieve near-perfect reconstruction (MSE $\approx$ 0.0001). The key insight: \emph{no modality-specific processing is required}---the same thermodynamic dynamics that handle text naturally handle frequency-domain images.

\IfFileExists{tables/cross_modal_summary.tex}{%
  \input{tables/cross_modal_summary.tex}
}{%
  \fbox{\parbox{0.9\textwidth}{\centering\vspace{0.8cm}\textit{Table will be generated by \texttt{make paper}}\vspace{0.8cm}}}
}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/cross_modal.png}{%
    \includegraphics[width=\textwidth]{figures/cross_modal.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Cross-modal processing. \textbf{Top two rows}: Original and reconstructed images showing four pattern types. \textbf{(A)} Frequency-space particle distribution for horizontal stripes showing energy concentration along vertical axis. \textbf{(B)} MSE comparison across patterns. \textbf{(C)} Text-image associations demonstrating semantic binding.}
  \label{fig:cross_modal}
\end{figure}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/cross_modal_embedding.png}{%
    \includegraphics[width=0.85\textwidth]{figures/cross_modal_embedding.png}
  }{%
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Cross-modal particles in common embedding space. Image frequencies (blue) and text tokens (red stars) coexist in a shared 3D representation. The manifold processes both modalities through identical thermodynamic dynamics, with semantic associations emerging from carrier coupling.}
  \label{fig:cross_modal_embedding}
\end{figure}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/frequency_particles.png}{%
    \includegraphics[width=0.7\textwidth]{figures/frequency_particles.png}
  }{%
    \fbox{\parbox{0.7\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Particle distribution in 2D frequency space. Each point represents a frequency component from the cross-modal experiment. Point size and color encode oscillator energy---higher energy indicates stronger resonance at that frequency. The cross pattern emerges from horizontal and vertical stripe images, with the DC component (center) having highest energy.}
  \label{fig:frequency_particles}
\end{figure}

\subsection{Scaling Analysis}
\label{sec:scaling_exp}

We empirically verify the scaling properties of the manifold, particularly the O($k$) latency claim and carrier population dynamics.

\paragraph{Carrier Population Dynamics.}
We track carrier births, deaths, and crystallization over 300 simulation steps. Results show that carriers rapidly fill to capacity (64) and crystallize, with zero metabolic pruning in steady state. The ``carrying capacity'' reaches 100\% of the configured maximum.

\paragraph{O($k$) Latency Independence.}
We measure per-step latency across sequence lengths from 500 to 8,000 tokens. \Cref{tab:scaling} shows that latency remains stable at $\approx$3 ms/step with only 11\% coefficient of variation---empirically confirming that latency scales with carrier count $k$ rather than sequence length $N$.

\paragraph{Generalization.}
We test structure emergence on four data types: repetitive text, semi-random patterns, natural-like text, and pure random data. All except semi-random achieve full crystallization, suggesting the manifold finds structure in both highly regular and high-entropy data.

\IfFileExists{tables/scaling_summary.tex}{%
  \input{tables/scaling_summary.tex}
}{%
  \fbox{\parbox{0.9\textwidth}{\centering\vspace{0.8cm}\textit{Table will be generated by \texttt{make paper}}\vspace{0.8cm}}}
}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/scaling_dynamics.png}{%
    \includegraphics[width=\textwidth]{figures/scaling_dynamics.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Scaling dynamics. \textbf{(A)} Carrier population over time showing rapid crystallization. \textbf{(B)} Birth/death events concentrated in early steps. \textbf{(C)} Crystallized carriers vs.\ pattern count showing capacity saturation. \textbf{(D)} Structure ratio across data types.}
  \label{fig:scaling_dynamics}
\end{figure}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/scaling_compute.png}{%
    \includegraphics[width=\textwidth]{figures/scaling_compute.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Compute scaling. \textbf{(A)} Wall-clock time vs.\ particle count showing near-linear scaling. \textbf{(B)} Time vs.\ grid size (larger grids are slightly faster due to GPU optimization). \textbf{(C)} O($k$) latency test showing stable ms/step across 16$\times$ range of sequence lengths.}
  \label{fig:scaling_compute}
\end{figure}

\subsection{Ablation Studies}
\label{sec:ablations}

\input{tables/ablation.tex}

% ============================================================================
% RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related}

\paragraph{Dissipative Structures and Thermodynamic Computing}
Prigogine's theory of dissipative structures \citep{prigogine1977self, prigogine1978time} describes how systems far from equilibrium can maintain complex organization by continuously dissipating energy. Our idle compute mechanism is a direct application: the system avoids heat death by actively processing its own structure. Recent work on thermodynamic computing \citep{conte2019thermodynamic, boyd2022thermodynamic, whitelam2025generative} explores physical substrates for computation based on these principles; we implement them in software via GPU-accelerated field solvers and spectral carrier dynamics.

\paragraph{Oscillator Networks and the Kuramoto Model}
The Kuramoto model \citep{kuramoto1975international, strogatz2000kuramoto} describes synchronization in coupled oscillator populations and has recently been applied to deep learning. Artificial Kuramoto Oscillatory Neurons (AKOrN) \citep{miyato2025akorn} replace threshold units with oscillatory neurons that synchronize through generalized Kuramoto dynamics, demonstrating improvements in object discovery, adversarial robustness, and reasoning. Our spectral carrier mechanism shares the phase-coupling principle but extends it with metabolic gating and crystallization for memory formation.

\paragraph{Binding by Synchrony}
The hypothesis that neural oscillation synchrony solves the binding problem \citep{singer1999neuronal, engel2001dynamic, wang2010neurophysiological} provides biological grounding for our approach. Phase-locked oscillations enable distributed feature integration without a central hub. Our carriers implement this mechanism: oscillators with similar frequencies bind through shared carriers, regardless of their spatial separation.

\paragraph{Modern Hopfield Networks and Associative Memory}
Classical Hopfield networks \citep{hopfield1982neural} implement associative memory via energy minimization. Modern Hopfield networks \citep{ramsauer2021hopfield} achieve exponential storage capacity and connect to transformer attention. Recent work on sparse Hopfield networks \citep{santos2024sparse} and Hopfield Encoding Networks \citep{widrich2024hopfield} extends these capabilities. Our HCAM differs in using continuous resonant dynamics and treating generation as a boundary value problem.

\paragraph{Content-Addressable Memory in Transformers}
Recent work integrates content-addressable memory into transformers: CAMELoT \citep{liu2024camelot} adds training-free associative memory, ARMT \citep{bulatov2024armt} combines attention with segment-level recurrence for 50M+ token sequences, and Memory Mosaics \citep{liu2024memorymosaics} provide interpretable compositional memory. Our carriers serve an analogous role but operate in frequency space.

\paragraph{Non-Autoregressive and Parallel Generation}
Non-autoregressive transformers \citep{gu2018nonautoregressive} generate all tokens in parallel but struggle with output dependencies. Masked diffusion language models \citep{sahoo2024mdlm, nie2024scaling} achieve competitive performance with autoregressive models while enabling parallel sampling. Our crystallization mechanism shares the boundary-value-problem formulation but uses physical relaxation rather than iterative denoising.

\paragraph{Diffusion Models and Langevin Dynamics}
Diffusion models \citep{sohl2015deep, ho2020denoising, song2021score} learn to reverse a noising process, generating samples through Langevin dynamics. Our carrier dynamics use Langevin flow on a resonance potential, but ``generation'' emerges from phase alignment rather than denoising.

\paragraph{Hamiltonian and Symplectic Neural Networks}
Hamiltonian Neural Networks \citep{greydanus2019hamiltonian} and Symplectic networks \citep{chen2020symplectic, jin2020sympnets} learn energy-conserving dynamics from data. Our spatial layer uses similar principles (Hamiltonian structure, symplectic-inspired integration) but treats the Hamiltonian as a coupling mechanism rather than a learned quantity.

\paragraph{Multimodal Architectures}
CLIP \citep{radford2021learning} and Flamingo \citep{alayrac2022flamingo} require explicit cross-modal coupling mechanisms. Unified-IO 2 \citep{lu2024unifiedio2} tokenizes all modalities into a shared space. Our Universal Tokenizer achieves modality-agnostic encoding through deterministic hashing, with cross-modal coupling emerging from shared spectral carrier dynamics.

\paragraph{Energy-Based Models}
Energy-based models \citep{lecun2006tutorial, du2021improved} define learning as energy minimization. Our framework uses energy differently: the Hamiltonian governs dynamics, but ``learning'' is the crystallization of carriers, not optimization of a loss function.

\paragraph{Hebbian Learning and Predictive Coding}
Our approach shares principles with Hebbian learning \citep{hebb1949organization}: carrier-oscillator coupling strengthens through resonance. Predictive coding \citep{rao1999predictive, friston2010free} models the brain as minimizing surprise; our top-down bias from crystallized carriers implements a similar mechanism.

% ============================================================================
% 9. DISCUSSION
% ============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{What We Claim}

\begin{enumerate}
  \item \textbf{Native multimodality}: All sensory modalities can be processed by the same thermodynamic dynamics on a shared manifold.
  \item \textbf{No backpropagation}: Learning emerges from local carrier-oscillator dynamics regulated by homeostasis and adaptive thermodynamics.
  \item \textbf{Online adaptation}: The system adapts continuously to streaming data and distributional shifts via spectral carrier evolution.
  \item \textbf{Dissipative self-organization}: Idle compute maintains structure far from equilibrium by processing carrier populations through consolidation, disambiguation, and exploration modes.
  \item \textbf{Zero-latency scaling}: Sequence generation latency scales with relaxation complexity $O(k)$, not sequence length $O(N)$. Time is treated as a spatial dimension.
\end{enumerate}

\subsection{What We Do Not Claim}

\begin{enumerate}
  \item \textbf{Strict thermodynamics}: We use thermodynamic \emph{metaphors}. The spatial layer dissipates kinetic energy into heat (first law); the spectral layer uses adaptive decay and metabolic gating. There is no global conservation law or detailed balance.
  \item \textbf{Gradient-free optimization}: We are not optimizing a loss function without gradients. We sidestep optimization entirely---structure emerges from carrier crystallization.
  \item \textbf{Transformer replacement}: Our experiments are on small-scale tasks. We make no claims about scaling to language model pretraining.
  \item \textbf{Exact continuous dynamics}: The spatial layer uses GPU field solvers (Jacobi iteration, Euler-Maruyama diffusion). These are discretized approximations to continuous PDEs.
\end{enumerate}

\subsection{The Zero-Latency Paradigm}

The Sensorium Manifold exposes a fundamental inefficiency in autoregressive architectures. Transformers require $O(N)$ sequential forward passes to generate $N$ tokens---each token must wait for all preceding tokens. This serial bottleneck is intrinsic to the causal mask.

Crystallization inverts this relationship. By treating the sequence index $t$ as a spatial coordinate rather than a temporal one, the manifold solves the entire field equation simultaneously. Spectral carriers propagate constraints in all directions: forward (causal), backward (non-causal), and laterally (cross-modal). A prompt at $t=0$ and a constraint at $t=N$ jointly determine the solution in between.

The practical implication: generating a 10-token sentence and a 1000-token paragraph require approximately the same number of relaxation steps, assuming the energy landscape complexity is similar. Latency scales with the number of carriers that must be excited (pattern complexity), not the number of oscillators that must be resolved (sequence length).

This is \emph{Time as Space}---a geometric rather than temporal view of sequence generation.

\subsection{The Physics of ``Horizontal''}

We claimed that ``the physics of horizontal is the same whether it is a word, a sound wave, or a pixel pattern.'' Let us make this precise:

\begin{itemize}
  \item \textbf{Text}: The word ``horizontal'' is a token with a D-dimensional embedding.
  \item \textbf{Audio}: A sound panning left-to-right has specific frequency characteristics (Doppler, stereo phase).
  \item \textbf{Image}: A horizontal line has energy concentrated at $v \approx 0$ in the 2D frequency domain.
\end{itemize}

All three representations enter the manifold as particles. Through co-activation during training, carriers couple these representations. The word ``horizontal'' activates carriers that also respond to horizontal image frequencies and horizontal audio characteristics. This is not a metaphor---it is the mechanism.

% ============================================================================
% 10. CONCLUSION
% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We have presented the Sensorium Manifold, a unified thermodynamic substrate for native multimodal computation. By representing all sensory inputs as oscillators with intrinsic frequencies (via the Universal Tokenizer), we achieve modality-agnostic dynamics. Learning emerges from spectral carrier crystallization---the spontaneous formation of resonant modes that couple oscillators across frequency space---without backpropagation.

The crystallization mechanism represents a fundamental departure from the autoregressive paradigm. By treating time as a spatial dimension and generation as a boundary value problem, we achieve latency that scales with pattern complexity rather than sequence length. The manifold functions as a Holographic Content Addressable Memory: partial inputs address complete patterns because information is distributed across the entire resonant field. ``Next token prediction'' is merely a degenerate case of this more general capability.

The framework suggests an alternative to the optimization-centric paradigm of modern machine learning. Physical principles---thermodynamics, diffusion, homeostasis, and holographic encoding---may offer paths to adaptive systems that are better suited to continuous, online, embodied learning.

The physics of ``horizontal'' really is the same across modalities. And that, perhaps, is how perception should work.

% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{plainnat}
\bibliography{references}

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\section{Additional Experiments}
\label{sec:appendix_experiments}

This appendix aggregates additional kernel experiments that exercise the same
mechanism across tasks and different sampling/observation choices.

\subsection{Universal Tokenizer Collision Regimes (TOY)}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/collision_compression_summary.png}{%
    \includegraphics[width=0.95\textwidth]{figures/collision_compression_summary.png}
  }{%
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Synthetic trie collision sweep. As collision rate increases, spatial clustering increases and compression ratio decreases, demonstrating that hash collisions act as compression.}
  \label{fig:collision_compression}
\end{figure}

\subsection{Image-Based Hash Collision Compression}
\input{tables/image_collision_summary.tex}
\begin{figure}[t]
  \centering
  \IfFileExists{figures/image_collision_hero.png}{%
    \includegraphics[width=\textwidth]{figures/image_collision_hero.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Hash collisions act as compression in a thermodynamic trie: multi-panel visualization demonstrating spatial clustering, energy accumulation, and information compression across different collision rates. As collision rate increases (left to right), particles with identical token IDs cluster together in 3D space, energy accumulates in colliding particles, and compression metrics improve (lower compression ratio and entropy). The visualization proves that hash collisions naturally create a hierarchical trie structure where shared patterns compress through thermodynamic dynamics.}
  \label{fig:image_collision_hero}
\end{figure}
\begin{figure}[t]
  \centering
  \IfFileExists{figures/image_collision_bifurcation.png}{%
    \includegraphics[width=\textwidth]{figures/image_collision_bifurcation.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Bifurcation charts showing the trie structure created by hash collisions. Top row: Hierarchical clustering dendrograms showing how token IDs branch into a tree structure. Bottom row: Spatial bifurcation plots showing how particles with different token IDs separate in space, with convex hulls highlighting clusters. Higher collision rates create more pronounced branching patterns, demonstrating that collisions create a thermodynamic trie where shared prefixes cluster together and unique patterns branch apart.}
  \label{fig:image_collision_bifurcation}
\end{figure}

\subsection{Next-Byte Prediction via Thermodynamic Trie}
\label{sec:next_byte_prediction}

This experiment demonstrates how the Universal Tokenizer creates a \emph{thermodynamic trie}---a branching structure that emerges from controlled hash collisions. The key insight is that hash collisions are not errors but \emph{compression}: identical (byte, position) pairs across different occurrences collide into the same particle, allowing the manifold to learn statistical patterns.

\paragraph{Method.}
We train on 7 distinct text patterns (e.g., ``The cat sat.'', ``The dog ran.'') with a segment size of 16 bytes. The tokenizer resets the position counter every 16 bytes, so the character `T' at position 0 receives the same token ID regardless of which pattern instance it appears in. This creates the trie structure:

\begin{enumerate}
\item \textbf{Hash}: Each byte at position $p$ is hashed to token ID $t = (b \cdot 31 + p) \mod 4096$.
\item \textbf{Collide}: Repeated patterns produce repeated token IDs, accumulating energy in shared particles.
\item \textbf{Crystallize}: Carriers form to couple oscillators that co-occur, encoding transition probabilities.
\end{enumerate}

\paragraph{Inference.}
Given context bytes, we compute their token IDs and search for matching sequences in the trained manifold. At branch points (e.g., after ``The cat '', where `s', `r', or `a' may follow), the manifold contains particles for all valid continuations. We score candidates by their accumulated energy and select the most probable.

\paragraph{Results.}
\Cref{tab:next_token} shows that the system achieves 99.3\% accuracy on deterministic paths (where only one continuation is valid) and 55.8\% on branch points (where it picks the most frequent continuation). The 92.4\% overall accuracy reflects the mixture of these cases. Importantly, the 99.4\% top-3 accuracy demonstrates that even at branch points, the correct answer is almost always among the top candidates---the system correctly learns the frequency distribution of continuations without any gradient-based training.

\input{tables/next_token_summary.tex}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/next_token.png}{%
    \includegraphics[width=\textwidth]{figures/next_token.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Thermodynamic trie structure for next-byte prediction. \textbf{(A)} Trie branching visualization showing candidate continuations at each position in the segment; node size encodes probability, edges show transitions between positions. \textbf{(B)} Probability distributions at branch points where multiple continuations are valid; the manifold learns frequency-proportional distributions (e.g., ``The cat '' $\to$ `s':50\%, `r':33\%, `a':17\%). \textbf{(C)} Accuracy by position in segment; green indicates deterministic paths ($>$95\%), orange indicates moderate ambiguity, red indicates branch points with multiple valid continuations.}
  \label{fig:next_token}
\end{figure}

\subsection{Time-Series Forecasting via Position Periodicity}
\label{sec:timeseries}

This experiment tests forecasting on byte-quantized synthetic time series using position-based periodicity---the same mechanism that enables the thermodynamic trie for text.

\paragraph{Method.}
Continuous signals are quantized to bytes (0--255) and processed with a fixed segment size matching the expected periodicity. For prediction, we exploit the key insight: \emph{values at the same segment position should be similar across periods}. This is the time-series analog of the thermodynamic trie---periodic structure creates hash collisions that enable prediction.

\paragraph{Results.}
\Cref{tab:timeseries} shows that the approach works well for stationary periodic signals (sawtooth: MAE 7.9, 57\% direction accuracy; periodic: MAE 21.4, 55\% direction) but struggles with non-stationary patterns. Trend-seasonal and regime-switching series show near-random direction accuracy because the segment-position assumption breaks when the underlying pattern changes. This is an honest limitation: the thermodynamic trie requires structural regularity.

\input{tables/timeseries_summary.tex}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/timeseries.png}{%
    \includegraphics[width=\textwidth]{figures/timeseries.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Time-series forecasting results. \textbf{(A)} Actual vs.\ predicted values for the periodic signal (first 100 test points). \textbf{(B)} Direction accuracy by signal type; 50\% is random baseline. \textbf{(C)} Error distribution for periodic signal showing the concentration of predictions near true values.}
  \label{fig:timeseries}
\end{figure}

\subsection{MNIST Classification from Raw Bytes}
\input{tables/mnist_bytes_summary.tex}
\begin{figure}[t]
  \centering
  \IfFileExists{figures/mnist_bytes.png}{%
    \includegraphics[width=0.75\textwidth]{figures/mnist_bytes.png}
  }{%
    \fbox{\parbox{0.75\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Kernel MNIST confusion matrix (Universal Tokenizer hashed pixel stream).}
  \label{fig:mnist_bytes_appendix}
\end{figure}

\subsection{MNIST Inpainting via Thermodynamic Trie}
\label{sec:mnist_inpainting}

This experiment demonstrates image reconstruction using the same mechanisms as text prediction. Pixels are treated as bytes, hashed with their 2D position to create a thermodynamic trie. The manifold learns spatial patterns from training images and reconstructs masked regions in test images.

\paragraph{Method.}
Each pixel at position $(x, y)$ is hashed to a token ID: $t = (p \cdot 31 + (y \cdot 28 + x)) \mod 4096$, where $p$ is the pixel intensity. For reconstruction, we use dual-domain inference: geometric locality (neighboring pixels) provides spatial smoothness, while spectral carriers provide pattern completion from similar images in the training set.

\paragraph{Results.}
\Cref{tab:image_gen} shows reconstruction quality degrades gracefully as mask fraction increases. At 50\% masking, the system still achieves 13.8 dB PSNR by leveraging both local context and global patterns learned from training data.

\IfFileExists{tables/image_gen_summary.tex}{%
  \input{tables/image_gen_summary.tex}
}{%
  \fbox{\parbox{0.9\textwidth}{\centering\vspace{0.8cm}\textit{Table will be generated by \texttt{make paper}}\vspace{0.8cm}}}
}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/image_gen.png}{%
    \includegraphics[width=\textwidth]{figures/image_gen.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{MNIST inpainting via thermodynamic trie. \textbf{(A)} Example reconstructions at different mask levels (top: original, bottom: reconstructed). \textbf{(B)} PSNR and MAE as functions of mask fraction, showing graceful degradation. \textbf{(C)} Error distribution histograms for each mask level.}
  \label{fig:image_gen}
\end{figure}

\subsection{Byte Denoising via Thermodynamic Trie}
\label{sec:text_diffusion}

This experiment tests masked byte reconstruction using the thermodynamic trie---analogous to masked language modeling but operating on raw bytes.

\paragraph{Method.}
We train on clean text (repetitive phrases to enable pattern learning), then mask random positions in a held-out test segment. For each masked position, we use unmasked context bytes to build token IDs, search for matching patterns in training data, and predict the byte that most frequently follows similar contexts. This is the text equivalent of image inpainting.

\paragraph{Results.}
\Cref{tab:text_diffusion} shows that reconstruction accuracy ranges from 3--11\% across mask levels, well above the random baseline (1/256 $\approx$ 0.4\%). Accuracy is highest at low masking (10\%) where more context is available. The relatively modest accuracy reflects the challenge: text has high entropy per character, and the thermodynamic trie works best when patterns repeat exactly.

\input{tables/text_diffusion_summary.tex}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/text_diffusion.png}{%
    \includegraphics[width=0.9\textwidth]{figures/text_diffusion.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Byte denoising results. \textbf{(A)} Character accuracy vs.\ mask percentage with random baseline shown as dashed line. \textbf{(B)} Accuracy at each mask level, showing consistent above-random performance.}
  \label{fig:text_diffusion}
\end{figure}

\IfFileExists{tables/text_diffusion_examples.tex}{%
  \input{tables/text_diffusion_examples.tex}
}{}

\subsection{Continuous Kernel Simulation Snapshot}
\begin{figure}[t]
  \centering
  \IfFileExists{figures/continuous_final.png}{%
    \includegraphics[width=\textwidth]{figures/continuous_final.png}
  }{%
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}\textit{Figure will be generated by \texttt{make paper}}\vspace{2cm}}}
  }
  \caption{Final dashboard frame from a finite kernel simulation run.}
  \label{fig:continuous_final}
\end{figure}

\section{Extended Translation Table}
\label{app:translation}

For readers seeking deeper correspondences:

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l l}
\toprule
\textcolor{physicsblue}{\textbf{Physics Concept}} & \textcolor{mlgreen}{\textbf{ML Analogue}} \\
\midrule
\textcolor{physicsblue}{3D Manifold space} & \textcolor{mlgreen}{Simulation volume / embedding space} \\
\textcolor{physicsblue}{Particle (spatial layer)} & \textcolor{mlgreen}{Token / feature vector} \\
\textcolor{physicsblue}{Oscillator (spectral layer)} & \textcolor{mlgreen}{Hidden state with phase/frequency} \\
\textcolor{physicsblue}{Intrinsic frequency $\omega$} & \textcolor{mlgreen}{Token identity (from hash)} \\
\textcolor{physicsblue}{Spectral carrier $C_k$} & \textcolor{mlgreen}{Learned mode / pattern detector} \\
\textcolor{physicsblue}{Tuning kernel $T_{ik}$} & \textcolor{mlgreen}{Attention weight (frequency-based)} \\
\textcolor{physicsblue}{Gate width $\sigma_k$} & \textcolor{mlgreen}{Receptive field / specialization} \\
\textcolor{physicsblue}{Carrier crystallization} & \textcolor{mlgreen}{Weight freezing / memory consolidation} \\
\textcolor{physicsblue}{Carrier conflict} & \textcolor{mlgreen}{Mode collision / interference} \\
\textcolor{physicsblue}{Carrier splitting} & \textcolor{mlgreen}{Mixture component birth} \\
\textcolor{physicsblue}{Anchored oscillators} & \textcolor{mlgreen}{Associated tokens in memory} \\
\textcolor{physicsblue}{Top-down energy bias} & \textcolor{mlgreen}{Prior / completion signal} \\
\textcolor{physicsblue}{Gravity field $\phi$} & \textcolor{mlgreen}{Attraction potential} \\
\textcolor{physicsblue}{Heat diffusion} & \textcolor{mlgreen}{Uncertainty propagation} \\
\textcolor{physicsblue}{Particle collision} & \textcolor{mlgreen}{Local interaction / gating} \\
\textcolor{physicsblue}{Langevin noise} & \textcolor{mlgreen}{Stochastic exploration / dropout} \\
\textcolor{physicsblue}{Idle compute} & \textcolor{mlgreen}{Offline consolidation / dreaming} \\
\textcolor{physicsblue}{Crystallization} & \textcolor{mlgreen}{All-token generation (cf. autoregression)} \\
\textcolor{physicsblue}{Holographic CAM} & \textcolor{mlgreen}{Content-addressable associative memory} \\
\textcolor{physicsblue}{Time as Space} & \textcolor{mlgreen}{Sequence index as spatial coordinate} \\
\textcolor{physicsblue}{Boundary Value Problem} & \textcolor{mlgreen}{Constrained generation (inpainting)} \\
\bottomrule
\end{tabular}
\end{center}

\section{Hyperparameter Settings}
\label{app:hyperparameters}

\IfFileExists{tables/hyperparameters.tex}{%
  \input{tables/hyperparameters.tex}
}{%
  \fbox{\parbox{0.9\textwidth}{\centering\vspace{0.8cm}\textit{Table will be generated by \texttt{make paper}}\vspace{0.8cm}}}
}

\section{Pseudocode}
\label{app:algorithms}

\begin{algorithm}[H]
\caption{Unified Manifold Step (Spatial + Spectral Layers)}
\label{alg:unified_step}
\begin{algorithmic}[1]
\Require Particles $\{(\bm{x}_i, \bm{v}_i, \eoscI{i}, \heat_i, \omega_i, \theta_i)\}$, Carriers $\{C_k, \Omega_k, \sigma_k\}$
\State \textbf{// Spatial Layer (Thermodynamic Physics)}
\State Scatter particle masses to gravity field; total internal energy $\eintI{i}=\heat_i+\eoscI{i}$ to internal-energy field
\State Solve $\nabla^2 \phi = 4\pi G \rho$ (Poisson equation for gravity potential)
\State Compute temperature field: $T(\bm{x}) = \eint(\bm{x})/(\rho(\bm{x})c_v)$
\State Diffuse thermal field (FFT/Jacobi): $T \leftarrow T + \alpha \nabla^2 T \cdot \dt$
\State Gather from fields: $\bm{F}_i = -m_i \nabla \phi - (m_i/\rho)\nabla P(\rho,T)$; sample $\eint(\bm{x}_i)$
\State Update velocities: $\bm{v}_i \leftarrow \bm{v}_i \cdot e^{-\gamma \dt} + \bm{F}_i \dt / m_i$
\State Update positions: $\bm{x}_i \leftarrow \bm{x}_i + \bm{v}_i \dt$
\State Compute particle collisions (spatial hash or brute force)
\State Local conservative exchange: $\heat_i \leftrightarrow \eoscI{i}$ via Planck relaxation (\Cref{eq:planck_energy}, \Cref{eq:planck_alpha})
\State \textbf{// Spectral Layer (Carrier Coupling)}
\State Compute energy statistics: $\bar{E}, \sigma_E$ (GPU reduction)
\State Compute oscillator amplitudes: $A_i = \sqrt{\eoscI{i}}$
\State Build frequency bins over carriers (GPU-only bucket + prefix sum)
\For{each oscillator $i$ (parallel)}
  \State Accumulate contributions over nearby carrier bins (skip fp32-zero couplings)
\EndFor
\For{each carrier $k$ (parallel)}
  \State Compute force: $F_k = \sum_i T_{ik} z_i - \lambda C_k + \sqrt{2T}\,\xi_k$
  \State Update carrier: $C_k \leftarrow C_k + F_k \dt$
  \State If conflict > threshold, spawn new carrier from offender
  \State Update crystallization state: volatile $\to$ stable $\to$ crystallized
\EndFor
\State Update oscillator phases: $\theta_i \leftarrow \theta_i + (\omega_i + \kappa \cdot \text{torque}_i) \dt$
\State Top-down bias: crystallized carriers inject energy into anchored oscillators
\State Spawn carriers for uncoupled oscillators
\end{algorithmic}
\end{algorithm}

\end{document}
